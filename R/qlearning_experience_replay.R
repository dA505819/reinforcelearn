#' Q-Learning with Function Approximation and Experience Replay
#'
#' @inheritParams qlearning_fa
#' @param replay.memory list: each list entry is a list with entries
#' state, action, reward, next.state. replay.memory might be filled
#' with experience sampled from a random policy.
#' @param batch.size scalar integer: batch size, how many samples are
#' drawn from the replay memory. Must be smaller than
#' size of the replay memory!
#'
#' @return list with weights
#' @export
#' @seealso [qlearning_fa]
#'
#' @examples
#' # define the environment
#' grid = WindyGridworld$new()
#' WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array,
#'   reward.matrix = grid$reward.matrix,
#'   terminal.states = grid$terminal.states,
#'   initial.state = 30)
#'
#' # define a tensorflow graph for the function approximator (here a neural network)
#' library(tensorflow)
#' tf$reset_default_graph()
#' batch.size = 32L
#' inputs = tf$placeholder(tf$float32, shape(NULL, WindyGridworld1$n.states))
#' weights = tf$Variable(tf$random_uniform(shape(WindyGridworld1$n.states,
#'   WindyGridworld1$n.actions), 0, 0.01))
#' Q = tf$matmul(inputs, weights)
#' nextQ = tf$placeholder(tf$float32, shape(NULL, WindyGridworld1$n.actions))
#' loss = tf$reduce_sum(tf$square(nextQ - Q))
#' optimizer = tf$train$GradientDescentOptimizer(learning_rate = 0.1)
#' trainModel = optimizer$minimize(loss)
#'
#' # initialize the session and the weights
#' sess = tf$Session()
#' sess$run(tf$global_variables_initializer())
#'
#' # takes the state and returns a one-hot vector
#' makeFeatureVector = function(state_) {
#'   one_hot = matrix(0L, nrow = length(state_), ncol = WindyGridworld1$n.states)
#'   one_hot[cbind(seq_along(state_), state_)] = 1L
#'   one_hot
#' }
#'
#' # predict returns vector of q values for a given state
#' predict = function(inputs_) {
#'   sess$run(Q, feed_dict = dict(inputs = inputs_))
#' }
#'
#' # train model, update weights, e.g. gradient descent: this is supervised learning
#' train = function(inputs_, outputs_, predictions_ = NULL) {
#'   sess$run(tuple(trainModel, weights),
#'     feed_dict = dict(inputs = inputs_, nextQ = outputs_))
#' }
#'
#' # define replay.memory
#' replay.memory.size = 10000 # steps
#' replay.memory = vector("list", length = replay.memory.size)
#'
#' # fill this initially with experience generated by random policy
#' WindyGridworld1$reset()
#' for (i in 1:replay.memory.size) {
#'   state = WindyGridworld1$state
#'   action = sample(0:3, 1)
#'   WindyGridworld1$step(action)
#'   replay.memory[[i]] <- list(action = action, reward = WindyGridworld1$reward,
#'     state = state, next.state = WindyGridworld1$state)
#'   if (WindyGridworld1$episode.over == TRUE) {
#'     print(i)
#'     WindyGridworld1$reset()
#'   }
#' }
#'
#' res = qlearning_fa_experience_replay(WindyGridworld1, makeFeatureVector,
#'   predict, train, replay.memory, n.episodes = 100, seed = 123)
#'
qlearning_fa_experience_replay <- function(envir, makeFeatureVector, predict, train,
  replay.memory, batch.size = 32,
  n.episodes = 10, epsilon = 0.1, epsilon.decay = 0.5,
  discount.factor = 1, seed = NULL, ...) {

  if (!is.null(seed)) { set.seed(seed) }
  replay.memory.size = length(replay.memory)

  # statistics about learning behaviour: steps per episode
  steps.per.episode = rep(0L, n.episodes)
  # replay.memory = vector("list", length = replay.memory.size) # fixed size
  k = 0

  for (i in seq_len(n.episodes)) {
    envir$reset()
    state = envir$state
    j = 0

    while (envir$episode.over == FALSE) {
      j = j + 1
      features.state_ = makeFeatureVector(state, ...)
      Q.state = predict(features.state_, ...)
      action = sample_epsilon_greedy_action(Q.state, epsilon)
      envir$step(action)

      next.state = envir$state

      k = k + 1
      if (k > replay.memory.size) k = 1
      replay.memory[[k]] = list(action = action, reward = envir$reward,
        state = state, next.state = next.state)

      # sample a batch of transitions s, a, r, s' from replay.memory
      indexes = sample(seq_len(replay.memory.size), batch.size)
      batch = replay.memory[indexes]

      batch.states = lapply(batch, "[[", "state")
      batch.next.states = lapply(batch, "[[", "next.state")
      batch.actions = sapply(batch, "[[", "action")
      batch.rewards = sapply(batch, "[[", "reward")

      features.state = Reduce(rbind, lapply(batch.states, makeFeatureVector))
      features.next.state = Reduce(rbind, lapply(batch.next.states, makeFeatureVector))
      Q.state = predict(features.state, ...)
      Q.next.state = predict(features.next.state, ...)

      td.target =  batch.rewards + discount.factor * apply(Q.next.state, 1, max) # max over rows
      target.Q = Q.state
      target.Q[cbind(seq_along(batch.actions), batch.actions + 1)] = td.target

      weights = train(features.state, target.Q, Q.state, ...) # train on minibatch

      state = next.state

      if (envir$episode.over) {
        if (i %% 100 == 0) {
          epsilon = epsilon * epsilon.decay
        }
        steps.per.episode[i] = j
        print(paste("Episode", i, "finished after", j, "time steps."))
        break
      }
    }
  }
  # env_monitor_close(envir$client, envir$instance_id)
  list(weights = weights, steps.per.episode = steps.per.episode)
}
