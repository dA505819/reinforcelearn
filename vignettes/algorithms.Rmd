---
output:rmarkdown::html_vignette:
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, eval = T, collapse = TRUE, comment = "#>")
```

## How to solve an environment?

This vignette explains the reinforcement learning algorithms implemented in `reinforcelearn`. 
For details of how to create an environment have a look at the [How to create an environment?](environments.html) vignette.

```{r}
library(reinforcelearn)
```

---

### Q-Learning

The most well-known algorithm for reinforcement learning is probably Q-Learning.

$Q$ is the action value function and specifies how good a state-action pair is. It is a matrix (n.states x n.actions). Interacting with the environment an action is chosen according to an $\epsilon$-greedy behavior policy

$$
\pi(a | S_t) = \left\{ \begin{array}{ll} 1 - \epsilon + \frac{\epsilon}{m}, & \text{if} \; a = \text{argmax}_a Q(S_t, a)  \\
                                        \frac{\epsilon}{m}, & \text{else.} \end{array} \right.
$$

Then the $Q$ value of the state-action pair is updated by

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right],
$$

where $\gamma$ is the so called discount factor and $\alpha$ the learning rate.

In `reinforcelearn` there is a `qlearning` function. In the following we will train on a simple navigation task, the windy gridworld. The first argument of all algorithms is the environment (the `envir`) argument. The number of episodes can be specified via `n.episodes`.

```{r}
# Windy gridworld environment
env = windyGridworld()

res = qlearning(env, n.episodes = 50)
# Note: to find the optimal policy we need to run at least 500 episodes.
```

The `qlearning` function returns the action value function $Q$ (here a matrix) and some statistics about learning behavior, e.g. the number of steps and return per episode.

```{r}
print(res$steps)
```

We can then find the optimal policy by acting greedily with respect to the action value function.

```{r}
# Values of each grid cell
state.values = matrix(apply(res$Q1, 1, max), ncol = 10, byrow = TRUE)
print(round(state.values, 1))

# Subtract 1 to be consistent with action numeration in env
optimal.policy = max.col(res$Q1) - 1
print(matrix(optimal.policy, ncol = 10, byrow = TRUE))
```

There are many parameters, which can be specified, e.g. the discount factor $\gamma$ via `discount`, the learning rate $\alpha$ via `learning.rate` and $\epsilon$ via the `epsilon` argument. These parameters can also be updated over time by passing a function `updateEpsilon` etc. The update function takes two arguments, the old value of the parameter and the number of episodes finished. It returns the updated parameter, e.g. a decreased learning rate. After each episode is finished the update functions are called.

```{r}
res = qlearning(env, epsilon = 0.2, learning.rate = 0.5, discount = 0.99)

# Decay epsilon over time. Every 10 episodes epsilon will be halfed.
decayEpsilon = function(epsilon, i) {
  if (i %% 10 == 0) {
    epsilon = epsilon * 0.5
  }
  epsilon
}

res = qlearning(env, epsilon = 0.5, updateEpsilon = decayEpsilon)
```

Initially the action value function will be initialized to 0. But you can also specify an initial value function via the `initial.value` argument.

```{r}
Q = matrix(100, nrow = env$n.states, ncol = env$n.actions)
res = qlearning(env, n.episodes = 5, initial.value = Q)
# After only 5 episodes the Q values will still be similar to the initial values.
print(matrix(round(apply(res$Q1, 1, max), 1), ncol = 10, byrow = TRUE))
```

### Function Approximation

So far the value function has been represented as a table (number of states x number of actions).
In many interesting problems there are lots of states and actions or the space is continuous. Then it is inpractical to store a tabular value function and to slow to update state-action pairs individually. The solution is to approximate the value function with a function approximator, e.g. a linear combination of features.

In the following we will have a look at the mountain car problem, where the goal is to drive an underpowered car up a steep hill. The state space is continuous in two dimensions, the position and velocity of the car, each bounded in some interval. There are three different actions: push back (0), do nothing (1) and push forward (2).

```{r}
env = MountainCar()
print(env$state.space)
print(env$state.space.bounds)

env$reset()
print(env$state)
```

We will solve this environment using linear function approximation. With linear function approximation the action value function is represented as

$$
\hat{q}(S_t, A_t, w) = x(S_t)^T w = \sum_{j=1}^{n} x_j(S_t) \, w_j.
$$

There is a distinct weight vector per action.

The weights are then updated by

$$
w_{t+1} = w_t + \alpha \left[ R_{t+1} + \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right] x(S_t).
$$

But how to get a good feature vector from the state observation? We will use grid tiling to aggregate the state space. Each tiling is a grid which overlays the state space. A state observation then falls into one tile per tiling and we will use as many weights as there are tiles. The feature vector is then just a one-hot vector of all active tiles.

```{r, out.width = "300px", fig.align="center", echo = FALSE}
knitr::include_graphics("gridtiling.JPG")
```

We can define a function, which takes the original state observation as an input and returns a preprocessed state observation.

```{r}
# Define preprocessing function (we use grid tiling)
n.tilings = 8
max.size = 4096
iht = IHT(max.size)

position.max = env$state.space.bounds[[1]][2]
position.min = env$state.space.bounds[[1]][1]
velocity.max = env$state.space.bounds[[2]][2]
velocity.min = env$state.space.bounds[[2]][1]
position.scale = n.tilings / (position.max - position.min)
velocity.scale = n.tilings / (velocity.max - velocity.min)

# Scale state first, then get active tiles and return n hot vector
gridTiling = function(state) {
  state = c(position.scale * state[1], velocity.scale * state[2])
  active.tiles = tiles(iht, 8, state)
  makeNHot(active.tiles, max.size, out = "vector")
}
```

We can then pass this function on via the `preprocessState` argument in `qlearning`. Via the `fun.approx` argument we tell the algorithm to use a linear combination of features to approximate the value function. Currently `fun.approx` supports `table` and `linear`.

```{r}
set.seed(123)
res = qlearning(env, fun.approx = "linear", preprocessState = gridTiling, n.episodes = 20)
print(res$steps)
```

### Sarsa, Expected Sarsa and Q(sigma)

The Sarsa algorithm is similar to Q-Learning but samples the next action instead of taking the max. Therefore the update rule is

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right].
$$

In contrast to Q-Learning Sarsa is an on-policy algorithm because the action values are adjusted using a target value $Q(S_{t+1}, A_{t+1})$ which uses an action $A_{t+1} $sampled from the same $\epsilon$-greedy policy as $A_t$.

In `reinforcelearn` this algorithm is implemented in the `sarsa` function. It takes the same arguments as the `qlearning` function.

```{r}
env = windyGridworld()
res = sarsa(env, n.episodes = 50)
print(res$steps)
```

Expected Sarsa is a generalization of Q-Learning to arbitrary target policies. While Sarsa samples the next action, Expected Sarsa computes the expectation by multiplying each action value with the probability of choosing this action. The update rule is 

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_{a \in \mathcal{A}} \pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right].
$$

The term $\pi(a|S_{t+1})$ is called the target policy because it is used to compute the target to update the values. If $\pi(a|S_{t+1})$ is the greedy policy Expected Sarsa is exactly Q-Learning. When the target policy uses the same $\epsilon$-greedy policy as to sample $A_t$, then Expected Sarsa is an on-policy algorithm. In R we can use the `target.policy` argument, which can be `"greedy"` or `"egreedy"` for $\epsilon$-greedy.

```{r}
# This is equivalent to qlearning(env):
res = expectedSarsa(env, target.policy = "greedy")

# With an epsilon-greedy target policy:
res = expectedSarsa(env, target.policy = "egreedy")
```

The Q($\sigma$) algorithm generalizes all the algorithms presented so far, i.e. Sarsa, Expected Sarsa and Q-Learning.

The update rule is

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \left(\sigma Q(S_{t+1}, A_{t+1}) + (1 - \sigma) \sum_{a \in \mathcal{A}} \pi(a|S_{t+1}) Q(S_{t+1}, a) \right) - Q(S_t, A_t) \right].
$$

The parameter $\sigma$ controls a weighting between the Sarsa target $Q(S_{t+1}, A_{t+1})$ and Expected Sarsa target $\sum_{a \in \mathcal{A}} \pi(a|S_{t+1}) Q(S_{t+1}, a)$. Therefore Sarsa is equal to Q(1) and Expected Sarsa to Q(0).

```{r}
res = qSigma(env, sigma = 0.5)

# This is equivalent to Sarsa:
res = qSigma(env, sigma = 1)

# This is equivalent to Q-learning:
res = qSigma(env, sigma = 0, target.policy = "greedy")
```

### Eligibility Traces

An eligibility trace is a scalar number per state-action pair (or weight when used with function approximation). The idea is to assign the current error back to all previously visited state-action pairs weighted by their eligibility. Recently and frequently visited state-action pairs will have a higher eligibility trace than those visited a long time ago. At the beginning of an episode all eligibility traces are set to 0. During the episode if a state-action pair is visited its eligibility trace is increased by

$$
E_t(S_t, A_t) = (1 - \beta) \, E_{t-1}(S_t, A_t) + 1,
$$

or for the weights

$$
E_t(w) = (1 - \beta) \, E_{t-1}(w) + x(S_t),
$$

when used with linear function approximation.

Then all action values are updated according to their share on the current error, e.g. for Q-Learning by

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right] \, E_t(s, a) \qquad \forall s \in \mathcal{S}, \forall a \in \mathcal{A},
$$

and then over time fade away with an exponential decrease

$$
E_{t+1}(s, a) = \gamma \lambda (\sigma + (1- \sigma) \pi(A_{t+1} | S_{t+1})) \, E_t(s, a) \qquad \forall s \in \mathcal{S}, \forall a \in \mathcal{A}.
$$

The use of eligibility traces can make reinforcement learning agents learn much faster because all state-action pairs are updated at each time step. 

The factor $\beta$ controls the type of eligibility, for $\beta = 0$ this is the standard accumulating trace, for $\beta = 1$ this is the replacing trace. The factor $\lambda$ is the eligibility decay parameter. The eligibility decrease is weighted with the target's policy probability of the next action for the Expected Sarsa part of the target.

We can specify $\beta$ via the `eligibility.type` argument and $\lambda$ via the `lambda` argument.

```{r}
res = sarsa(env, lambda = 0.9, eligibility.type = 1, n.episodes = 50)
print(res$steps)
```

### Double Learning

The idea of double learning is to decouple action selection and action evaluation. For example in Q-Learning the update is 

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right].
$$

When using double learning we will store two action value functions (or two weight vectors in function approximation), one is used to find the best action

$$
a_* = \text{argmax}_a Q_A(S_{t+1}, a),
$$

while the other is then used to evaluate this action

$$
Q_B(S_{t+1}, a_*).
$$

Learning with two separate action values is slower, but tend to be more robust.

Actions are sampled due to an $\epsilon$-greedy policy with respect to $Q_A + Q_B$. Then at each step randomly one of the two action value functions is updated, e.g. when $Q_A$ is updated the update equation becomes

$$
Q_A(S_t, A_t) \leftarrow Q_A(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q_B(S_{t+1}, \text{argmax}_{a \in \mathcal{A}} Q_A(S_{t+1}, a)) - Q_A(S_t, A_t) \right]
$$

Double Learning can also be used for Sarsa, Expected Sarsa and Q($\sigma$). The update rule for Q($\sigma$) is 

$$
Q_A(S_t, A_t) \leftarrow Q_A(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \left( \sigma Q_B(S_{t+1}, A_{t+1}) + (1 - \sigma) \sum_a \pi(a|S_{t+1}) Q_B(S_{t+1}, a) \right) - Q_A(S_t, A_t)
 \right].
$$

Update equations for $Q_B$ can be obtained by interchanging the roles of $Q_A$ and $Q_B$ in the above equations. 

To use Double Learning with `qSigma`, `qlearning`, `sarsa` and `expectedSarsa` just pass `double.learning = TRUE` to the algorithm. Double Learning works with tables and linear value function approximation.

```{r}
res = expectedSarsa(env, double.learning = TRUE, n.episodes = 50)
print(res$steps)
```

### Experience Replay

When using function approximation in reinforcement learning training can be difficult because subsequent state observations are often highly correlated and we train on these states the order they are experienced. Experience replay is a simple idea to break these correlations and stabilize learning. Instead of training on a simple observation at each time step the algorithm trains now on more than one observation sampled randomly from a replay memory, which stores all previously visited states and actions. Because the observations are trained on in a random order correlations are much smaller.

In `reinforcelearn` experience replay can be used by passing on a list of experiences to the `replay.memory` argument. Each list entry is itself a list with the entries `state`, `action`, `reward` and `next.state`. `state` and `next.state` should have been preprocessed, e.g. by calling `preprocessState(state)`. A different possibility is to specify the `replay.memory.size` argument, which will then be initialized with experiences generated by a random policy. The number of experiences trained on at each step is controled via the `batch.size` argument.

When experiencing a new transition the algorithm replaces the oldest entry in the replay memory by the new transition.

```{r}
# Fill a replay memory of size 100 on the mountain car task.
# We will use grid tiling as defined above.
memory = vector("list", length = 100)
env = MountainCar()
env$reset()
for (i in 1:100) {
  if (env$done) {
    env$reset()
  }
  action = sample(0:2, size = 1, prob = c(0.5, 0, 0.5))
  env$step(action)
  memory[[i]] = list(state = gridTiling(env$previous.state), action = action, 
    reward = env$reward, next.state = gridTiling(env$state))
}

# res = sarsa(env, fun.approx = "linear", preprocessState = gridTiling, 
#   replay.memory = memory, batch.size = 32, n.episodes = 30)
# print(res$steps)

# res = sarsa(env, fun.approx = "linear", preprocessState = gridTiling, 
#   replay.memory.size = 100, batch.size = 32, n.episodes = 30)
```

As a default experiences will be randomly sampled from the replay memory. A prioritized experience replay prioritizes experiences with a high error, where the error is for example in Sarsa given by

$$
R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t).
$$

Each entry of the replay memory $j$ has a priority $p_j$ , proportional to the probability of being sampled.

$$
j \tilde{} P(j) = \frac{p_j^\alpha}{\sum_i p_i^\alpha}.
$$

When $\alpha = 0$ (the default) experiences are sampled with equal probability else experiences with a high error have a higher probability of being sampled.
To each priority a small positive constant $\theta$ is added to prevent that experiences with an error of 0 are never replayed.

```{r}
# Prioritized experience replay
# res = sarsa(env, fun.approx = "linear", preprocessState = gridTiling,
#   replay.memory.size = 100, batch.size = 32, n.episodes = 30,
#   alpha = 0.5, theta = 0.05)
```

----

### TD

The TD algorithm is used to evaluate a fixed policy. It follows the policy to generate transitions and updates the state value function at each step by

$$
V(S_t) = V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) \right].
$$

The `td` function implements TD. It can be used with a tabular or linear function approximation and with eligibility traces. There is now one eligibility trace per state value (or weight) and its incrementally increased by

$$
E_t = (1 - \beta) E_{t-1} + 1
$$

and then the state value function is updated by

$$
V(s) = V(s) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) \right] E_t \quad \forall s \in \mathcal{S}.
$$

Afterwards the eilgibility traces are decayed by

$$
E_{t+1} = \gamma \lambda E_t.
$$

`td` takes a `policy` argument, which is the policy to evaluate. In the tabular case this is just a matrix (number of states x number of actions) with the probabilities of each action given a state. For the linear function approximation `policy` must be a function, which returns an action given a preprocessed state observation. You can specify a maximal number of steps or episodes, so `td` can be used with both continuing and episodic environments.

Here we will solve a random walk task.

```{r}
# Random Walk Task (Sutton & Barto Example 6.2)
P = array(dim = c(7, 7, 2))
P[, , 1] = matrix(c(rep(c(1, rep(0, 6)), 2), c(0, 1, rep(0, 5)), 
  c(0, 0, 1, rep(0, 4)), c(rep(0, 3), 1, rep(0, 3)), c(rep(0, 4), 1, rep(0, 2)), 
  c(rep(0, 6), 1)), ncol = 7, byrow = TRUE)
P[, , 2] = matrix(c(c(1, rep(0, 6)), c(0, 0, 1, rep(0, 4)), 
  c(rep(0, 3), 1, rep(0, 3)), c(rep(0, 4), 1, rep(0, 2)), 
  c(rep(0, 5), 1, 0), c(rep(0, 6), 1), c(rep(0, 6), 1)), ncol = 7, byrow = TRUE)
R = matrix(c(rep(0, 12), 1, 0), ncol = 2)
env = makeEnvironment(transitions = P, rewards = R, initial.state = 3)

# Uniform random policy
random.policy = matrix(1 / env$n.actions, nrow = env$n.states, 
  ncol = env$n.actions)

# Estimate state value function with TD(0)
res = td(env, random.policy, n.episodes = 100, lambda = 0.5)
print(res$V)
```
Note that the state values fluctuate around the true values due to the learning rate, which changes the estimates in the direction of the last transition.

----

### Dynamic Programming

Dynamic programming is a class of solution methods solving a MDP not by interaction but by iterative computations using the state transition array and reward matrix. It can therefore only be applied when the model of the MDP is known.

Iterative policy evaluation evaluates a policy by applying the following iterative update for every state

$$
v_{k+1}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left[\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_k(s')\right].
$$

In R we can use

```{r}
# Set up gridworld problem
env = gridworld()
  
# Define uniform random policy, take each action with equal probability
random.policy = matrix(1 / env$n.actions, nrow = env$n.states, 
  ncol = env$n.actions)

# Evaluate this policy
res = evaluatePolicy(env, random.policy, precision = 0.01)
print(round(matrix(res$v, ncol = 4, byrow = TRUE)))
```

In theory it converges to the true values, but in practise we have to stop iteration before that. You can either specify a maximal number of iterations via the `n.iter` argument or a `precision` term, then the evaluation stops if the change in two subsequent values is less than `precision` for every state. You can specify an initial value function via the `v` argument. Note that the values of all terminal states must be 0 else the algorithm does not work.

Policy iteration tries to find the best policy in the MDP by iterating between evaluating and improving a policy. The update cycle is

$$
\pi_1 \rightarrow v_{\pi_1} \rightarrow \pi_2 \rightarrow v_{\pi_2} \rightarrow \; ... \; \rightarrow \pi_* \rightarrow v_{\pi_*}.
$$

The policy improvement step finds the best action by acting greedily with respect to the value function of the previous policy

$$
\pi'(a | s) = \left\{ \begin{array}{ll} 1, & \text{if} \; a = \text{argmax}_{a \in \mathcal{A}} (\mathcal{R}_s^a + \gamma P_{ss'}^a \, V(s'))  \\
                                        0, & \text{else.} \end{array} \right.
$$

Here is an example finding the optimal value function and policy in the gridworld using `iteratePolicy`.

```{r}
# Find optimal policy using Policy Iteration
res = iteratePolicy(env)
print(round(matrix(res$v, ncol = 4, byrow = TRUE)))
```

You can specify an initial policy else the initial policy will be the uniform random policy.

`iteratePolicy` stops if the policy does not change in two subsequent iterations or if the specified number of iterations is exhausted. For the policy evaluation step in policy iteration the same stop criteria as in `evaluatePolicy` are applied via the `precision.eval` and `n.iter.eval` can be passed on.

Value iteration evaluates each policy only once and then immediately improves the policy by acting greedily

$$
V_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \left[ \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a V_k(s') \right].
$$

```{r}
# Find optimal policy using Value Iteration
res = iterateValue(env, n.iter = 100)
print(res$policy)
```

`iterateValue` runs until the improvement in the value function in two subsequent steps is smaller than the given precision in all states or if the specified number of iterations is exhausted.

`evaluatePolicy`, `iteratePolicy` and `iterateValue` return a list with state value function, action value function and policy.

----

### Actor Critic

An actor critic is a policy-based reinforcement learning algorithm. It uses both parametrized policy and value function. In `reinforcelearn` a TD actor critic is implemented, which uses a state value function as a critic.

The policy can be a softmax policy for discrete actions or a gaussian policy for a continuous action space.

The parameters of the critic are updated (for linear function approximation) by

$$
w_{t+1} = w_t + \beta [ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] x(S_t).
$$

and the policy's parameter by

$$
\theta_{t+1} = \theta_t + \alpha \gamma^t [ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] \nabla_\theta \log \pi(A_t | S_t, \theta),
$$

where $\nabla_\theta \log \pi(A_t | S_t, \theta)$ is the gradient of the log policy. There are now two learning rates $\alpha$ and $\beta$.

```{r}
env = MountainCar()

# Linear function approximation and softmax policy
res = actorCritic(env, fun.approx = "linear", 
  preprocessState = gridTiling, n.episodes = 30)
print(res$steps)
```

With a gaussian policy we can also solve problems with a continuous action space.

Here we will solve a continuous version of the mountain car problem, where the action is a real number.

```{r}
# Mountain Car with continuous action space
env = MountainCar(action.space = "Continuous")

# Linear function approximation and gaussian policy
set.seed(123)
res = actorCritic(env, fun.approx = "linear", policy = "gaussian", 
  preprocessState = gridTiling, n.episodes = 20)
print(res$steps)
```

----

#### Multi-armed Bandit

A multi-armed bandit is a simplified reinforcement learning problem with an episode which consists only of one step. In the simplest form there are no states. The goal is to estimate the values of all actions and to find the best action by trying these out over a number of episodes.

In the following we will consider an example bandit with four different actions. For each action the reward will be sampled from a probability distribution. The reward of the first action is sampled from a normal distribution with mean 1 and standard deviation 1, the second action from a normal distribution with mean 2 and standard deviation 4, the third action from a uniform distribution with minimum 0 and maximum 5 and the fourth action from an exponential distribution with rate parameter 0.25. Therefore the fourth action is the best with an expected reward of 4.

To solve this bandit problem we need to specify the reward function,

```{r, eval = FALSE}
# Define reward function
step = function(action) {
  if (action == 0) {
    reward = rnorm(1, mean = 1, sd = 1)
  }
  if (action == 1) {
    reward = rnorm(1, mean = 2, sd = 4)
  }
  if (action == 2) {
    reward = runif(1, min = 0, max = 5)
  }
  if (action == 3) {
    reward = rexp(1, rate = 0.25)
  }
  reward
}
```

To solve the bandit, i.e. to find out, which action returns the most reward, we can use the `solveBandit` function. There are several different action selection methods implemented, e.g. `greedy`, `epsilon-greedy`, `UCB` and `gradient-bandit`.

```{r, eval = FALSE}
solveBandit(step, n.actions = 4, n.episodes = 1000, 
  action.selection = "egreedy")
```

In the `solveBandit` function we can specify the argument `initial.value` which sets all Q values initially to this number. Additionally we can assign a confidence to this initial value via the `initial.visits` argument. A value of 10 for example means that the algorithm has already seen 10 rewards for each action with an average value of `initial.value`.

```{r, eval = FALSE}
solveBandit(step, n.actions = 4, n.episodes = 1000, 
  action.selection = "greedy", 
  initial.value = 5, initial.visits = 100)
```

An epsilon-greedy policy can be used as well.

```{r, eval = FALSE}
solveBandit(step, n.actions = 4, n.episodes = 1000, 
  action.selection = "epsilon-greedy", epsilon = 0.5)
```

In the following we will decrease epsilon every 100 episodes by a factor of 0.5.
```{r, eval = FALSE}
solveBandit(step, n.actions = 4, n.episodes = 1000, 
  action.selection = "epsilon-greedy", epsilon = 0.5,
  epsilon.decay = 0.5, epsilon.decay.after = 100)
```

The schedule after which epsilon is decreased can be controled by the `epsilon.decay` and `epsilon.decay.after` parameters. After every `epsilon.decay.after` episodes epsilon will be multiplied with `epsilon.decay`, usually a value between 0 and 1, and therefore decreases over time.

The estimates come close to the true values and will therefore correctly estimate the 4th action as the best one.

```{r, eval = FALSE}
solveBandit(step, n.actions = 4, n.episodes = 1000, 
  action.selection = "UCB", C = 2)
```

The Bandit algorithm is implemented in the following way. Action values are increased incrementally which is computationally cheaper as storing all rewards in a table.

In R:

```{r, eval = FALSE}
solveBandit(step, n.actions = 4, n.episodes = 10000, 
  action.selection = "gradient-bandit", alpha = 0.1)
```

---

Have a look at the other vignettes: [Introduction to reinforcelearn](introduction.html) and [How to create an environment?](environments.html).

Author: Markus Dumke
