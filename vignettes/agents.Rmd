---
title: "Agents"
author: Markus Dumke
date: "`r Sys.Date()`"
output:rmarkdown::html_vignette:
fig_caption: yes
bibliography: references.bib
vignette: >
 %\VignetteIndexEntry{Vignette Title}
 %\VignetteEngine{knitr::rmarkdown}
 %\VignetteEncoding{UTF-8}
---
  
<style type="text/css">
  h1.title {
  font-size: 34px;
  }
</style>
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(message = TRUE, eval = TRUE, collapse = TRUE, comment = "#>")
```

```{r}
library(reinforcelearn)
```


## Policies

A policy is the agent's behavior function. We can define the policy with `makePolicy`.

```{r}
# Uniform random policy
policy = makePolicy("random")

# Epsilon-greedy policy
policy = makePolicy("epsilon.greedy", epsilon = 0.2)

# Softmax policy
policy = makePolicy("softmax")
```

## Value Functions



## Algorithms


### Experience replay



### Eligibility traces

Eligibility traces allow to pass the current error back to all the previously visited states based on their eligibility, a measure of how long ago these states have been visited. This often leads to faster convergence.

Currently eligibility traces can be used with Q-Learning. Therefore you need to specify the trace decay parameter $\lambda$ and the type of eligibility traces, `"replace"` or `"accumulate"` traces.

```{r}
alg = makeAlgorithm("qlearning", lambda = 0.8, traces = "accumulate")
```

## Process a state


## Agent

```{r}

```

## Interaction

