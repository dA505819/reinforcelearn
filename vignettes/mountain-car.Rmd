---
title: "Solve MountainCar using Q-Learning with function approximation"
author: "Markus Dumke"
date: "`r Sys.Date()`"
output:rmarkdown::html_vignette:
fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
## MountainCar

MountainCar is a classical reinforcement learning problem. The goal is to drive a car up a hill. The car cannot drive the hill up in a single pass, so it first needs to go backwards, build up momentum and then go forwards into the goal. The state space is characterized by velocity and position, each continuous within certain bounds. Actions include throttle backward, forward or not at all (- 1, 0, + 1), so the action space is discrete.

<div class="figure">
<img src="mountain_car.JPG"/>
<p class="caption">Mountain Car environment</p>
</div>

In the following we will try to solve a simplified version of the problem, which allows 10000 steps per episode.

## Q-Learning

Q-Learning is an off-policy control algorithm using temporal-difference learning. Given a state and action pair the next state $S'$ and Reward $R$ are sampled from the environment. Then we take the max of the Q values of all possible successor actions. The action value of $S$ and $A$ is then updated towards the observed reward plus a discounted (discount factor $\gamma$) Q value of the next state $S'$ and best action $a$.

The update equation is: 

$Q(S, A) \leftarrow  Q(S, A) + \alpha[R + \gamma \max_a(Q(S', a)) - Q(S, A)]$

We will use the Gym implementation of the MountainCar environment. Therefore we need to install python and gym-http-api first. Then we can load the `reinforcelearn` package and create the environment.

```{r, eval = FALSE}
library(reinforcelearn)
MountainCar = makeEnvironment("MountainCar-v0")
```

Then we can apply Q-Learning to find the optimal policy. Because the state space is continuous, we will use a function approximator to represent Q values of states and actions. Here we use a very simple grid across the two-dimensional state space and return a one-hot vector representing in which grid cell the agent is. Then we can easily apply Q-Learning to the task. We need to pass on the function which returns the feature vector for a state observation and a predict and train function from the function approximator.

```{r, results = "hide", eval = FALSE}


```



```{r, eval = FALSE}
plot(1:100, res$steps.per.episode[1:100], ylim = c(0, 1000), 
   type = "l", xlab = "Episode", ylab = "Steps per Episode")
```


plot cost-to-go function
plot episode reward over time


## More Reinforcement Learning in R:

Learn more about reinforcement learning in R: [reinforcelearn](https://github.com/markdumke/reinforcelearn)

## Bibliography

Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
