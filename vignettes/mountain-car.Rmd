---
title: "Solve MountainCar using Q-Learning with linear function approximation"
author: "Markus Dumke"
date: "`r Sys.Date()`"
output:rmarkdown::html_vignette:
fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
## MountainCar
  
We will use the Gym MountainCar environment. Therefore we need to install python, gym and gym-http-api first.

After that we open a command line tool, go to the folder gym-http-api and run 

```{python, eval = FALSE}
python gym_http_server.py
```

<div class="figure">
<img src="mountain_car.JPG"/>
<p class="caption">Mountain Car environment</p>
</div>

## SARSA

SARSA is an off-policy control algorithm using temporal-difference learning. Given a state and action pair the next state $S'$ and Reward $R$ are sampled from the environment. Then following an $\epsilon$-greedy policy the next action $A'$ is sampled. The action value of $S$ and $A$ is then updated towards the observed reward plus a discounted (discount factor $\gamma$) of the subsequent state action pair $(S', A')$.

The update equation is: 

$Q(S, A) \leftarrow  Q(S, A) + \alpha[R + \gamma Q(S', A') - Q(S, A)]$

In the following we will solve the windy gridworld problem, i.e. find the shortest path from the start state to the goal state. First we load the `reinforcelearn` package and create the environment. 

```{r, eval = FALSE}
library(reinforcelearn)
MountainCar = makeEnvironment("MountainCarEasy-v0")
```

Then we can apply SARSA to find the optimal action value function $Q$. Here we will use SARSA(0), which only updates the current state action pair. Try different $\lambda$ values for a trade-off between temporal-difference and monte carlo learning.

```{r, results = "hide", eval = FALSE}
res = sarsa(MountainCar, lambda = 0, n.steps = 10000)
```

plot cost-to-go function
plot episode length
plot episode reward over time

gif einf√ºgen


## More Reinforcement Learning in R:

Learn more about reinforcement learning in R: [reinforcelearn](https://github.com/markdumke/reinforcelearn)

## Bibliography

[Sutton and Barto (2017)](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=249)
