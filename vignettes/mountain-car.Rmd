---
title: "Solve MountainCar using Q-Learning with linear function approximation"
author: "Markus Dumke"
date: "`r Sys.Date()`"
output:rmarkdown::html_vignette:
fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
## MountainCar

MountainCar is a classical reinforcement learning problem. The goal is to drive a car up a hill. The car cannot drive the hill up in a single pass, so it first needs to go backwards, build up momentum and then go forwards into the goal. The state space is characterized by velocity and position, each continuous within certain bounds. Actions include throttle backward, forward or not at all (- 1, 0, + 1), so the action space is discrete.

<div class="figure">
<img src="mountain_car.JPG"/>
<p class="caption">Mountain Car environment</p>
</div>

In the following we will try to solve a simplified version of the problem, which allows 10000 steps per episode.

## Q-Learning

Q-Learning is an off-policy control algorithm using temporal-difference learning. Given a state and action pair the next state $S'$ and Reward $R$ are sampled from the environment. Then we take the max of the Q values of all possible successor actions. The action value of $S$ and $A$ is then updated towards the observed reward plus a discounted (discount factor $\gamma$) Q value of the next state $S'$ and best action $a$.

The update equation is: 

$Q(S, A) \leftarrow  Q(S, A) + \alpha[R + \gamma \max_a(Q(S', a)) - Q(S, A)]$

We will use the Gym implementation of the MountainCar environment. Therefore we need to install python and gym-http-api first. Then we can load the `reinforcelearn` package and create the environment. First we need to set the path to the gym-http-api folder.

```{r, eval = FALSE}
library(reinforcelearn)
options(gym.api.path = "C:/Users/M/Downloads/WinPython-64bit-3.6.0.1Qt5/scripts/gym-http-api")
MountainCar = makeEnvironment("MountainCarEasy-v0", max.steps.episode = 10000)
```

Then we can apply Q-Learning to find the optimal policy. Because the state space is continuous, we will use linear function approximation to represent the states. Here we use a very simple grid across the two-dimensional state space and return a one-hot vector representing in which grid cell the agent is. Then we can easily apply Q-Learning to the task. We need to pass on the function which creates the feature vector from the state observation.

```{r, results = "hide", eval = FALSE}
# weights = qlearning_approx(MountainCar, make_feature_vector, n.features = 10, 
#    state.space.bounds = MountainCar$state.space.bounds, n.grid = 10, n.episodes = 10)
```

plot cost-to-go function
plot episode length
plot episode reward over time

gif einf√ºgen


## More Reinforcement Learning in R:

Learn more about reinforcement learning in R: [reinforcelearn](https://github.com/markdumke/reinforcelearn)

## Bibliography

[Sutton and Barto (2017)](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=249)
