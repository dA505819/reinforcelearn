---
title: "Reinforcement Learning"
author: "Markus Dumke"
date: "`r Sys.Date()`"
output:rmarkdown::html_vignette:
fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette explains the basic functionality of the `reinforcelearn` package. The package can be installed via

```{r, eval = FALSE}
install.packages("devtools")
devtools::install_github("markdumke/reinforcelearn")
```

## What is Reinforcement Learning

Reinforcement Learning is a field of Machine Learning adressing the problem of optimal decision making. The problem could be anything from mastering a complex game to controlling the behaviour of a robot.

RL problems can be formulated as an interaction between an agent and an environment over time. At each time step the agent can choose one action from a set of possible actions and gets rewarded by the environment with a numeric value. The goal is to maximize the sum of rewards over time. The state of the environment tells something about the problem, e.g. the position of all pawns on a chess board, and can be used from the agent as a basis for the action selection.

## Set up an environment

First we need to set up a reinforcement learning environment. Therefore we need to specify the transition and reward dynamics of the problem. The state transition array is a three-dimensional array specifying all transition probabilities between the states for each action. The reward matrix specifies the rewards obtained for each state-action combination. We can then simply pass these on to the `makeEnvironment` function.

```{r}
library(reinforcelearn)
```

```{r}
t = array(c(0.5, 0, 0.5, 1, 0.2, 0, 0.8, 1), c(2, 2, 2))
r = matrix(c(- 1, 0, - 1, 0), ncol = 2)
Env = makeEnvironment(transition.array = t, reward.matrix = r)
```

Another possibility is to use one of the existing OpenAI Gym environments: [https://gym.openai.com/docs](https://gym.openai.com/docs)

Using one of these environments is then as simple as

```{r, eval = FALSE}
MCar = makeEnvironment("MountainCar-v0")
```

Note that you need to have the prerequisites Python and Gym installed.

The environment created is now an R6 class with a set of attributes and methods. There are attrbutes describing the properties of the state and action space, e.g.

```{r, eval = FALSE}
MCar$action.space # Discrete
MCar$state.space.bounds # [-1.2, 0.6], [-0.07, 0.07]
```

The `reset`, `step` and `close` method can be used to sample experience. Here is an example running a random agent for 200 steps on the MountainCar task.

```{r, eval = FALSE}
MCar$reset()
for (i in 1:200) {
  action = sample(MCar$actions, 1)
  MCar$step(action)
}
MCar$close()
```

## Solve an environment

Once you have created an environment you can then solve the environment, i.e. find the optimal way to behave (called policy) with one of the implemented algorithms, e.g. Sarsa or Q-Learning.

```{r, eval = FALSE}
res = sarsa(Env)
```

## Reinforcement Learning with function approximation

For many tasks the value function cannot be represented by a table, especially if state or action space are continuous. Therefore a function approximator like a neural network is needed. In the `reinforcelearn` package the algorithms can be very flexibly used with function approximation.

```{r, eval = FALSE}

```
  
## More Reinforcement Learning in R:

Learn more about reinforcement learning in R: [reinforcelearn](https://github.com/markdumke/reinforcelearn)

## Bibliography

Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
