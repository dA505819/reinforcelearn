---
title: "Solve windy gridworld problem using SARSA"
author: "Markus Dumke"
date: "`r Sys.Date()`"
output:rmarkdown::html_vignette:
fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
## Windy Gridworld Environment
  
The windy gridworld is a simple example from Sutton & Barto (2017). The goal is to move from a starting state (S) to a goal state (G) as fast as possible, because each step you receive a negative reward of -1 along the way. You can move along the grid using the standard moves (left, right, up and down). In each column of the grid a deterministic wind pushes you up a certain number of steps. E.g. if you are in the cell to the left of the goal state and go right, you will end up two cells above the goal.
 
<div class="figure">
<img src="windy_gridworld.PNG"/>
<p class="caption">Windy gridworld example, image from D.Silver</p>
</div>

## SARSA

SARSA is an on-policy control algorithm using temporal-difference learning. Given a state and action pair the next state $S'$ and Reward $R$ are sampled from the environment. Then following an $\epsilon$-greedy policy the next action $A'$ is sampled. The action value of $S$ and $A$ is then updated towards the observed reward plus a discounted (discount factor $\gamma$) of the subsequent state action pair $(S', A')$.

The update equation is: 

$Q(S, A) \leftarrow  Q(S, A) + \alpha[R + \gamma Q(S', A') - Q(S, A)]$

In the following we will solve the windy gridworld problem, i.e. find the shortest path from the start state to the goal state. First we load the `reinforcelearn` package and create the environment. 

```{r}
library(reinforcelearn)
grid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix, terminal.states = grid$terminal.states - 1, 
  initial.state = 30)
```

Then we can apply SARSA to find the optimal action value function $Q$. Here we will use SARSA(0), which only updates the current state action pair. Try different $\lambda$ values for a trade-off between temporal-difference and monte carlo learning.

```{r, results = "hide"}
# res = sarsa(WindyGridworld1, lambda = 0, n.steps = 10000)
# res = qlearning(WindyGridworld1, n.episodes = 100)
# Q = res$Q
```

Then we can find the optimal policy by taking the argmax over the $Q$ values.

```{r}
# optimal.policy = max.col(Q)
```

We can plot the number of time steps an episode takes. As expected the first episode takes quite a large number of time steps as we start wandering around randomly. With time we improve the action value function $Q$ and therefore the policy and need less time steps to finish an episode. The optimal policy takes 15 steps from the start state to the goal state. Because of the $\epsilon$-greedy policy we have chosen, we will still explore and fluctuate around these optimal 15 steps.

```{r}
# plot(x = res$time.steps.episode, y = seq_along(res$time.steps.episode), 
#  type = "l", xlim = c(0, 10000), ylab = "Episode", xlab = "Time steps", 
#  main = "Episodes completed per time step")
# plot(x = seq_along(res$time.steps.episode), y = diff(c(0, res$time.steps.episode)), 
#  type = "l", xlim = c(0, 100), ylab = "Episode length", xlab = "Episode", 
#  main = "Episode length over time")
```

Plot $Q_*$.

Graphically the optimal policy looks like:

<div class="figure">
<img src="optimal_policy.PNG"/>
<p class="caption">Optimal policy, image from Sutton and Barto (2017)</p>
</div>


## More Reinforcement Learning in R:

Learn more about reinforcement learning in R: [reinforcelearn](https://github.com/markdumke/reinforcelearn)

## Bibliography

[Sutton and Barto (2017)](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=156)
