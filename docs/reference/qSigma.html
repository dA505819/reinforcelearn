<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Q(sigma) / Q-Learning / Sarsa / Expected Sarsa — qSigma â€¢ reinforcelearn</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>
  
  
<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">reinforcelearn</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/algorithms.html">How to solve an environment?</a>
    </li>
    <li>
      <a href="../articles/environments.html">How to create an environment?</a>
    </li>
    <li>
      <a href="../articles/introduction.html">Introduction to reinforcelearn</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Q(sigma) / Q-Learning / Sarsa / Expected Sarsa</h1>
    </div>

    
    <p>Value-based reinforcement learning control algorithms.</p>
    

    <pre class="usage"><span class='fu'>qSigma</span>(<span class='no'>envir</span>, <span class='kw'>fun.approx</span> <span class='kw'>=</span> <span class='st'>"table"</span>, <span class='kw'>preprocessState</span> <span class='kw'>=</span> <span class='no'>identity</span>,
  <span class='kw'>model</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>initial.value</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.states</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>100</span>,
  <span class='kw'>sigma</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>target.policy</span> <span class='kw'>=</span> <span class='st'>"egreedy"</span>, <span class='kw'>lambda</span> <span class='kw'>=</span> <span class='fl'>0</span>,
  <span class='kw'>eligibility.type</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>learning.rate</span> <span class='kw'>=</span> <span class='fl'>0.1</span>, <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.1</span>,
  <span class='kw'>discount</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>double.learning</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>replay.memory</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>replay.memory.size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>batch.size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>alpha</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>theta</span> <span class='kw'>=</span> <span class='fl'>0.01</span>,
  <span class='kw'>updateEpsilon</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateSigma</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateLambda</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateAlpha</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateLearningRate</span> <span class='kw'>=</span> <span class='no'>identity2</span>)

<span class='fu'>qlearning</span>(<span class='no'>envir</span>, <span class='kw'>fun.approx</span> <span class='kw'>=</span> <span class='st'>"table"</span>, <span class='kw'>preprocessState</span> <span class='kw'>=</span> <span class='no'>identity</span>,
  <span class='kw'>model</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>initial.value</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.states</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>100</span>,
  <span class='kw'>lambda</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>eligibility.type</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>learning.rate</span> <span class='kw'>=</span> <span class='fl'>0.1</span>, <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.1</span>,
  <span class='kw'>discount</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>double.learning</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>replay.memory</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>replay.memory.size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>batch.size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>alpha</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>theta</span> <span class='kw'>=</span> <span class='fl'>0.01</span>,
  <span class='kw'>updateEpsilon</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateSigma</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateLambda</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateAlpha</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateLearningRate</span> <span class='kw'>=</span> <span class='no'>identity2</span>)

<span class='fu'>sarsa</span>(<span class='no'>envir</span>, <span class='kw'>fun.approx</span> <span class='kw'>=</span> <span class='st'>"table"</span>, <span class='kw'>preprocessState</span> <span class='kw'>=</span> <span class='no'>identity</span>,
  <span class='kw'>model</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>initial.value</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.states</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>100</span>,
  <span class='kw'>lambda</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>eligibility.type</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>learning.rate</span> <span class='kw'>=</span> <span class='fl'>0.1</span>, <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.1</span>,
  <span class='kw'>discount</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>double.learning</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>replay.memory</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>replay.memory.size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>batch.size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>alpha</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>theta</span> <span class='kw'>=</span> <span class='fl'>0.01</span>,
  <span class='kw'>updateEpsilon</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateSigma</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateLambda</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateAlpha</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateLearningRate</span> <span class='kw'>=</span> <span class='no'>identity2</span>)

<span class='fu'>expectedSarsa</span>(<span class='no'>envir</span>, <span class='kw'>fun.approx</span> <span class='kw'>=</span> <span class='st'>"table"</span>, <span class='kw'>preprocessState</span> <span class='kw'>=</span> <span class='no'>identity</span>,
  <span class='kw'>model</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>initial.value</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.states</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>100</span>,
  <span class='kw'>target.policy</span> <span class='kw'>=</span> <span class='st'>"egreedy"</span>, <span class='kw'>lambda</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>eligibility.type</span> <span class='kw'>=</span> <span class='fl'>0</span>,
  <span class='kw'>learning.rate</span> <span class='kw'>=</span> <span class='fl'>0.1</span>, <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.1</span>, <span class='kw'>discount</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>double.learning</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>replay.memory</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>replay.memory.size</span> <span class='kw'>=</span> <span class='fl'>1</span>,
  <span class='kw'>batch.size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>alpha</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>theta</span> <span class='kw'>=</span> <span class='fl'>0.01</span>, <span class='kw'>updateEpsilon</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateSigma</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateLambda</span> <span class='kw'>=</span> <span class='no'>identity2</span>,
  <span class='kw'>updateAlpha</span> <span class='kw'>=</span> <span class='no'>identity2</span>, <span class='kw'>updateLearningRate</span> <span class='kw'>=</span> <span class='no'>identity2</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a> Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>envir</th>
      <td><p>[<code>R6 class</code>] 
The reinforcement learning environment created by <code><a href='makeEnvironment.html'>makeEnvironment</a></code>.</p></td>
    </tr>
    <tr>
      <th>fun.approx</th>
      <td><p>[<code>character(1)</code>] 
How to represent the value function? Currently <code>"table"</code>, <code>"linear"</code>
and <code>"neural.network"</code> are supported.</p></td>
    </tr>
    <tr>
      <th>preprocessState</th>
      <td><p>[<code>function</code>] 
A function that takes the state observation returned from the environment as an input and
preprocesses this in a way the algorithm can work with it.</p></td>
    </tr>
    <tr>
      <th>model</th>
      <td><p>[<code>keras model</code>] 
A neural network model specified using the <code>keras</code> package.</p></td>
    </tr>
    <tr>
      <th>initial.value</th>
      <td><p>[<code>numeric</code>] 
Initial value function matrix or weight matrix.
If <code>NULL</code> weights will be initialized to 0.
Only used for tabular or linear function approximation.</p></td>
    </tr>
    <tr>
      <th>n.states</th>
      <td><p>[<code>integer(1)</code>] 
Number of states for tabular value function.
Only needed if the state space is continuous, but
will be transformed by <code>preprocessState</code>, so that a single integer value is returned.
Else the number of states is accessed from the <code>envir$n.states</code> argument.</p></td>
    </tr>
    <tr>
      <th>n.episodes</th>
      <td><p>[<code>integer(1)</code>] 
Number of episodes.</p></td>
    </tr>
    <tr>
      <th>sigma</th>
      <td><p>[<code>numeric(1) in [0, 1]</code>] 
Parameter of the Q(sigma) algorithm. It controls if the temporal-difference target
is equal to the sarsa target (for <code>sigma = 1</code>) or the expected sarsa target
(for <code>sigma = 0</code>). For intermediate values of <code>sigma</code> a weighted mean
between the two targets is used.</p></td>
    </tr>
    <tr>
      <th>target.policy</th>
      <td><p>[<code>character(1)</code>] 
Should the expected sarsa target be computed on-policy
using the epsilon-greedy behavior policy (<code>target.policy = "egreedy"</code>)
or off-policy using a greedy target policy (<code>target.policy = "greedy"</code>)?</p></td>
    </tr>
    <tr>
      <th>lambda</th>
      <td><p>[<code>numeric(1) in [0, 1]</code>] 
Eligibility trace decay parameter.</p></td>
    </tr>
    <tr>
      <th>eligibility.type</th>
      <td><p>[<code>numeric(1)</code>] 
Type of eligibility trace, use <code>eligibility.type = 1</code> for replacing traces,
<code>eligibility.type = 0</code> for accumulating traces or
intermediate values for a mixture between both.</p></td>
    </tr>
    <tr>
      <th>learning.rate</th>
      <td><p>[<code>numeric(1)</code>] 
Learning rate used for gradient descent.</p></td>
    </tr>
    <tr>
      <th>epsilon</th>
      <td><p>[<code>numeric(1) in [0, 1]</code>] 
Ratio of random exploration in epsilon-greedy action selection.</p></td>
    </tr>
    <tr>
      <th>discount</th>
      <td><p>[<code>numeric(1) in [0, 1]</code>] 
Discount factor.</p></td>
    </tr>
    <tr>
      <th>double.learning</th>
      <td><p>[<code>logical(1)</code>] 
Should double learning be used?</p></td>
    </tr>
    <tr>
      <th>replay.memory</th>
      <td><p>[<code>list</code>] 
Initial replay memory, which can be passed on.
Each list element must be a list containing <code>state</code>, <code>action</code>,
<code>reward</code> and <code>next.state</code>.</p></td>
    </tr>
    <tr>
      <th>replay.memory.size</th>
      <td><p>[<code>integer(1)</code>] 
Size of the replay memory. Only used if <code>replay.memory</code> is <code>NULL</code>. Then
the replay memory will be initially filled with experiences following a uniform random policy.</p></td>
    </tr>
    <tr>
      <th>batch.size</th>
      <td><p>[<code>integer(1)</code>] 
Batch size, how many experiences are sampled from the replay memory at each step?
Must be smaller than the size of the replay memory!</p></td>
    </tr>
    <tr>
      <th>alpha</th>
      <td><p>[<code>numeric(1) in [0, 1]</code>] 
If <code>alpha = 0</code> sampling from the replay memory will be uniform,
otherwise observations with a high error will be proportionally prioritized.</p></td>
    </tr>
    <tr>
      <th>theta</th>
      <td><p>[<code>numeric(1) in (0, 1]</code>] 
<code>theta</code> is a small positive constant that prevents the edge-case of transitions
in the replay memory not being revisited once their error is zero.</p></td>
    </tr>
    <tr>
      <th>updateEpsilon</th>
      <td><p>[<code>function</code>] 
A function that updates <code>epsilon</code>. It takes two arguments,
<code>epsilon</code> and the current number of episodes which are finished,
and returns the new <code>epsilon</code> value.</p></td>
    </tr>
    <tr>
      <th>updateSigma</th>
      <td><p>[<code>function</code>] 
A function that updates <code>sigma</code>. It takes two arguments,
<code>sigma</code> and the current number of episodes which are finished,
and returns the new <code>sigma</code> value.</p></td>
    </tr>
    <tr>
      <th>updateLambda</th>
      <td><p>[<code>function</code>] 
A function that updates <code>lambda</code>. It takes two arguments,
<code>lambda</code> and the current number
of episodes which are finished, and returns the new <code>lambda</code> value.</p></td>
    </tr>
    <tr>
      <th>updateAlpha</th>
      <td><p>[<code>function</code>] 
A function that updates <code>alpha</code>. It takes two arguments,
<code>alpha</code> and the current number
of episodes which are finished, and returns the new <code>alpha</code> value.</p></td>
    </tr>
    <tr>
      <th>updateLearningRate</th>
      <td><p>[<code>function</code>] 
A function that updates the learning rate. It takes two arguments, <code>learning.rate</code>
and the current number of episodes which are finished, and returns the new
<code>learning.rate</code> value.</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>[<code>list(4)</code>] 
  Returns the action value function or model parameters [<code>matrix</code>] and the
  return and number of steps per episode [<code>numeric</code>].
  For Double Learning both value functions will be returned.</p>
    
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>Q(sigma), Q-Learning, Expected Sarsa and Sarsa build a family of temporal-difference learning
algorithms, which can be used to find the optimal action value function using the principle
of generalized policy iteration.
Q(sigma) subsumes Q-Learning, Sarsa and Expected Sarsa as special cases.
For more details on the algorithm read De Asis et al. (2017).
Expected Sarsa can be used on-policy if <code>target.policy == "egreedy"</code> else it is identical
to Q-Learning.</p>
<p>When <code>fun.approx == "table"</code> the action value function will be represented as a matrix,
but you can also use a linear combination of features or a neural network
for function approximation.
For a neural network you need to pass on a keras model via the <code>model</code> argument.</p>
<p>The raw state observation returned from the environment must be preprocessed using
the <code>preprocessState</code> argument. This function takes the state observation as input and
returns a preprocessed state which can be directly used by the function approximator.
To use a tabular value function <code>preprocessState</code> must return an integer value between
[0, number of states - 1]. For linear function approximation the output of
<code>preprocessState</code> must be a matrix with one row. For a neural network you have to make
sure that the dimensions of the preprocessed state and the neural network match, so that
<code>model$predict(preprocessState(envir$state))</code> works.</p>
<p>Experience replay can be used by specifying a prefilled replay memory using the
<code>replay.memory</code> argument or by specifying the length of the replay memory,
which is then filled with experience using a random uniform policy.
Sampling can also be prioritized by the error as proposed by Schaul et al. (2016).</p>
<p>Note that eligibility traces cannot be used with experience replay.</p>
<p>The hyperparameters <code>epsilon</code>, <code>sigma</code>, <code>alpha</code>, <code>lambda</code> and
<code>learning.rate</code> can be changed over time. Therefore pass on functions that return a new
value of the hyperparameter. These updates will be applied after each episode. First argument of
the function must be the parameter itself, the second argument the current episode number.</p>
<p>For neural networks the options experience replay, eligibility traces and double learning
are currently not available.</p>
    
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>De Asis et al. (2017): Multi-step Reinforcement Learning: A Unifying Algorithm</p>
<p>Hasselt et al. (2010): Double Q-Learning</p>
<p>Mnih et al. (2013): Playing Atari with Deep Reinforcement Learning</p>
<p>Schaul et al. (2016): Prioritized Experience Replay</p>
<p>Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction</p>
    

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'># Solve Windy Gridworld</span>
<span class='no'>env</span> <span class='kw'>=</span> <span class='fu'><a href='windyGridworld.html'>windyGridworld</a></span>()

<span class='no'>res</span> <span class='kw'>=</span> <span class='fu'>qlearning</span>(<span class='no'>env</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>20</span>)</div><div class='output co'>#&gt; <span class='message'>Episode 1 finished after 794 steps with a return of -794</span></div><div class='output co'>#&gt; <span class='message'>Episode 2 finished after 1701 steps with a return of -1701</span></div><div class='output co'>#&gt; <span class='message'>Episode 3 finished after 746 steps with a return of -746</span></div><div class='output co'>#&gt; <span class='message'>Episode 4 finished after 468 steps with a return of -468</span></div><div class='output co'>#&gt; <span class='message'>Episode 5 finished after 319 steps with a return of -319</span></div><div class='output co'>#&gt; <span class='message'>Episode 6 finished after 352 steps with a return of -352</span></div><div class='output co'>#&gt; <span class='message'>Episode 7 finished after 366 steps with a return of -366</span></div><div class='output co'>#&gt; <span class='message'>Episode 8 finished after 256 steps with a return of -256</span></div><div class='output co'>#&gt; <span class='message'>Episode 9 finished after 131 steps with a return of -131</span></div><div class='output co'>#&gt; <span class='message'>Episode 10 finished after 136 steps with a return of -136</span></div><div class='output co'>#&gt; <span class='message'>Episode 11 finished after 172 steps with a return of -172</span></div><div class='output co'>#&gt; <span class='message'>Episode 12 finished after 232 steps with a return of -232</span></div><div class='output co'>#&gt; <span class='message'>Episode 13 finished after 219 steps with a return of -219</span></div><div class='output co'>#&gt; <span class='message'>Episode 14 finished after 202 steps with a return of -202</span></div><div class='output co'>#&gt; <span class='message'>Episode 15 finished after 40 steps with a return of -40</span></div><div class='output co'>#&gt; <span class='message'>Episode 16 finished after 85 steps with a return of -85</span></div><div class='output co'>#&gt; <span class='message'>Episode 17 finished after 158 steps with a return of -158</span></div><div class='output co'>#&gt; <span class='message'>Episode 18 finished after 200 steps with a return of -200</span></div><div class='output co'>#&gt; <span class='message'>Episode 19 finished after 120 steps with a return of -120</span></div><div class='output co'>#&gt; <span class='message'>Episode 20 finished after 140 steps with a return of -140</span></div><div class='input'><span class='fu'>print</span>(<span class='no'>res</span>$<span class='no'>steps</span>)</div><div class='output co'>#&gt;  [1]  794 1701  746  468  319  352  366  256  131  136  172  232  219  202   40
#&gt; [16]   85  158  200  120  140</div><div class='input'>
<span class='co'># State value function</span>
<span class='fu'>print</span>(<span class='fu'>matrix</span>(<span class='fu'>round</span>(<span class='fu'>apply</span>(<span class='no'>res</span>$<span class='no'>Q1</span>, <span class='fl'>1</span>, <span class='no'>max</span>), <span class='fl'>1</span>), <span class='kw'>ncol</span> <span class='kw'>=</span> <span class='fl'>10</span>, <span class='kw'>byrow</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>))</div><div class='output co'>#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
#&gt; [1,] -4.2 -4.4 -4.8 -5.6 -6.3 -6.7 -6.7 -6.2 -5.5  -4.7
#&gt; [2,] -3.9 -4.0 -4.1 -4.4 -4.0 -3.4 -1.8 -3.4 -3.8  -3.8
#&gt; [3,] -3.6 -3.5 -3.4 -3.3 -2.6 -1.3 -0.3 -1.8 -2.7  -3.0
#&gt; [4,] -3.3 -3.1 -2.8 -2.5 -1.4 -0.4  0.0  0.0 -1.8  -2.2
#&gt; [5,] -2.8 -2.6 -2.2 -1.7 -0.7 -0.2  0.0 -0.2 -0.8  -1.5
#&gt; [6,] -2.5 -2.2 -1.7 -1.2 -0.4  0.0  0.0  0.0 -0.5  -0.9
#&gt; [7,] -2.2 -1.9 -1.4 -0.8  0.0  0.0  0.0  0.0 -0.1  -0.5</div><div class='input'>
<span class='co'># Policy</span>
<span class='no'>policy</span> <span class='kw'>=</span> <span class='fu'>max.col</span>(<span class='no'>res</span>$<span class='no'>Q1</span>) - <span class='fl'>1L</span>
<span class='fu'>print</span>(<span class='fu'>matrix</span>(<span class='no'>policy</span>, <span class='kw'>ncol</span> <span class='kw'>=</span> <span class='fl'>10</span>, <span class='kw'>byrow</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>))</div><div class='output co'>#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
#&gt; [1,]    3    0    3    1    1    0    0    1    1     3
#&gt; [2,]    3    0    3    3    3    0    2    1    0     3
#&gt; [3,]    2    1    3    3    2    1    0    3    3     3
#&gt; [4,]    3    3    1    0    1    1    3    2    2     2
#&gt; [5,]    2    1    3    2    3    2    0    0    0     2
#&gt; [6,]    3    0    0    0    3    3    0    3    3     1
#&gt; [7,]    1    0    1    0    2    2    3    2    2     1</div><div class='input'>
<span class='co'># Decay epsilon over time. Each 10 episodes epsilon will be halfed.</span>
<span class='no'>decayEpsilon</span> <span class='kw'>=</span> <span class='kw'>function</span>(<span class='no'>epsilon</span>, <span class='no'>i</span>) {
  <span class='kw'>if</span> (<span class='no'>i</span> <span class='kw'>%%</span> <span class='fl'>10</span> <span class='kw'>==</span> <span class='fl'>0</span>) {
    <span class='no'>epsilon</span> <span class='kw'>=</span> <span class='no'>epsilon</span> * <span class='fl'>0.5</span>
  }
  <span class='no'>epsilon</span>
}

<span class='no'>res</span> <span class='kw'>=</span> <span class='fu'>expectedSarsa</span>(<span class='no'>env</span>, <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.5</span>, <span class='kw'>updateEpsilon</span> <span class='kw'>=</span> <span class='no'>decayEpsilon</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>20</span>)</div><div class='output co'>#&gt; <span class='message'>Episode 1 finished after 478 steps with a return of -478</span></div><div class='output co'>#&gt; <span class='message'>Episode 2 finished after 2727 steps with a return of -2727</span></div><div class='output co'>#&gt; <span class='message'>Episode 3 finished after 884 steps with a return of -884</span></div><div class='output co'>#&gt; <span class='message'>Episode 4 finished after 365 steps with a return of -365</span></div><div class='output co'>#&gt; <span class='message'>Episode 5 finished after 530 steps with a return of -530</span></div><div class='output co'>#&gt; <span class='message'>Episode 6 finished after 312 steps with a return of -312</span></div><div class='output co'>#&gt; <span class='message'>Episode 7 finished after 353 steps with a return of -353</span></div><div class='output co'>#&gt; <span class='message'>Episode 8 finished after 210 steps with a return of -210</span></div><div class='output co'>#&gt; <span class='message'>Episode 9 finished after 242 steps with a return of -242</span></div><div class='output co'>#&gt; <span class='message'>Episode 10 finished after 293 steps with a return of -293</span></div><div class='output co'>#&gt; <span class='message'>Episode 11 finished after 100 steps with a return of -100</span></div><div class='output co'>#&gt; <span class='message'>Episode 12 finished after 140 steps with a return of -140</span></div><div class='output co'>#&gt; <span class='message'>Episode 13 finished after 87 steps with a return of -87</span></div><div class='output co'>#&gt; <span class='message'>Episode 14 finished after 146 steps with a return of -146</span></div><div class='output co'>#&gt; <span class='message'>Episode 15 finished after 79 steps with a return of -79</span></div><div class='output co'>#&gt; <span class='message'>Episode 16 finished after 84 steps with a return of -84</span></div><div class='output co'>#&gt; <span class='message'>Episode 17 finished after 156 steps with a return of -156</span></div><div class='output co'>#&gt; <span class='message'>Episode 18 finished after 99 steps with a return of -99</span></div><div class='output co'>#&gt; <span class='message'>Episode 19 finished after 226 steps with a return of -226</span></div><div class='output co'>#&gt; <span class='message'>Episode 20 finished after 74 steps with a return of -74</span></div><div class='input'>
<span class='co'># Solve the Mountain Car problem using linear function approximation</span>
<span class='no'>m</span> <span class='kw'>=</span> <span class='fu'><a href='mountainCar.html'>mountainCar</a></span>()

<span class='co'># Define preprocessing function (we use grid tiling)</span>
<span class='no'>n.tilings</span> <span class='kw'>=</span> <span class='fl'>8</span>
<span class='no'>max.size</span> <span class='kw'>=</span> <span class='fl'>4096</span>
<span class='no'>iht</span> <span class='kw'>=</span> <span class='fu'><a href='tilecoding.html'>IHT</a></span>(<span class='no'>max.size</span>)

<span class='no'>position.max</span> <span class='kw'>=</span> <span class='no'>m</span>$<span class='no'>state.space.bounds</span><span class='kw'>[[</span><span class='fl'>1</span>]][<span class='fl'>2</span>]
<span class='no'>position.min</span> <span class='kw'>=</span> <span class='no'>m</span>$<span class='no'>state.space.bounds</span><span class='kw'>[[</span><span class='fl'>1</span>]][<span class='fl'>1</span>]
<span class='no'>velocity.max</span> <span class='kw'>=</span> <span class='no'>m</span>$<span class='no'>state.space.bounds</span><span class='kw'>[[</span><span class='fl'>2</span>]][<span class='fl'>2</span>]
<span class='no'>velocity.min</span> <span class='kw'>=</span> <span class='no'>m</span>$<span class='no'>state.space.bounds</span><span class='kw'>[[</span><span class='fl'>2</span>]][<span class='fl'>1</span>]
<span class='no'>position.scale</span> <span class='kw'>=</span> <span class='no'>n.tilings</span> / (<span class='no'>position.max</span> - <span class='no'>position.min</span>)
<span class='no'>velocity.scale</span> <span class='kw'>=</span> <span class='no'>n.tilings</span> / (<span class='no'>velocity.max</span> - <span class='no'>velocity.min</span>)

<span class='co'># Scale state first, then get active tiles and return n hot vector</span>
<span class='no'>gridTiling</span> <span class='kw'>=</span> <span class='kw'>function</span>(<span class='no'>state</span>) {
  <span class='no'>state</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='no'>position.scale</span> * <span class='no'>state</span>[<span class='fl'>1</span>], <span class='no'>velocity.scale</span> * <span class='no'>state</span>[<span class='fl'>2</span>])
  <span class='no'>active.tiles</span> <span class='kw'>=</span> <span class='fu'><a href='tilecoding.html'>tiles</a></span>(<span class='no'>iht</span>, <span class='fl'>8</span>, <span class='no'>state</span>)
  <span class='fu'><a href='nHot.html'>nHot</a></span>(<span class='no'>active.tiles</span>, <span class='no'>max.size</span>, <span class='kw'>out</span> <span class='kw'>=</span> <span class='st'>"vector"</span>)
}

<span class='fu'>set.seed</span>(<span class='fl'>123</span>)
<span class='no'>res</span> <span class='kw'>=</span> <span class='fu'>sarsa</span>(<span class='no'>m</span>, <span class='kw'>fun.approx</span> <span class='kw'>=</span> <span class='st'>"linear"</span>, <span class='kw'>preprocessState</span> <span class='kw'>=</span> <span class='no'>gridTiling</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>20</span>)</div><div class='output co'>#&gt; <span class='message'>Episode 1 finished after 840 steps with a return of -839</span></div><div class='output co'>#&gt; <span class='message'>Episode 2 finished after 784 steps with a return of -783</span></div><div class='output co'>#&gt; <span class='message'>Episode 3 finished after 684 steps with a return of -683</span></div><div class='output co'>#&gt; <span class='message'>Episode 4 finished after 694 steps with a return of -693</span></div><div class='output co'>#&gt; <span class='message'>Episode 5 finished after 318 steps with a return of -317</span></div><div class='output co'>#&gt; <span class='message'>Episode 6 finished after 475 steps with a return of -474</span></div><div class='output co'>#&gt; <span class='message'>Episode 7 finished after 270 steps with a return of -269</span></div><div class='output co'>#&gt; <span class='message'>Episode 8 finished after 274 steps with a return of -273</span></div><div class='output co'>#&gt; <span class='message'>Episode 9 finished after 235 steps with a return of -234</span></div><div class='output co'>#&gt; <span class='message'>Episode 10 finished after 199 steps with a return of -198</span></div><div class='output co'>#&gt; <span class='message'>Episode 11 finished after 276 steps with a return of -275</span></div><div class='output co'>#&gt; <span class='message'>Episode 12 finished after 160 steps with a return of -159</span></div><div class='output co'>#&gt; <span class='message'>Episode 13 finished after 213 steps with a return of -212</span></div><div class='output co'>#&gt; <span class='message'>Episode 14 finished after 304 steps with a return of -303</span></div><div class='output co'>#&gt; <span class='message'>Episode 15 finished after 216 steps with a return of -215</span></div><div class='output co'>#&gt; <span class='message'>Episode 16 finished after 235 steps with a return of -234</span></div><div class='output co'>#&gt; <span class='message'>Episode 17 finished after 224 steps with a return of -223</span></div><div class='output co'>#&gt; <span class='message'>Episode 18 finished after 180 steps with a return of -179</span></div><div class='output co'>#&gt; <span class='message'>Episode 19 finished after 153 steps with a return of -152</span></div><div class='output co'>#&gt; <span class='message'>Episode 20 finished after 222 steps with a return of -221</span></div><div class='input'><span class='fu'>print</span>(<span class='no'>res</span>$<span class='no'>returns</span>)</div><div class='output co'>#&gt;  [1] -839 -783 -683 -693 -317 -474 -269 -273 -234 -198 -275 -159 -212 -303 -215
#&gt; [16] -234 -223 -179 -152 -221</div><div class='input'>
</div><span class='co'># NOT RUN {</span>
<span class='no'>env</span> <span class='kw'>=</span> <span class='fu'><a href='gridworld.html'>gridworld</a></span>(<span class='fu'>c</span>(<span class='fl'>3</span>, <span class='fl'>3</span>), <span class='kw'>goal.states</span> <span class='kw'>=</span> <span class='fl'>8</span>, <span class='kw'>initial.state</span> <span class='kw'>=</span> <span class='fl'>0</span>)

<span class='co'># Use a neural network as function approximator</span>
<span class='no'>oneHot</span> <span class='kw'>=</span> <span class='kw'>function</span>(<span class='no'>state</span>) {
  <span class='no'>one.hot</span> <span class='kw'>=</span> <span class='fu'>matrix</span>(<span class='fu'>rep</span>(<span class='fl'>0</span>, <span class='fl'>9</span>), <span class='kw'>nrow</span> <span class='kw'>=</span> <span class='fl'>1</span>)
  <span class='no'>one.hot</span>[<span class='fl'>1</span>, <span class='no'>state</span> + <span class='fl'>1</span>] <span class='kw'>=</span> <span class='fl'>1</span>
  <span class='no'>one.hot</span>
}

<span class='co'># Define keras model</span>
<span class='fu'>library</span>(<span class='no'>keras</span>)
<span class='no'>model</span> <span class='kw'>=</span> <span class='fu'><a href='http://www.rdocumentation.org/packages/keras/topics/keras_model_sequential'>keras_model_sequential</a></span>()
<span class='no'>model</span> <span class='kw'>%&gt;%</span> <span class='fu'><a href='http://www.rdocumentation.org/packages/keras/topics/layer_dense'>layer_dense</a></span>(<span class='kw'>units</span> <span class='kw'>=</span> <span class='fl'>4</span>, <span class='kw'>activation</span> <span class='kw'>=</span> <span class='st'>'linear'</span>, <span class='kw'>input_shape</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='fl'>9</span>))

<span class='no'>res</span> <span class='kw'>=</span> <span class='fu'>qSigma</span>(<span class='no'>env</span>, <span class='kw'>fun.approx</span> <span class='kw'>=</span> <span class='st'>"neural.network"</span>, <span class='kw'>model</span> <span class='kw'>=</span> <span class='no'>model</span>,
  <span class='kw'>preprocessState</span> <span class='kw'>=</span> <span class='no'>oneHot</span>, <span class='kw'>n.episodes</span> <span class='kw'>=</span> <span class='fl'>20</span>)
<span class='co'># }</span><div class='input'>
</div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#details">Details</a></li>

      <li><a href="#references">References</a></li>
      
      <li><a href="#examples">Examples</a></li>
    </ul>

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Markus Dumke.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
