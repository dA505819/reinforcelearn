<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How to create an environment? • reinforcelearn</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">reinforcelearn</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/algorithms.html">How to solve an environment?</a>
    </li>
    <li>
      <a href="../articles/environments.html">How to create an environment?</a>
    </li>
    <li>
      <a href="../articles/introduction.html">Introduction to reinforcelearn</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/markdumke/reinforcelearn">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>How to create an environment?</h1>
            
          </div>

    
    
<div class="contents">
<style type="text/css">
h1.title {
  font-size: 34px;
}
</style>
<p>This vignette explains the different possibilities to create a reinforcement learning environment in <code>reinforcelearn</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(reinforcelearn)
<span class="kw">set.seed</span>(<span class="dv">1</span>)</code></pre></div>
<hr>
<div id="what-is-an-environment-in-reinforcelearn" class="section level2">
<h2 class="hasAnchor">
<a href="#what-is-an-environment-in-reinforcelearn" class="anchor"></a>What is an environment in reinforcelearn?</h2>
<p>Environments in <code>reinforcelearn</code> are implemented as <code>R6</code> classes with certain methods and attributes. The environment can then be passed on to the algorithms using the <code>envir</code> argument.</p>
<p>There are some attributes of the <code>R6</code> class, which are essential for the interaction between environment and agent:</p>
<ul>
<li><p><code>state</code>: The current state observation of the environment. Depending on the problem this can be anything, e.g. a scalar integer, a matrix or a list.</p></li>
<li><p><code>reward</code>: The current reward of the environment. It is always a scalar numeric value.</p></li>
<li><p><code>done</code>: A logical flag specifying whether an episode is finished.</p></li>
<li><p><code>n.steps</code>: Number of steps in the current episode. Will be reset to 0 when <code>reset</code> is called. Each time <code>step</code> is called it is increased by 1.</p></li>
</ul>
<p>The interaction between agent and environment is done via the <code>reset</code> and <code>step</code> methods:</p>
<ul>
<li><p><code>reset()</code>: Resets the environment, i.e. it sets the <code>state</code> attribute to a starting state and sets the <code>done</code> flag to <code>FALSE</code>. It is usually called at the beginning of an episode.</p></li>
<li><p><code>step(action)</code>: The basic interaction function between agent and environment. <code>step</code> is called with an action as an argument. It then takes the action and alters the <code>state</code> and <code>reward</code> attributes of the <code>R6</code> class. If the episode is done, e.g. a terminal state reached, the <code>done</code> flag is set to <code>TRUE</code>.</p></li>
</ul>
<p>Note: All states and actions are numerated starting with 0!</p>
<p>The <code>makeEnvironment</code> function provides different ways to create an environment. It takes care of the creation of an <code>R6</code> class with the above mentioned attributes and methods.</p>
<hr>
<div id="markov-decision-process" class="section level3">
<h3 class="hasAnchor">
<a href="#markov-decision-process" class="anchor"></a>Markov Decision Process</h3>
<p>A Markov Decision Process (MDP) is a stochastic process, which is commonly used for reinforcement learning environments. When the problem can be formulated as a MDP, all you need to pass to <code>makeEnvironment</code> is the state transition array <span class="math inline">\(P^a_{ss'}\)</span> and reward matrix <span class="math inline">\(R_s^a\)</span> of the MDP. The state transition array describes the probability of a transition from state <span class="math inline">\(s\)</span> to state <span class="math inline">\(s'\)</span> when taking action <span class="math inline">\(a\)</span>. It is a 3-dimensional array with dimensions [number of states x number of states x number of actions], so for each action there is one state transition matrix. The reward matrix has the dimensions [number of states x number of actions], each entry is the expected reward obtained from taking action <span class="math inline">\(a\)</span> in a state <span class="math inline">\(s\)</span>.</p>
<p>We can create a simple MDP with 2 states and 2 actions with the following code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># State transition array</span>
P =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))
P[, , <span class="dv">1</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>), <span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
P[, , <span class="dv">2</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.9</span>), <span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
<span class="kw">print</span>(P)
<span class="co">#&gt; , , 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]  0.5  0.5</span>
<span class="co">#&gt; [2,]  0.8  0.2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; , , 2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]  0.0  1.0</span>
<span class="co">#&gt; [2,]  0.1  0.9</span>
<span class="co"># Reward matrix</span>
R =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), <span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
<span class="kw">print</span>(R)
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]    5   10</span>
<span class="co">#&gt; [2,]   -1    2</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">rewards =</span> R)
<span class="co">#&gt; Warning in self$initializeMDP(transitions, rewards, initial.state, reset, :</span>
<span class="co">#&gt; There are no terminal states in the MDP!</span></code></pre></div>
<p>We will get a warning that there are no terminal states in the MDP, i.e. an episode never ends in this MDP. Some algorithms assume that there is a terminal state, so we have to be careful, when we want to solve this. A terminal state has a probability of 1 remaining in this state. Here is an example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">P =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))
P[, , <span class="dv">1</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
P[, , <span class="dv">2</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
<span class="kw">print</span>(P)
<span class="co">#&gt; , , 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]  0.5  0.5</span>
<span class="co">#&gt; [2,]  0.0  1.0</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; , , 2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]  0.1  0.9</span>
<span class="co">#&gt; [2,]  0.0  1.0</span>

env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">rewards =</span> R)
<span class="kw">print</span>(env<span class="op">$</span>terminal.states)
<span class="co">#&gt; [1] 1</span></code></pre></div>
<p>Every episode starts in some starting state. There are different ways to pass on the starting state in <code>makeEnvironment</code>. The simplest is to specify the <code>initial.state</code> argument with a scalar integer or an integer vector. When <code>initial.state</code> is a scalar, then every episode will start in this state. If <code>initial.state</code> is a vector then the starting state will be uniformly sampled from all elements of the vector. As a default the initial state will be sampled randomly from all non-terminal states.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">rewards =</span> R, <span class="dt">initial.state =</span> <span class="dv">0</span>)
env<span class="op">$</span><span class="kw">reset</span>()
<span class="kw">print</span>(env)
<span class="co">#&gt; Number of steps: 0 </span>
<span class="co">#&gt; State: 0 </span>
<span class="co">#&gt; Reward:  </span>
<span class="co">#&gt; Done: FALSE</span></code></pre></div>
<p>A different possibility is to specify a custom <code>reset</code> function, which takes no arguments and returns the starting state. This is a way to specify a custom probability distribution over starting states. If the starting state is a terminal state you will get a warning!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Specify a custom probability distribution for the starting state.</span>
reset =<span class="st"> </span><span class="cf">function</span>() {
  p =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.8</span>)
  <span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">prob =</span> p, <span class="dt">size =</span> <span class="dv">1</span>)
}
env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">rewards =</span> R, <span class="dt">reset =</span> reset)
env<span class="op">$</span><span class="kw">reset</span>()
<span class="co">#&gt; Warning in env$reset(): The starting state is a terminal state!</span>
<span class="kw">print</span>(env)
<span class="co">#&gt; Number of steps: 0 </span>
<span class="co">#&gt; State: 1 </span>
<span class="co">#&gt; Reward:  </span>
<span class="co">#&gt; Done: TRUE</span></code></pre></div>
<p>The reward argument can also be a three-dimensional array, i.e. the reward can also depend on the next state.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">R =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))
R[, <span class="dv">1</span>, ] =<span class="st"> </span><span class="dv">1</span>
R[<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>] =<span class="st"> </span><span class="dv">10</span>
<span class="kw">print</span>(R)
<span class="co">#&gt; , , 1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]    1    0</span>
<span class="co">#&gt; [2,]    1    0</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; , , 2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]    1    0</span>
<span class="co">#&gt; [2,]    1   10</span>

env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">rewards =</span> R)

env<span class="op">$</span><span class="kw">reset</span>()
env<span class="op">$</span><span class="kw">step</span>(<span class="dv">1</span>)
<span class="kw">print</span>(env)
<span class="co">#&gt; Number of steps: 1 </span>
<span class="co">#&gt; State: 1 </span>
<span class="co">#&gt; Reward: 0 </span>
<span class="co">#&gt; Done: TRUE</span></code></pre></div>
<p>Instead of specifying a reward array you can also pass on a function <code>sampleReward</code>, which takes three arguments, the current state, action and next state and returns a scalar numeric reward. This way the reward of taking an action can be stochastic. Here is a simple example, where the reward is either 0 or sampled from a normal distribution depending on the next state and action.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sampleReward =<span class="st"> </span><span class="cf">function</span>(state, action, n.state) {
  <span class="cf">if</span> (n.state <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>action <span class="op">==</span><span class="st"> </span>1L) {
    <span class="dv">0</span>
  } <span class="cf">else</span> {
    <span class="kw">rnorm</span>(<span class="dv">1</span>)
  }
}
env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">sampleReward =</span> sampleReward)
env<span class="op">$</span><span class="kw">reset</span>()
env<span class="op">$</span><span class="kw">step</span>(<span class="dv">0</span>)
<span class="kw">print</span>(env)
<span class="co">#&gt; Number of steps: 1 </span>
<span class="co">#&gt; State: 0 </span>
<span class="co">#&gt; Reward: 1.3297992629225 </span>
<span class="co">#&gt; Done: FALSE</span></code></pre></div>
<hr>
</div>
<div id="gridworlds" class="section level3">
<h3 class="hasAnchor">
<a href="#gridworlds" class="anchor"></a>Gridworlds</h3>
<p>A gridworld is a simple navigation task with a discrete state and action space. The agent has to move through a grid from a start state to a goal state. Each episode starts in the start state and terminates if the agent reaches a goal state. States are always numerated row-wise starting with 0. Possible actions are the standard moves (left, right, up, down) or could also include the diagonal moves (leftup, leftdown, rightup, rightdown).</p>
<p>If an action would take the agent off the grid, the next state will be the nearest cell inside the grid. For each step the agent gets a reward, e.g. - 1, until it reaches a goal state, then the episode is done.</p>
<p>Gridworlds with different shapes, rewards and transition dynamics can be created with the function <code>makeGridworld</code>. It computes the state transition array and reward matrix of the specified gridworld (because a gridworld is a MDP) and then internally calls <code>makeEnvironment</code>. Arguments from <code>makeEnvironment</code> can be passed on via the <code>...</code> argument, e.g. <code>initial.state</code>.</p>
<p>Here is an example of a 4x4 gridworld <span class="citation">(Sutton and Barto 2017, Example 4.1)</span> with the 4 standard actions and two terminal states in the lower right and upper left of the grid. Rewards are - 1 for every transition until reaching a terminal state.</p>
<p><img src="gridworld.JPG" width="300px" style="display: block; margin: auto;"></p>
<p>The following code creates this gridworld.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Gridworld Environment (Sutton &amp; Barto (2017) Example 4.1)</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="dt">goal.states =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">15</span>))
<span class="kw">print</span>(env<span class="op">$</span>states)
<span class="co">#&gt;  [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15</span>
<span class="kw">print</span>(env<span class="op">$</span>actions)
<span class="co">#&gt; [1] 0 1 2 3</span>

<span class="co"># Same gridworld, but with diagonal moves</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="dt">goal.states =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">15</span>), 
  <span class="dt">diagonal.moves =</span> <span class="ot">TRUE</span>)
<span class="kw">print</span>(env<span class="op">$</span>actions)
<span class="co">#&gt; [1] 0 1 2 3 4 5 6 7</span></code></pre></div>
<p>In the above gridworld actions will deterministically change the state, e.g. when going left from state 5 the new state will always be 4. A stochastic gridworld can be specified via the <code>stochasticity</code> argument. Then the next state will be randomly sampled from all eight successor state with a probability <code>stochasticity</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Gridworld with 10% random transitions</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="dt">goal.states =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">15</span>), <span class="dt">stochasticity =</span> <span class="fl">0.1</span>)</code></pre></div>
<p><img src="cliff.JPG" width="500px" style="display: block; margin: auto;"></p>
<p>The cliff walking gridworld <span class="citation">(Sutton and Barto 2017, Example 6.6)</span> has a cliff in the lower part of the grid. Stepping into this cliff will result in a high negative reward of -100 and a transition back to the starting state in the lower left part of the grid. So the agent has to learn to avoid stepping into this cliff. Other transitions have the usual reward of -1. The optimal path is directly above the cliff, while the safe path runs at the top of the gridworld far away from the dangerous cliff. The optimal path has a length of 13 steps.</p>
<p>In <code>makeGridworld</code> we can specify the cliff via the <code>cliff.states</code> argument and the reward when stepping into the cliff via <code>reward.cliff</code>. The states to which the agent transitions, when stepping into the cliff can be specified via <code>cliff.transition.states</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Cliff Walking (Sutton &amp; Barto (2017) Example 6.6)   </span>
cliff =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">12</span>), <span class="dt">goal.states =</span> <span class="dv">47</span>, 
  <span class="dt">cliff.states =</span> <span class="dv">37</span><span class="op">:</span><span class="dv">46</span>, <span class="dt">reward.step =</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">reward.cliff =</span> <span class="op">-</span><span class="st"> </span><span class="dv">100</span>, 
  <span class="dt">cliff.transition.states =</span> <span class="dv">36</span>, <span class="dt">initial.state =</span> <span class="dv">36</span>)</code></pre></div>
<p><img src="windygrid.PNG" width="350px" style="display: block; margin: auto;"></p>
<p>The windy gridworld <span class="citation">(Sutton and Barto 2017, Example 6.5)</span> is a gridworld with shape 7x10. The agent will be pushed up a number of cells when transitioning into a column with an upward wind. The <code>wind</code> argument specifies the strength of this wind. It is an integer vector with the same size as the number of columns in the grid. E.g. going right from the state directly left to the goal, will push the agent to a state two cells above the goal. The reward for each step is -1. The optimal path has a length of 15 steps.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Windy Gridworld (Sutton &amp; Barto (2017) Example 6.5) </span>
windy.gridworld =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">10</span>), <span class="dt">goal.states =</span> <span class="dv">37</span>, 
  <span class="dt">reward.step =</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">wind =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">initial.state =</span> <span class="dv">30</span>)</code></pre></div>
<hr>
</div>
<div id="openai-gym-environments" class="section level3">
<h3 class="hasAnchor">
<a href="#openai-gym-environments" class="anchor"></a>OpenAI Gym Environments</h3>
<p>OpenAI Gym <span class="citation">(Brockman et al. 2016)</span> is a toolkit for developing and comparing reinforcement learning algorithms. It provides a set of environments, which can be used as benchmark problems. The environments are implemented in Python and can be acessed via the OpenAI Gym API. Have a look at <a href="https://gym.openai.com/envs" class="uri">https://gym.openai.com/envs</a> for possible environments. To use this in R you need to install the dependencies listed <a href="https://github.com/openai/gym-http-api">here</a>. You also need to install the R package <code>gym</code> <span class="citation">(Hendricks 2016)</span>.</p>
<p>Then it is simple to use one of the existing OpenAI Gym environments. First you need to start a server via the <code>gym</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create an OpenAI Gym environment.</span>
<span class="co"># Make sure you have Python and Gym installed.</span>
<span class="co"># Start server and create gym client.</span>
package.path =<span class="st"> </span><span class="kw">system.file</span>(<span class="dt">package =</span> <span class="st">"reinforcelearn"</span>)
path2pythonfile =<span class="st"> </span><span class="kw">paste0</span>(package.path, <span class="st">"/gym_http_server.py"</span>)
<span class="kw">system2</span>(<span class="st">"python"</span>, <span class="dt">args =</span> path2pythonfile, <span class="dt">stdout =</span> <span class="ot">NULL</span>, <span class="dt">wait =</span> <span class="ot">FALSE</span>)
client =<span class="st"> </span>gym<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/gym/topics/create_GymClient">create_GymClient</a></span>(<span class="st">"http://127.0.0.1:5000"</span>)
instance.id =<span class="st"> </span>gym<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/gym/topics/env_create">env_create</a></span>(client, <span class="st">"MountainCar-v0"</span>)

env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="kw">list</span>(client, instance.id))</code></pre></div>
<p>The <code>render</code> argument specifies whether to render the environment. If <code>render = TRUE</code> a Python window will open showing a graphical interface of the environment when calling the <code>step</code> method.</p>
<p>The <code>reset</code>, <code>step</code> and <code>close</code> method can then be used to sample experience. Here is an example running a random agent for 200 steps on the mountain car task.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env<span class="op">$</span><span class="kw">reset</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">200</span>) {
  action =<span class="st"> </span><span class="kw">sample</span>(env<span class="op">$</span>actions, <span class="dv">1</span>)
  env<span class="op">$</span><span class="kw">step</span>(action)
}
env<span class="op">$</span><span class="kw">close</span>()</code></pre></div>
<p>You should see a window opening showing the graphical interface.</p>
<p><img src="mountaincar.JPG" width="300px" style="display: block; margin: auto;"></p>
<hr>
</div>
<div id="create-your-own-environment" class="section level3">
<h3 class="hasAnchor">
<a href="#create-your-own-environment" class="anchor"></a>Create your own environment</h3>
<p>Some reinforcement learning problems cannot be formulated in the above way. Then it is necessary to create the environment yourself and pass it on to the algorithms. Make sure, that the environment is an <code>R6 class</code> with the necessary attributes and methods.</p>
<p>Here is a full list describing all attributes of the <code>R6</code> class created by <code>makeEnvironment</code>. Depending on the algorithm different of these attributes and methods may be necessary, e.g. <code>terminal.states</code>, <code>rewards</code> and <code>transitions</code> for model-based dynamic programming or <code>step</code>, <code>reset</code>, <code>state</code>, <code>previous.state</code>, <code>n.steps</code>, <code>reward</code> and <code>done</code>, if using a model-free algorithm.</p>
<p><strong>Attributes</strong>:</p>
<ul>
<li><p><code>state</code>: The current state observation of the environment. Depending on the problem this can be anything, e.g. a scalar integer, a matrix or a list.</p></li>
<li><p><code>reward</code>: The current reward of the environment. It is always a scalar numeric value.</p></li>
<li><p><code>done</code>: A logical flag specifying whether an episode is finished.</p></li>
<li><p><code>n.steps</code>: Number of steps in the current episode. Will be reset to 0 when <code>reset</code> is called. Each time <code>step</code> is called it is increased by 1.</p></li>
<li><p><code>previous.state</code>: The previous state of the environment. This is often the state which is updated in a reinforcement learning algorithm.</p></li>
<li><p><code>state.space</code>: One of <code>Discrete</code> or <code>Box</code>.</p></li>
<li><p><code>state.shape</code>: Number of state variables in a continuous state space.</p></li>
<li><p><code>state.space.bounds</code>: The bounds of a boxed state space, a list with lower and upper bound for each state variable as one list element.</p></li>
<li><p><code>states</code>: States (in a discrete state space). Numerated starting with 0.</p></li>
<li><p><code>terminal.states</code>: Terminal states in a Markov Decision Process. Will be derived from the <code>transition</code> argument in <code>makeEnvironment</code>.</p></li>
<li><p><code>n.states</code>: Number of states (for a discrete state space).</p></li>
<li><p><code>action.space</code>: One of <code>Discrete</code> or <code>Box</code>.</p></li>
<li><p><code>action.shape</code>: Number of action variables in a continuous action space.</p></li>
<li><p><code>action.space.bounds</code>: The bounds of a boxed action space, a list with lower and upper bound for each action variable as one list element.</p></li>
<li><p><code>actions</code>: Actions (in a discrete action space). Numerated starting with 0.</p></li>
<li><p><code>n.actions</code>: Number of actions (for a discrete action space).</p></li>
<li><p><code>transitions</code>: A state transition array (n.states x n.states x n.actions).</p></li>
<li><p><code>rewards</code>: A state reward matrix (n.states x n.actions).</p></li>
</ul>
<p><strong>Methods</strong>:</p>
<ul>
<li><p><code>reset()</code>: Resets the environment, i.e. it sets the <code>state</code> attribute to a starting state and sets the <code>done</code> flag to <code>FALSE</code>. It is usually called at the beginning of an episode.</p></li>
<li><p><code>step(action)</code>: The basic interaction function between agent and environment. <code>step</code> is called with an action as an argument. It then takes the action and alters the <code>state</code> and <code>reward</code> attributes of the <code>R6</code> class. If the episode is done, e.g. a terminal state reached, the <code>done</code> flag is set to <code>TRUE</code>.</p></li>
<li><p><code>done()</code>: When using an OpenAI Gym environment this method closes the python window. Else it returns the <code>R6</code> class object unchanged, i.e. <code>self$close = function() {invisible(self)}</code>.</p></li>
</ul>
<hr>
</div>
</div>
<div id="mountain-car" class="section level2">
<h2 class="hasAnchor">
<a href="#mountain-car" class="anchor"></a>Mountain Car</h2>
<p>The Mountain Car problem <span class="citation">(Sutton and Barto 2017)</span> is a simple episodic reinforcement learning task with a continuous state space and discrete action space. The goal is to drive an underpowered car up a steep slope. Because the car cannot accelerate fast enough, the optimal strategy consists of first going backwards to build enough momentum and then drive up the slope. The two-dimensional state space is characterized by the position and velocity of the car. These are updated due to the following equations:</p>
<span class="math display">\[\begin{align}
\text{position}_{t+1} &amp;\leftarrow \text{bound}[\text{position}_t + \text{velocity}_{t+1}] \\
\text{velocity}_{t+1} &amp;\leftarrow \text{bound}[\text{velocity}_t + 0.001 A_t - 0.0025 \cos(3 * \; \text{position}_t)].
\end{align}\]</span>
<p>The position is bounded in <span class="math inline">\([-1.2, 0.5]\)</span>, the velocity in <span class="math inline">\([-0.07, 0.07]\)</span>. When reaching the left position bound, the velocity will be set to 0. Each episode starts from a random position in the valley (position <span class="math inline">\(\in [-0.6, -0.4]\)</span>) and a velocity of 0.</p>
<p>The original formulation of the problem has three actions: “push left”, “no push” and “push right”, which are coded as -1, 0 and 1. The reward for each step is - 1 until the terminal state at the right mountain summit is reached.</p>
<p>Here is an example implementation of the mountain car environment.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mountainCar =<span class="st"> </span>R6<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/R6/topics/R6Class">R6Class</a></span>(<span class="st">"MountainCar"</span>, 
  <span class="dt">public =</span> <span class="kw">list</span>(
    <span class="dt">action.space =</span> <span class="st">"Discrete"</span>,
    <span class="dt">actions =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">2</span>,
    <span class="dt">n.actions =</span> <span class="dv">3</span>,
    <span class="dt">state.space =</span> <span class="st">"Box"</span>,
    <span class="dt">state.space.bounds =</span> <span class="kw">list</span>(<span class="kw">c</span>(<span class="op">-</span><span class="fl">1.2</span>, <span class="fl">0.5</span>), <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.07</span>, <span class="fl">0.07</span>)),
    <span class="dt">done =</span> <span class="ot">FALSE</span>,
    <span class="dt">n.steps =</span> <span class="dv">0</span>,
    <span class="dt">state =</span> <span class="ot">NULL</span>,
    <span class="dt">previous.state =</span> <span class="ot">NULL</span>,
    <span class="dt">reward =</span> <span class="ot">NULL</span>,
    <span class="dt">velocity =</span> <span class="ot">NULL</span>,
    <span class="dt">position =</span> <span class="ot">NULL</span>,
    
    <span class="dt">reset =</span> <span class="cf">function</span>() {
      self<span class="op">$</span>n.steps =<span class="st"> </span><span class="dv">0</span>
      self<span class="op">$</span>previous.state =<span class="st"> </span><span class="ot">NULL</span>
      self<span class="op">$</span>done =<span class="st"> </span><span class="ot">FALSE</span>
      self<span class="op">$</span>position =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>, <span class="op">-</span><span class="st"> </span><span class="fl">0.6</span>, <span class="op">-</span><span class="st"> </span><span class="fl">0.4</span>)
      self<span class="op">$</span>velocity =<span class="st"> </span><span class="dv">0</span>
      self<span class="op">$</span>state =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(self<span class="op">$</span>position, self<span class="op">$</span>velocity), <span class="dt">ncol =</span> <span class="dv">2</span>)
      <span class="kw">invisible</span>(self)
    },
    
    <span class="dt">step =</span> <span class="cf">function</span>(action) {
      self<span class="op">$</span>previous.state =<span class="st"> </span>self<span class="op">$</span>state
      self<span class="op">$</span>n.steps =<span class="st"> </span>self<span class="op">$</span>n.steps <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
      
      self<span class="op">$</span>velocity =<span class="st"> </span>self<span class="op">$</span>velocity <span class="op">+</span><span class="st"> </span><span class="fl">0.001</span> <span class="op">*</span><span class="st"> </span>(action <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">-</span>
<span class="st">        </span><span class="fl">0.0025</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>self<span class="op">$</span>position)
      self<span class="op">$</span>velocity =<span class="st"> </span><span class="kw">min</span>(<span class="kw">max</span>(self<span class="op">$</span>velocity, self<span class="op">$</span>state.space.bounds[[<span class="dv">2</span>]][<span class="dv">1</span>]), 
        self<span class="op">$</span>state.space.bounds[[<span class="dv">2</span>]][<span class="dv">2</span>])
      self<span class="op">$</span>position =<span class="st"> </span>self<span class="op">$</span>position <span class="op">+</span><span class="st"> </span>self<span class="op">$</span>velocity
      <span class="cf">if</span> (self<span class="op">$</span>position <span class="op">&lt;</span><span class="st"> </span>self<span class="op">$</span>state.space.bounds[[<span class="dv">1</span>]][<span class="dv">1</span>]) {
        self<span class="op">$</span>position =<span class="st"> </span>self<span class="op">$</span>state.space.bounds[[<span class="dv">1</span>]][<span class="dv">1</span>]
        self<span class="op">$</span>velocity =<span class="st"> </span><span class="dv">0</span>
      }
      
      self<span class="op">$</span>state =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(self<span class="op">$</span>position, self<span class="op">$</span>velocity), <span class="dt">ncol =</span> <span class="dv">2</span>)
      self<span class="op">$</span>reward =<span class="st"> </span><span class="op">-</span><span class="st"> </span><span class="dv">1</span>
      <span class="cf">if</span> (self<span class="op">$</span>position <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.5</span>) {
        self<span class="op">$</span>done =<span class="st"> </span><span class="ot">TRUE</span>
        self<span class="op">$</span>reward =<span class="st"> </span><span class="dv">0</span>
      }
      <span class="kw">invisible</span>(self)
    },
    
    <span class="dt">close =</span> <span class="cf">function</span>() {
      <span class="kw">invisible</span>(self)
    }
  )
)</code></pre></div>
<p>Then we can use this to generate samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m =<span class="st"> </span>mountainCar<span class="op">$</span><span class="kw">new</span>()
<span class="kw">set.seed</span>(<span class="dv">123456</span>)
m<span class="op">$</span><span class="kw">reset</span>()
<span class="cf">while</span>(<span class="op">!</span>m<span class="op">$</span>done) {
  action =<span class="st"> </span><span class="kw">sample</span>(m<span class="op">$</span>actions, <span class="dv">1</span>)
  m<span class="op">$</span><span class="kw">step</span>(action)
}
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">"Episode finished after"</span>, m<span class="op">$</span>n.steps, <span class="st">"steps."</span>))
<span class="co">#&gt; [1] "Episode finished after 787 steps."</span></code></pre></div>
<p>Note: The mountain car implementation above is already included in the package and can be called with the <code>mountainCar</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mountain Car with discrete action space</span>
m =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>()
m<span class="op">$</span><span class="kw">reset</span>()
m<span class="op">$</span><span class="kw">step</span>(<span class="dv">1</span>)
<span class="kw">print</span>(m)
<span class="co">#&gt; Number of steps: 1 </span>
<span class="co">#&gt; State: -0.454482559330265 -0.000518469774599052 </span>
<span class="co">#&gt; Reward: -1 </span>
<span class="co">#&gt; Done: FALSE</span></code></pre></div>
<p>There is also a variant with a continuous action space (bounded in <span class="math inline">\([-1, 1]\)</span>)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mountain Car with continuous action space</span>
m =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>(<span class="dt">action.space =</span> <span class="st">"Continuous"</span>)
m<span class="op">$</span><span class="kw">reset</span>()
<span class="kw">print</span>(m)
<span class="co">#&gt; Number of steps: 0 </span>
<span class="co">#&gt; State: -0.454710142640397 0 </span>
<span class="co">#&gt; Reward:  </span>
<span class="co">#&gt; Done: FALSE</span>
m<span class="op">$</span><span class="kw">step</span>(<span class="fl">0.27541</span>)
<span class="kw">print</span>(m)
<span class="co">#&gt; Number of steps: 1 </span>
<span class="co">#&gt; State: -0.454810022373572 -9.98797331752701e-05 </span>
<span class="co">#&gt; Reward: -1 </span>
<span class="co">#&gt; Done: FALSE</span></code></pre></div>
<hr>
<div id="other-vignettes" class="section level3">
<h3 class="hasAnchor">
<a href="#other-vignettes" class="anchor"></a>Other vignettes</h3>
<p>Have a look at the other vignettes:</p>
<ul>
<li><p><a href="introduction.html">Introduction to reinforcelearn</a></p></li>
<li><p><a href="algorithms.html">How to solve an environment?</a></p></li>
</ul>
<hr>
</div>
<div id="references" class="section level3 unnumbered">
<h3 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h3>
<div id="refs" class="references">
<div id="ref-gym_openai">
<p>Brockman, Greg, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. “OpenAI Gym.” <em>CoRR</em> abs/1606.01540. <a href="http://arxiv.org/abs/1606.01540" class="uri">http://arxiv.org/abs/1606.01540</a>.</p>
</div>
<div id="ref-gym">
<p>Hendricks, Paul. 2016. <em>Gym: Provides Access to the Openai Gym Api</em>. <a href="https://github.com/paulhendricks/gym-R" class="uri">https://github.com/paulhendricks/gym-R</a>.</p>
</div>
<div id="ref-sutton2017">
<p>Sutton, Richard S., and Andrew G. Barto. 2017. “Reinforcement Learning : An Introduction.” Cambridge, MA, USA: <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html" class="uri">http://incompleteideas.net/sutton/book/the-book-2nd.html</a>; MIT Press.</p>
</div>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#what-is-an-environment-in-reinforcelearn">What is an environment in reinforcelearn?</a></li>
      <li><a href="#mountain-car">Mountain Car</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Markus Dumke.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
