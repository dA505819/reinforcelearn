<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How to solve an environment? • reinforcelearn</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">reinforcelearn</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/algorithms.html">How to solve an environment?</a>
    </li>
    <li>
      <a href="../articles/environments.html">How to create an environment?</a>
    </li>
    <li>
      <a href="../articles/introduction.html">Introduction to reinforcelearn</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/markdumke/reinforcelearn">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>How to solve an environment?</h1>
            
          </div>

    
    
<div class="contents">
<style type="text/css">
h1.title {
  font-size: 34px;
}
</style>
<p>This vignette explains the reinforcement learning algorithms implemented in <code>reinforcelearn</code>. For details of how to create an environment have a look at the <a href="environments.html">How to create an environment?</a> vignette.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(reinforcelearn)
<span class="kw">set.seed</span>(<span class="dv">123</span>)</code></pre></div>
<hr>
<div id="q-learning-sarsa-expected-sarsa-and-qsigma" class="section level2">
<h2 class="hasAnchor">
<a href="#q-learning-sarsa-expected-sarsa-and-qsigma" class="anchor"></a>Q-Learning, Sarsa, Expected Sarsa and Q(sigma)</h2>
<p>Q(sigma), Q-Learning, Expected Sarsa and Sarsa build a family of reinforcement learning algorithms, which can be used to find the optimal action value function using the principle of generalized policy iteration.</p>
<p>In <code>reinforcelearn</code> you can use the <code>qlearning</code>, <code>sarsa</code>, <code>expectedSarsa</code> and <code>qSigma</code> functions. In the following we will train on a simple gridworld navigation task. The first argument of these algorithms is called <code>envir</code>, where the environment can be specified. The number of episodes can be specified via <code>n.episodes</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Windy gridworld environment</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="dt">goal.states =</span> <span class="dv">15</span>, <span class="dt">initial.state =</span> <span class="dv">0</span>)

res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="co"># Note: to find a good policy we need to run more episodes.</span></code></pre></div>
<p>These functions return the action value function <span class="math inline">\(Q\)</span> (here a matrix).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(res<span class="op">$</span>Q)
<span class="co">#&gt; NULL</span></code></pre></div>
<p>We can then find a policy by acting greedily with respect to the action value function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Values of each grid cell</span>
state.values =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">apply</span>(res<span class="op">$</span>Q1, <span class="dv">1</span>, max), <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
<span class="kw">print</span>(<span class="kw">round</span>(state.values, <span class="dv">1</span>))
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,] -1.7 -1.4 -1.1 -1.0</span>
<span class="co">#&gt; [2,] -1.4 -1.2 -1.1 -0.9</span>
<span class="co">#&gt; [3,] -1.1 -1.1 -0.9 -0.7</span>
<span class="co">#&gt; [4,] -1.0 -0.9 -0.6  0.0</span>

<span class="co"># Policy: Subtract 1 to be consistent with action numeration in env</span>
policy =<span class="st"> </span><span class="kw">max.col</span>(res<span class="op">$</span>Q1) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
<span class="kw">print</span>(<span class="kw">matrix</span>(policy, <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,]    3    1    0    0</span>
<span class="co">#&gt; [2,]    1    2    2    0</span>
<span class="co">#&gt; [3,]    2    0    3    0</span>
<span class="co">#&gt; [4,]    3    0    1    1</span></code></pre></div>
<p>They also return some statistics about learning behavior, e.g. the number of steps and returns per episode.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 91 20 75 25 22 42 61 12 89 10 18 10 32 37 61 10 26 22 16 22</span></code></pre></div>
<p>Expected Sarsa can be used as an on-policy or an off-policy algorithm depending on the <code>target.policy</code> argument, which can be <code>"greedy"</code> or <code>"egreedy"</code> for <span class="math inline">\(\epsilon\)</span>-greedy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This is equivalent to qlearning(env):</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">expectedSarsa</a></span>(env, <span class="dt">target.policy =</span> <span class="st">"greedy"</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)

<span class="co"># Expected Sarsa with an epsilon-greedy target policy:</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">expectedSarsa</a></span>(env, <span class="dt">target.policy =</span> <span class="st">"egreedy"</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)</code></pre></div>
<p>The Q(<span class="math inline">\(\sigma\)</span>) algorithm <span class="citation">(Asis et al. 2017)</span> generalizes Sarsa, Expected Sarsa and Q-Learning. Its parameter <span class="math inline">\(\sigma\)</span> controls a weighting between Sarsa and Expected Sarsa. Q(1) is equal to Sarsa and Q(0) to Expected Sarsa.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qSigma</a></span>(env, <span class="dt">sigma =</span> <span class="fl">0.5</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)

<span class="co"># This is equivalent to Sarsa:</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qSigma</a></span>(env, <span class="dt">sigma =</span> <span class="dv">1</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)

<span class="co"># This is equivalent to Q-Learning:</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qSigma</a></span>(env, <span class="dt">sigma =</span> <span class="dv">0</span>, <span class="dt">target.policy =</span> <span class="st">"greedy"</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)</code></pre></div>
<p>The hyperparameters of the algorithms can be specified as arguments, e.g. the discount factor <span class="math inline">\(\gamma\)</span> via <code>discount</code>, the learning rate <span class="math inline">\(\alpha\)</span> via <code>learning.rate</code> and the exploration factor <span class="math inline">\(\epsilon\)</span> via the <code>epsilon</code> argument. These parameters can also be adjusted over time by specifying an <code>update</code> function, e.g. <code>updateEpsilon</code>. The <code>update</code> function takes two arguments, the old value of the parameter and the number of episodes finished. It returns the updated parameter, e.g. a decreased learning rate. The update functions are called after each episode is finished.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">epsilon =</span> <span class="fl">0.2</span>, <span class="dt">learning.rate =</span> <span class="fl">0.5</span>, 
  <span class="dt">discount =</span> <span class="fl">0.99</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)

<span class="co"># Decay epsilon over time. Every 10 episodes epsilon will be halfed.</span>
decayEpsilon =<span class="st"> </span><span class="cf">function</span>(epsilon, i) {
  <span class="cf">if</span> (i <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {
    epsilon =<span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span>
  }
  epsilon
}

res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">epsilon =</span> <span class="fl">0.5</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>,
  <span class="dt">updateEpsilon =</span> decayEpsilon)</code></pre></div>
<p>The action value function will be initialized to 0. But you can also pass on an initial value function via the <code>initial.value</code> argument.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Q =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">100</span>, <span class="dt">nrow =</span> env<span class="op">$</span>n.states, <span class="dt">ncol =</span> env<span class="op">$</span>n.actions)
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">n.episodes =</span> <span class="dv">5</span>, <span class="dt">initial.value =</span> Q)

<span class="co"># After 5 episodes the Q values will still be similar to 100.</span>
<span class="kw">print</span>(<span class="kw">matrix</span>(<span class="kw">round</span>(<span class="kw">apply</span>(res<span class="op">$</span>Q1, <span class="dv">1</span>, max), <span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">10</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))
<span class="co">#&gt; Warning in matrix(round(apply(res$Q1, 1, max), 1), ncol = 10, byrow =</span>
<span class="co">#&gt; TRUE): Datenlänge [16] ist kein Teiler oder Vielfaches der Anzahl der</span>
<span class="co">#&gt; Spalten [10]</span>
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5]  [,6] [,7] [,8] [,9] [,10]</span>
<span class="co">#&gt; [1,] 99.4 99.6 99.7 99.7 99.6  99.7 99.8 99.8 99.7  99.7</span>
<span class="co">#&gt; [2,] 99.8 99.9 99.8 99.8 99.8 100.0 99.4 99.6 99.7  99.7</span></code></pre></div>
<div id="function-approximation" class="section level3">
<h3 class="hasAnchor">
<a href="#function-approximation" class="anchor"></a>Function Approximation</h3>
<p>So far the value function has been represented as a table (number of states x number of actions). In many interesting problems there are lots of states and actions or the space is continuous. Then it is inpractical to store a tabular value function and to slow to update state-action pairs individually. The solution is to approximate the value function with a function approximator, e.g. a linear combination of features.</p>
<p>In the following we will have a look at the mountain car problem, where the goal is to drive an underpowered car up a steep hill. The state space is continuous in two dimensions, the position and velocity of the car, each bounded in some interval. There are three different actions: push back (0), do nothing (1) and push forward (2).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>()
<span class="kw">print</span>(env<span class="op">$</span>state.space)
<span class="co">#&gt; [1] "Box"</span>
<span class="kw">print</span>(env<span class="op">$</span>state.space.bounds)
<span class="co">#&gt; [[1]]</span>
<span class="co">#&gt; [1] -1.2  0.5</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[2]]</span>
<span class="co">#&gt; [1] -0.07  0.07</span>

env<span class="op">$</span><span class="kw">reset</span>()
<span class="kw">print</span>(env<span class="op">$</span>state)
<span class="co">#&gt;            [,1] [,2]</span>
<span class="co">#&gt; [1,] -0.4538761    0</span></code></pre></div>
<p>We will solve this environment using linear function approximation. With linear function approximation the action value function is represented as</p>
<p><span class="math display">\[
\hat{q}(S_t, A_t, w) = x(S_t)^T w = \sum_{j=1}^{n} x_j(S_t) \, w_j
\]</span></p>
<p>and updated by gradient descent. There is a distinct weight vector per action.</p>
<div id="preprocessing-the-state" class="section level4">
<h4 class="hasAnchor">
<a href="#preprocessing-the-state" class="anchor"></a>Preprocessing the state</h4>
<p>The raw state observation returned from the environment must be preprocessed using the <code>preprocessState</code> argument. This function takes the state observation as input and returns a preprocessed state which can be directly used by the function approximator. To use a tabular value function <code>preprocessState</code> must return an integer value between [0, number of states - 1]. For linear function approximation the output of <code>preprocessState</code> must be a matrix with one row. For a neural network you have to make sure that the dimensions of the preprocessed state and the neural network match, so that <code>model$predict(preprocessState(envir$state))</code> works.</p>
<p>But how to get a good feature vector from the state observation? One idea is to use grid tiling <span class="citation">(Sutton and Barto 2017)</span> to aggregate the state space. Each tiling is a grid which overlays the state space. A state observation then falls into one tile per tiling and we will use as many weights as there are tiles. The feature vector is then just a one-hot vector of all active tiles.</p>
<p><img src="gridtiling.JPG" width="300px" style="display: block; margin: auto;"></p>
<p>We can define a function, which takes the original state observation as an input and returns a preprocessed state observation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define preprocessing function (we use grid tiling)</span>
n.tilings =<span class="st"> </span><span class="dv">8</span>
max.size =<span class="st"> </span><span class="dv">4096</span>
iht =<span class="st"> </span><span class="kw"><a href="../reference/tilecoding.html">IHT</a></span>(max.size)

position.max =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">1</span>]][<span class="dv">2</span>]
position.min =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">1</span>]][<span class="dv">1</span>]
velocity.max =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">2</span>]][<span class="dv">2</span>]
velocity.min =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">2</span>]][<span class="dv">1</span>]
position.scale =<span class="st"> </span>n.tilings <span class="op">/</span><span class="st"> </span>(position.max <span class="op">-</span><span class="st"> </span>position.min)
velocity.scale =<span class="st"> </span>n.tilings <span class="op">/</span><span class="st"> </span>(velocity.max <span class="op">-</span><span class="st"> </span>velocity.min)

<span class="co"># Scale state first, then get active tiles and return n hot vector</span>
gridTiling =<span class="st"> </span><span class="cf">function</span>(state) {
  state =<span class="st"> </span><span class="kw">c</span>(position.scale <span class="op">*</span><span class="st"> </span>state[<span class="dv">1</span>], velocity.scale <span class="op">*</span><span class="st"> </span>state[<span class="dv">2</span>])
  active.tiles =<span class="st"> </span><span class="kw"><a href="../reference/tilecoding.html">tiles</a></span>(iht, <span class="dv">8</span>, state)
  <span class="kw"><a href="../reference/makeNHot.html">makeNHot</a></span>(active.tiles, max.size, <span class="dt">out =</span> <span class="st">"vector"</span>)
}</code></pre></div>
<p>We can then pass this function to the <code>preprocessState</code> argument in <code>qlearning</code>. Via the <code>fun.approx</code> argument we tell the algorithm to use a linear combination of features to approximate the value function. Currently <code>fun.approx</code> supports <code>table</code>, <code>linear</code> and <code>neural.network</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">fun.approx =</span> <span class="st">"linear"</span>, <span class="dt">preprocessState =</span> gridTiling, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 1315  838  584  529  369  322  242  233  302  272  235  277  238  200</span>
<span class="co">#&gt; [15]  278  244  299  163  240  191</span></code></pre></div>
</div>
<div id="neural-network" class="section level4">
<h4 class="hasAnchor">
<a href="#neural-network" class="anchor"></a>Neural Network</h4>
<p>To use a neural network you have to specify a <code>keras</code> model. Here is an example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="dt">goal.states =</span> <span class="dv">15</span>, <span class="dt">initial.state =</span> <span class="dv">0</span>)

<span class="co"># Use a neural network as function approximator</span>
makeOneHot =<span class="st"> </span><span class="cf">function</span>(state) {
  one.hot =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, env<span class="op">$</span>n.states), <span class="dt">nrow =</span> <span class="dv">1</span>)
  one.hot[<span class="dv">1</span>, state <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span>
  one.hot
}

<span class="co"># Define keras model</span>
<span class="kw">library</span>(keras)
model =<span class="st"> </span><span class="kw">keras_model_sequential</span>()
model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> env<span class="op">$</span>n.actions, <span class="dt">activation =</span> <span class="st">'linear'</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(env<span class="op">$</span>n.states))

res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qSigma</a></span>(env, <span class="dt">fun.approx =</span> <span class="st">"neural.network"</span>, <span class="dt">model =</span> model,
  <span class="dt">preprocessState =</span> makeOneHot, <span class="dt">n.episodes =</span> <span class="dv">20</span>)</code></pre></div>
<p>Note that neural network training can be slow because at each step the Keras API is called.</p>
</div>
</div>
<div id="extensions" class="section level3">
<h3 class="hasAnchor">
<a href="#extensions" class="anchor"></a>Extensions</h3>
<p>There are several extensions of the algorithms, which can improve learning behavior.</p>
<div id="eligibility-traces" class="section level4">
<h4 class="hasAnchor">
<a href="#eligibility-traces" class="anchor"></a>Eligibility Traces</h4>
<p>Eligibility traces assign credit for the error back to all previously visited states and actions <span class="citation">(Sutton and Barto 2017)</span>. The trace decay parameter <span class="math inline">\(\lambda\)</span> and the type of the eligibility trace can be specified as arguments.</p>
<p>If <code>eligibility.type = 1</code> a replacing trace is used, if <code>eligibility.type = 0</code> an accumulating trace <span class="citation">(Singh and Sutton 1996)</span>. Intermediate values are also possible.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">4</span>), <span class="dt">goal.states =</span> <span class="dv">15</span>, <span class="dt">initial.state =</span> <span class="dv">0</span>)

<span class="co"># Sarsa with replacing traces</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">sarsa</a></span>(env, <span class="dt">lambda =</span> <span class="fl">0.9</span>, <span class="dt">eligibility.type =</span> <span class="dv">1</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 67 25 28 34 13 20 46 28 13 39 18  8 12 10 13 14 18 18 10 20</span></code></pre></div>
</div>
<div id="double-learning" class="section level4">
<h4 class="hasAnchor">
<a href="#double-learning" class="anchor"></a>Double Learning</h4>
<p>The idea of double learning <span class="citation">(Hasselt 2010)</span> is to decouple action selection and action evaluation.</p>
<p>To use Double Learning with <code>qSigma</code>, <code>qlearning</code>, <code>sarsa</code> and <code>expectedSarsa</code> just pass <code>double.learning = TRUE</code> to the algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">expectedSarsa</a></span>(env, <span class="dt">double.learning =</span> <span class="ot">TRUE</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 78  8 46 60 54 30 54 53 42 60 42 30 22 18 48 77 28 24 44 12</span></code></pre></div>
</div>
<div id="experience-replay" class="section level4">
<h4 class="hasAnchor">
<a href="#experience-replay" class="anchor"></a>Experience Replay</h4>
<p>When using function approximation in reinforcement learning training can be instable because subsequent state observations are often highly correlated and we train on these states the order they are experienced. Experience replay <span class="citation">(Mnih et al. 2013)</span> is a simple idea to break these correlations and stabilize learning. Instead of training on a simple observation at each time step the algorithm trains now on more than one observation sampled randomly from a replay memory, which stores all previously visited states and actions. Because the observations are trained on in a random order correlations are much smaller .</p>
<p>In <code>reinforcelearn</code> experience replay can be used by passing on a list of experiences to the <code>replay.memory</code> argument. Each list entry is itself a list with the entries <code>state</code>, <code>action</code>, <code>reward</code> and <code>next.state</code>. <code>state</code> and <code>next.state</code> should have been preprocessed, e.g. by calling <code>preprocessState(state)</code>. A different possibility is to specify the <code>replay.memory.size</code> argument, which will then be initialized with experiences generated by a random policy. The number of experiences trained on at each step is controled via the <code>batch.size</code> argument.</p>
<p>When experiencing a new transition the algorithm replaces the oldest entry in the replay memory by the new transition.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fill a replay memory of size 100 on the gridworld task.</span>
memory =<span class="st"> </span><span class="kw">vector</span>(<span class="st">"list"</span>, <span class="dt">length =</span> <span class="dv">100</span>)
env<span class="op">$</span><span class="kw">reset</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  <span class="cf">if</span> (env<span class="op">$</span>done) {
    env<span class="op">$</span><span class="kw">reset</span>()
  }
  action =<span class="st"> </span><span class="kw">sample</span>(env<span class="op">$</span>actions, <span class="dt">size =</span> <span class="dv">1</span>)
  env<span class="op">$</span><span class="kw">step</span>(action)
  memory[[i]] =<span class="st"> </span><span class="kw">list</span>(<span class="dt">state =</span> env<span class="op">$</span>previous.state, <span class="dt">action =</span> action, 
    <span class="dt">reward =</span> env<span class="op">$</span>reward, <span class="dt">next.state =</span> env<span class="op">$</span>state)
}
<span class="kw">print</span>(memory[[<span class="dv">1</span>]])
<span class="co">#&gt; $state</span>
<span class="co">#&gt; [1] 0</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $action</span>
<span class="co">#&gt; [1] 0</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $reward</span>
<span class="co">#&gt; [1] -1</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $next.state</span>
<span class="co">#&gt; [1] 0</span>

<span class="co"># Pass on replay memory.</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">sarsa</a></span>(env, <span class="dt">replay.memory =</span> memory, <span class="dt">batch.size =</span> <span class="dv">32</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)

<span class="co"># Specify replay memory size, replay memory will be filled internally.</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">sarsa</a></span>(env, <span class="dt">replay.memory.size =</span> <span class="dv">100</span>, <span class="dt">batch.size =</span> <span class="dv">32</span>, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 32 52 19 19 19 14  6  6 14 11  6  6 13  6  6 10  6  8  6  6</span></code></pre></div>
<p>As a default experiences will be randomly sampled from the replay memory. A prioritized experience replay prioritizes experiences with a high error <span class="citation">(Schaul et al. 2015)</span>.</p>
<p>When <span class="math inline">\(\alpha = 0\)</span> (the default) experiences are sampled with equal probability else experiences with a high error have a higher probability of being sampled. To each priority a small positive constant <span class="math inline">\(\theta\)</span> is added to prevent that experiences with an error of 0 are never replayed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Prioritized experience replay</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">sarsa</a></span>(env, <span class="dt">replay.memory.size =</span> <span class="dv">100</span>, <span class="dt">batch.size =</span> <span class="dv">32</span>, 
  <span class="dt">n.episodes =</span> <span class="dv">20</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>, <span class="dt">theta =</span> <span class="fl">0.01</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 63 36 29 22 29 11  6  6  6  6  7  7 15 10  6  6 11  6 10  6</span></code></pre></div>
<hr>
<p>There are other algorithms implemented in the package which will be described in the following.</p>
</div>
</div>
<div id="tdlambda" class="section level3">
<h3 class="hasAnchor">
<a href="#tdlambda" class="anchor"></a>TD(lambda)</h3>
<p>The TD(lambda) algorithm is used to evaluate a fixed policy. In <code>reinforcelearn</code> <code>td</code> can be used with a tabular or linear function approximation and with eligibility traces.</p>
<p><code>td</code> takes a <code>policy</code> argument, which is the policy to evaluate. In the tabular case this is just a matrix (number of states x number of actions) with the probabilities of each action given a state. For the linear function approximation <code>policy</code> must be a function, which returns an action given a preprocessed state observation. You can specify a maximal number of steps or episodes, so <code>td</code> can be used with both continuing and episodic environments.</p>
<p>Here we will solve a random walk task <span class="citation">(Sutton and Barto 2017, Example 6.2)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Random Walk Task (Sutton &amp; Barto Example 6.2)</span>
P =<span class="st"> </span><span class="kw">array</span>(<span class="dt">dim =</span> <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">7</span>, <span class="dv">2</span>))
P[, , <span class="dv">1</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>)), <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>)), 
  <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>)), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>)), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)), 
  <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="dv">1</span>)), <span class="dt">ncol =</span> <span class="dv">7</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
P[, , <span class="dv">2</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>)), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>)), 
  <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>)), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)), 
  <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dv">1</span>, <span class="dv">0</span>), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="dv">1</span>), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="dv">1</span>)), <span class="dt">ncol =</span> <span class="dv">7</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
R =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">12</span>), <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)
env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">rewards =</span> R, <span class="dt">initial.state =</span> <span class="dv">3</span>)

<span class="co"># Uniform random policy</span>
random.policy =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>env<span class="op">$</span>n.actions, <span class="dt">nrow =</span> env<span class="op">$</span>n.states, 
  <span class="dt">ncol =</span> env<span class="op">$</span>n.actions)

<span class="co"># Estimate state value function with TD(0)</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/td.html">td</a></span>(env, random.policy, <span class="dt">n.episodes =</span> <span class="dv">20</span>, <span class="dt">lambda =</span> <span class="fl">0.5</span>)
<span class="kw">print</span>(res<span class="op">$</span>V)
<span class="co">#&gt; [1] 0.00000000 0.01281662 0.04908379 0.26979868 0.43058114 0.61169674</span>
<span class="co">#&gt; [7] 0.00000000</span></code></pre></div>
<hr>
</div>
<div id="dynamic-programming" class="section level3">
<h3 class="hasAnchor">
<a href="#dynamic-programming" class="anchor"></a>Dynamic Programming</h3>
<p>Dynamic programming <span class="citation">(Sutton and Barto 2017)</span> is a class of solution methods solving a MDP not by interaction but by iterative computations using the state transition array and reward matrix. It can therefore only be applied when the model of the MDP is known.</p>
<p>In R we can evaluate a policy with dynamic programming with the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Set up gridworld problem</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/gridworld.html">gridworld</a></span>()
  
<span class="co"># Define uniform random policy, take each action with equal probability</span>
random.policy =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>env<span class="op">$</span>n.actions, <span class="dt">nrow =</span> env<span class="op">$</span>n.states, 
  <span class="dt">ncol =</span> env<span class="op">$</span>n.actions)

<span class="co"># Evaluate this policy</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/dp.html">evaluatePolicy</a></span>(env, random.policy, <span class="dt">precision =</span> <span class="fl">0.01</span>)
<span class="kw">print</span>(<span class="kw">round</span>(<span class="kw">matrix</span>(res<span class="op">$</span>v, <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)))
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,]    0  -14  -20  -22</span>
<span class="co">#&gt; [2,]  -14  -18  -20  -20</span>
<span class="co">#&gt; [3,]  -20  -20  -18  -14</span>
<span class="co">#&gt; [4,]  -22  -20  -14    0</span></code></pre></div>
<p>In theory it converges to the true values, but in practise we have to stop iteration before that. You can either specify a maximal number of iterations via the <code>n.iter</code> argument or a <code>precision</code> term, then the evaluation stops if the change in two subsequent values is less than <code>precision</code> for every state. You can specify an initial value function via the <code>v</code> argument. Note that the values of all terminal states must be 0 else the algorithm does not work.</p>
<p>Policy iteration tries to find the best policy in the MDP by iterating between evaluating and improving a policy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Find optimal policy using Policy Iteration</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/dp.html">iteratePolicy</a></span>(env)
<span class="kw">print</span>(<span class="kw">round</span>(<span class="kw">matrix</span>(res<span class="op">$</span>v, <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)))
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,]    0   -1   -2   -3</span>
<span class="co">#&gt; [2,]   -1   -2   -3   -2</span>
<span class="co">#&gt; [3,]   -2   -3   -2   -1</span>
<span class="co">#&gt; [4,]   -3   -2   -1    0</span></code></pre></div>
<p>You can specify an initial policy else the initial policy will be the uniform random policy.</p>
<p><code>iteratePolicy</code> stops if the policy does not change in two subsequent iterations or if the specified number of iterations is exhausted. For the policy evaluation step in policy iteration the same stop criteria as in <code>evaluatePolicy</code> are applied via the <code>precision.eval</code> and <code>n.iter.eval</code> can be passed on.</p>
<p>Value iteration evaluates each policy only once and then immediately improves the policy by acting greedily.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Find optimal policy using Value Iteration</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/dp.html">iterateValue</a></span>(env, <span class="dt">n.iter =</span> <span class="dv">100</span>)
<span class="kw">print</span>(res<span class="op">$</span>policy)
<span class="co">#&gt;       [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt;  [1,]    1    0    0    0</span>
<span class="co">#&gt;  [2,]    1    0    0    0</span>
<span class="co">#&gt;  [3,]    1    0    0    0</span>
<span class="co">#&gt;  [4,]    1    0    0    0</span>
<span class="co">#&gt;  [5,]    0    0    1    0</span>
<span class="co">#&gt;  [6,]    1    0    0    0</span>
<span class="co">#&gt;  [7,]    1    0    0    0</span>
<span class="co">#&gt;  [8,]    0    0    0    1</span>
<span class="co">#&gt;  [9,]    0    0    1    0</span>
<span class="co">#&gt; [10,]    1    0    0    0</span>
<span class="co">#&gt; [11,]    0    1    0    0</span>
<span class="co">#&gt; [12,]    0    0    0    1</span>
<span class="co">#&gt; [13,]    0    1    0    0</span>
<span class="co">#&gt; [14,]    0    1    0    0</span>
<span class="co">#&gt; [15,]    0    1    0    0</span>
<span class="co">#&gt; [16,]    1    0    0    0</span></code></pre></div>
<p><code>iterateValue</code> runs until the improvement in the value function in two subsequent steps is smaller than the given precision in all states or if the specified number of iterations is exhausted.</p>
<p><code>evaluatePolicy</code>, <code>iteratePolicy</code> and <code>iterateValue</code> return a list with state value function, action value function and policy.</p>
<hr>
</div>
<div id="actor-critic" class="section level3">
<h3 class="hasAnchor">
<a href="#actor-critic" class="anchor"></a>Actor Critic</h3>
<p>An actor critic is a policy-based reinforcement learning algorithm, which parametrizes value function and policy <span class="citation">(Sutton and Barto 2017)</span>. In <code>reinforcelearn</code> a simple advantage actor critic is implemented, which uses the td error of the state value function as a critic.</p>
<p>The policy can be a softmax policy for discrete actions or a gaussian policy for a continuous action space.</p>
<p>There are now two learning rates <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, one for the critic and one for the actor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>()

<span class="co"># Linear function approximation and softmax policy</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/actorCritic.html">actorCritic</a></span>(env, <span class="dt">fun.approx =</span> <span class="st">"linear"</span>, 
  <span class="dt">preprocessState =</span> gridTiling, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 1034 1087  303  170  233  310  236  230  262  237  167  215  237  232</span>
<span class="co">#&gt; [15]  148  234  181  154  180  156</span></code></pre></div>
<p>With a gaussian policy we can also solve problems with a continuous action space.</p>
<p>Here we will solve a continuous version of the mountain car problem, where the action is a real number.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mountain Car with continuous action space</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>(<span class="dt">action.space =</span> <span class="st">"Continuous"</span>)

<span class="co"># Linear function approximation and gaussian policy</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
res =<span class="st"> </span><span class="kw"><a href="../reference/actorCritic.html">actorCritic</a></span>(env, <span class="dt">fun.approx =</span> <span class="st">"linear"</span>, <span class="dt">policy =</span> <span class="st">"gaussian"</span>, 
  <span class="dt">preprocessState =</span> gridTiling, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 780 319 317 157 149 194 153 132 106 151 106 136 110  98  96 101 144</span>
<span class="co">#&gt; [18]  96 103 165</span></code></pre></div>
<p>The actor critic can be used with eligibility traces, then there are separate eligibility traces for the policy parameters and the value function parameters, which can be decayed by a different factor <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Cliff walking environment</span>
rewardFun =<span class="st"> </span><span class="cf">function</span>(state, action, n.state) {
  <span class="cf">if</span> (n.state <span class="op">%in%</span><span class="st"> </span><span class="dv">37</span><span class="op">:</span><span class="dv">46</span>) {
    <span class="kw">return</span>(<span class="op">-</span><span class="st"> </span><span class="dv">100</span>)
  } <span class="cf">else</span> {
    <span class="kw">return</span>(<span class="op">-</span><span class="st"> </span><span class="dv">1</span>)
  }
}
env =<span class="st"> </span><span class="kw"><a href="../reference/makeGridworld.html">makeGridworld</a></span>(<span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">12</span>), <span class="dt">goal.states =</span> <span class="dv">47</span>,
  <span class="dt">cliff.states =</span> <span class="dv">37</span><span class="op">:</span><span class="dv">46</span>, <span class="dt">reward.step =</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">reward.cliff =</span> <span class="op">-</span><span class="st"> </span><span class="dv">100</span>,
  <span class="dt">cliff.transition.done =</span> <span class="ot">TRUE</span>, <span class="dt">initial.state =</span> <span class="dv">36</span>, <span class="dt">sampleReward =</span> rewardFun)

res =<span class="st"> </span><span class="kw"><a href="../reference/actorCritic.html">actorCritic</a></span>(env, <span class="dt">n.episodes =</span> <span class="dv">20</span>, <span class="dt">lambda.actor =</span> <span class="fl">0.5</span>, <span class="dt">lambda.critic =</span> <span class="fl">0.8</span>)
<span class="kw">print</span>(res<span class="op">$</span>returns)
<span class="co">#&gt;  [1] -100 -103 -162 -148 -113 -179 -119 -124 -191 -104 -143 -100 -142 -108</span>
<span class="co">#&gt; [15] -149 -127 -172 -221 -158 -145</span></code></pre></div>
<hr>
</div>
<div id="multi-armed-bandit" class="section level3">
<h3 class="hasAnchor">
<a href="#multi-armed-bandit" class="anchor"></a>Multi-armed Bandit</h3>
<p>There are also solution methods for simple multi-armed bandit problems.</p>
<p>In the following we will consider an example bandit with four different actions. For each action the reward will be sampled from a probability distribution. The reward of the first action is sampled from a normal distribution with mean 1 and standard deviation 1, the second action from a normal distribution with mean 2 and standard deviation 4, the third action from a uniform distribution with minimum 0 and maximum 5 and the fourth action from an exponential distribution with rate parameter 0.25. Therefore the fourth action is the best with an expected reward of 4.</p>
<p>To solve this bandit problem we need to specify the reward function,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define reward function</span>
rewardFun =<span class="st"> </span><span class="cf">function</span>(action) {
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {
    reward =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
  }
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {
    reward =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">2</span>, <span class="dt">sd =</span> <span class="dv">4</span>)
  }
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) {
    reward =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">5</span>)
  }
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) {
    reward =<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">1</span>, <span class="dt">rate =</span> <span class="fl">0.25</span>)
  }
  reward
}</code></pre></div>
<p>To solve the bandit, i.e. to find out, which action returns the highest reward, we can use the <code>bandit</code> function. There are several different action selection methods implemented, e.g. <code>greedy</code>, <code>epsilon-greedy</code>, <code>UCB</code> and <code>gradient-bandit</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Greedy action selection.</span>
<span class="kw"><a href="../reference/bandit.html">bandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>,
  <span class="dt">action.selection =</span> <span class="st">"greedy"</span>)
<span class="co">#&gt; [1] -0.1976585 -1.3866575  2.1102407  3.9465064</span>

<span class="co"># Epsilon-greedy action selection.</span>
<span class="kw"><a href="../reference/bandit.html">bandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"egreedy"</span>, <span class="dt">epsilon =</span> <span class="fl">0.2</span>)
<span class="co">#&gt; [1] 1.032849 1.929346 2.711898 4.041045</span>

<span class="co"># Upper-confidence bound action selection.</span>
<span class="kw"><a href="../reference/bandit.html">bandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"UCB"</span>, <span class="dt">C =</span> <span class="dv">2</span>)
<span class="co">#&gt; [1] 0.4598108 1.1766058 2.4985695 1.4298745</span>

<span class="co"># Gradient-bandit algorithm.</span>
<span class="kw"><a href="../reference/bandit.html">bandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">10000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"gradientbandit"</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>)
<span class="co">#&gt; [1] 5.379818e-05 3.348153e-05 2.775565e-04 9.996352e-01</span></code></pre></div>
<p>In the <code>bandit</code> function we can specify the argument <code>initial.value</code> which sets all Q values initially to this number. Additionally we can assign a confidence to this initial value via the <code>initial.visits</code> argument. A value of 10 for example means that the algorithm has already seen 10 rewards for each action with an average value of <code>initial.value</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Greedy action selection with optimistic initial values.</span>
<span class="kw"><a href="../reference/bandit.html">bandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"greedy"</span>, 
  <span class="dt">initial.value =</span> <span class="dv">5</span>, <span class="dt">initial.visits =</span> <span class="dv">100</span>)
<span class="co">#&gt; [1] 3.987659 3.984982 3.988113 4.005252</span></code></pre></div>
<hr>
</div>
<div id="other-vignettes" class="section level3">
<h3 class="hasAnchor">
<a href="#other-vignettes" class="anchor"></a>Other vignettes</h3>
<p>Have a look at the other vignettes:</p>
<ul>
<li><p><a href="introduction.html">Introduction to reinforcelearn</a></p></li>
<li><p><a href="environments.html">How to create an environment?</a></p></li>
</ul>
<hr>
<div id="refs" class="references">
<div id="ref-deasis2017">
<p>Asis, Kristopher De, J. Fernando Hernandez-Garcia, G. Zacharias Holland, and Richard S. Sutton. 2017. “Multi-Step Reinforcement Learning: A Unifying Algorithm.” <em>CoRR</em> abs/1703.01327. <a href="http://arxiv.org/abs/1703.01327" class="uri">http://arxiv.org/abs/1703.01327</a>.</p>
</div>
<div id="ref-hasselt2010">
<p>Hasselt, Hado V. 2010. “Double Q-Learning.” In <em>Advances in Neural Information Processing Systems 23</em>, edited by J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, 2613–21. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/3964-double-q-learning.pdf" class="uri">http://papers.nips.cc/paper/3964-double-q-learning.pdf</a>.</p>
</div>
<div id="ref-mnih2013">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning.” <em>CoRR</em> abs/1312.5602. <a href="http://arxiv.org/abs/1312.5602" class="uri">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-schaul2015">
<p>Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. 2015. “Prioritized Experience Replay.” <em>CoRR</em> abs/1511.05952. <a href="http://arxiv.org/abs/1511.05952" class="uri">http://arxiv.org/abs/1511.05952</a>.</p>
</div>
<div id="ref-eligibility">
<p>Singh, Satinder P., and Richard S. Sutton. 1996. “Reinforcement Learning with Replacing Eligibility Traces.” <em>Machine Learning</em> 22 (1): 123–58. doi:<a href="https://doi.org/10.1007/BF00114726">10.1007/BF00114726</a>.</p>
</div>
<div id="ref-sutton2017">
<p>Sutton, Richard S., and Andrew G. Barto. 2017. “Reinforcement Learning : An Introduction.” Cambridge, MA, USA: <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html" class="uri">http://incompleteideas.net/sutton/book/the-book-2nd.html</a>; MIT Press.</p>
</div>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#q-learning-sarsa-expected-sarsa-and-qsigma">Q-Learning, Sarsa, Expected Sarsa and Q(sigma)</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Markus Dumke.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
