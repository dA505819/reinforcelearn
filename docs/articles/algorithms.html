<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>How to solve an environment? • reinforcelearn</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">reinforcelearn</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/algorithms.html">How to solve an environment?</a>
    </li>
    <li>
      <a href="../articles/environments.html">How to create an environment?</a>
    </li>
    <li>
      <a href="../articles/introduction.html">Introduction to reinforcelearn</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/markdumke/reinforcelearn">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>How to solve an environment?</h1>
            
          </div>

    
    
<div class="contents">
<style type="text/css">
h1.title {
  font-size: 34px;
}
</style>
<p>This vignette explains the reinforcement learning algorithms implemented in <code>reinforcelearn</code>. For details of how to create an environment have a look at the <a href="environments.html">How to create an environment?</a> vignette.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(reinforcelearn)</code></pre></div>
<hr>
<div id="q-learning" class="section level3">
<h3 class="hasAnchor">
<a href="#q-learning" class="anchor"></a>Q-Learning</h3>
<p>The most well-known algorithm for reinforcement learning is probably Q-Learning <span class="citation">(Watkins 1989)</span>.</p>
<p><span class="math inline">\(Q\)</span> is the action value function and specifies how good a state-action pair is. It is a matrix (number of states x number of actions). Interacting with the environment an action is chosen according to an <span class="math inline">\(\epsilon\)</span>-greedy behavior policy</p>
<p><span class="math display">\[
\pi(a | S_t) = \left\{ \begin{array}{ll} 1 - \epsilon + \frac{\epsilon}{m}, &amp; \text{if} \; a = \text{argmax}_a Q(S_t, a)  \\
                                        \frac{\epsilon}{m}, &amp; \text{else.} \end{array} \right.
\]</span></p>
<p>Then the <span class="math inline">\(Q\)</span> value of the state-action pair is updated by</p>
<p><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right],
\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is the so called discount factor and <span class="math inline">\(\alpha\)</span> the learning rate.</p>
<p>In <code>reinforcelearn</code> there is a <code>qlearning</code> function. In the following we will train on a simple navigation task, the windy gridworld. The first argument of all algorithms is the environment (the <code>envir</code>) argument. The number of episodes can be specified via <code>n.episodes</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Windy gridworld environment</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/windyGridworld.html">windyGridworld</a></span>()

res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">n.episodes =</span> <span class="dv">50</span>)
<span class="co"># Note: to find the optimal policy we need to run at least 500 episodes.</span></code></pre></div>
<p>The <code>qlearning</code> function returns the action value function <span class="math inline">\(Q\)</span> (here a matrix) and some statistics about learning behavior, e.g. the number of steps and return per episode.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1]  740 2023  600  202  555  109  261  358  236  247   91  208  154  320</span>
<span class="co">#&gt; [15]   88   98  174  154   50  130  348   46   56  179  112  213   85   95</span>
<span class="co">#&gt; [29]   67  116  142  103   79   38  135  131  122   37   81   81   24  143</span>
<span class="co">#&gt; [43]  194   90   46   92   62  180   31  118</span></code></pre></div>
<p>We can then find the optimal policy by acting greedily with respect to the action value function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Values of each grid cell</span>
state.values =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">apply</span>(res<span class="op">$</span>Q1, <span class="dv">1</span>, max), <span class="dt">ncol =</span> <span class="dv">10</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
<span class="kw">print</span>(<span class="kw">round</span>(state.values, <span class="dv">1</span>))
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]</span>
<span class="co">#&gt; [1,] -5.6 -5.8 -6.4 -7.2 -7.9 -8.2 -8.0 -7.3 -6.5  -5.6</span>
<span class="co">#&gt; [2,] -5.4 -5.4 -5.7 -6.0 -6.1 -5.6 -4.1 -4.3 -4.8  -4.7</span>
<span class="co">#&gt; [3,] -5.1 -5.0 -5.0 -5.2 -4.4 -3.4 -2.2 -2.6 -3.7  -3.8</span>
<span class="co">#&gt; [4,] -4.9 -4.5 -4.3 -4.2 -3.3 -1.7 -0.7  0.0 -2.6  -2.9</span>
<span class="co">#&gt; [5,] -4.3 -4.0 -3.6 -3.3 -2.0 -0.7  0.0 -0.5 -1.0  -1.9</span>
<span class="co">#&gt; [6,] -3.8 -3.5 -3.0 -2.4 -1.1  0.0  0.0 -0.1 -0.9  -1.3</span>
<span class="co">#&gt; [7,] -3.4 -3.1 -2.5 -1.7  0.0  0.0  0.0  0.0 -0.3  -0.8</span>

<span class="co"># Subtract 1 to be consistent with action numeration in env</span>
optimal.policy =<span class="st"> </span><span class="kw">max.col</span>(res<span class="op">$</span>Q1) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
<span class="kw">print</span>(<span class="kw">matrix</span>(optimal.policy, <span class="dt">ncol =</span> <span class="dv">10</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))
<span class="co">#&gt;      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]</span>
<span class="co">#&gt; [1,]    1    3    3    0    1    2    3    1    1     3</span>
<span class="co">#&gt; [2,]    2    2    3    1    2    0    1    0    0     3</span>
<span class="co">#&gt; [3,]    2    0    3    2    0    1    3    1    0     3</span>
<span class="co">#&gt; [4,]    1    1    3    0    3    1    3    2    0     2</span>
<span class="co">#&gt; [5,]    1    1    2    2    1    1    0    3    0     0</span>
<span class="co">#&gt; [6,]    1    0    3    0    1    1    1    0    1     1</span>
<span class="co">#&gt; [7,]    0    0    1    1    3    1    1    3    2     1</span></code></pre></div>
<p>There are many parameters, which can be specified, e.g. the discount factor <span class="math inline">\(\gamma\)</span> via <code>discount</code>, the learning rate <span class="math inline">\(\alpha\)</span> via <code>learning.rate</code> and <span class="math inline">\(\epsilon\)</span> via the <code>epsilon</code> argument. These parameters can also be updated over time by passing a function <code>updateEpsilon</code> etc. The update function takes two arguments, the old value of the parameter and the number of episodes finished. It returns the updated parameter, e.g. a decreased learning rate. After each episode is finished the update functions are called.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">epsilon =</span> <span class="fl">0.2</span>, <span class="dt">learning.rate =</span> <span class="fl">0.5</span>, <span class="dt">discount =</span> <span class="fl">0.99</span>)

<span class="co"># Decay epsilon over time. Every 10 episodes epsilon will be halfed.</span>
decayEpsilon =<span class="st"> </span><span class="cf">function</span>(epsilon, i) {
  <span class="cf">if</span> (i <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {
    epsilon =<span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span>
  }
  epsilon
}

res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">epsilon =</span> <span class="fl">0.5</span>, <span class="dt">updateEpsilon =</span> decayEpsilon)</code></pre></div>
<p>Initially the action value function will be initialized to 0. But you can also specify an initial value function via the <code>initial.value</code> argument.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Q =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">100</span>, <span class="dt">nrow =</span> env<span class="op">$</span>n.states, <span class="dt">ncol =</span> env<span class="op">$</span>n.actions)
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">n.episodes =</span> <span class="dv">5</span>, <span class="dt">initial.value =</span> Q)
<span class="co"># After only 5 episodes the Q values will still be similar to the initial values.</span>
<span class="kw">print</span>(<span class="kw">matrix</span>(<span class="kw">round</span>(<span class="kw">apply</span>(res<span class="op">$</span>Q1, <span class="dv">1</span>, max), <span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">10</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))
<span class="co">#&gt;      [,1] [,2] [,3] [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]</span>
<span class="co">#&gt; [1,] 97.2 97.0 96.5 95.8  95.2  94.9  94.9  95.2  95.8  96.5</span>
<span class="co">#&gt; [2,] 97.4 97.3 97.2 97.0  97.7  98.9 100.0  98.1  97.2  97.2</span>
<span class="co">#&gt; [3,] 97.7 97.7 97.8 98.1  99.1  99.8 100.0  99.3  98.3  98.0</span>
<span class="co">#&gt; [4,] 98.1 98.2 98.4 98.8  99.6 100.0 100.0 100.0  99.2  98.7</span>
<span class="co">#&gt; [5,] 98.4 98.5 98.8 99.3  99.8 100.0 100.0 100.0  99.6  99.3</span>
<span class="co">#&gt; [6,] 98.7 98.8 99.1 99.5 100.0 100.0 100.0 100.0  99.8  99.6</span>
<span class="co">#&gt; [7,] 98.8 98.9 99.3 99.8 100.0 100.0 100.0 100.0 100.0  99.8</span></code></pre></div>
</div>
<div id="function-approximation" class="section level3">
<h3 class="hasAnchor">
<a href="#function-approximation" class="anchor"></a>Function Approximation</h3>
<p>So far the value function has been represented as a table (number of states x number of actions). In many interesting problems there are lots of states and actions or the space is continuous. Then it is inpractical to store a tabular value function and to slow to update state-action pairs individually. The solution is to approximate the value function with a function approximator, e.g. a linear combination of features.</p>
<p>In the following we will have a look at the mountain car problem, where the goal is to drive an underpowered car up a steep hill. The state space is continuous in two dimensions, the position and velocity of the car, each bounded in some interval. There are three different actions: push back (0), do nothing (1) and push forward (2).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>()
<span class="kw">print</span>(env<span class="op">$</span>state.space)
<span class="co">#&gt; [1] "Box"</span>
<span class="kw">print</span>(env<span class="op">$</span>state.space.bounds)
<span class="co">#&gt; [[1]]</span>
<span class="co">#&gt; [1] -1.2  0.5</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; [[2]]</span>
<span class="co">#&gt; [1] -0.07  0.07</span>

env<span class="op">$</span><span class="kw">reset</span>()
<span class="kw">print</span>(env<span class="op">$</span>state)
<span class="co">#&gt;            [,1] [,2]</span>
<span class="co">#&gt; [1,] -0.5256833    0</span></code></pre></div>
<p>We will solve this environment using linear function approximation. With linear function approximation the action value function is represented as</p>
<p><span class="math display">\[
\hat{q}(S_t, A_t, w) = x(S_t)^T w = \sum_{j=1}^{n} x_j(S_t) \, w_j.
\]</span></p>
<p>There is a distinct weight vector per action.</p>
<p>The weights are then updated by</p>
<p><span class="math display">\[
w_{t+1} = w_t + \alpha \left[ R_{t+1} + \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right] x(S_t).
\]</span></p>
<p>But how to get a good feature vector from the state observation? We will use grid tiling <span class="citation">(Sutton and Barto 2017)</span> to aggregate the state space. Each tiling is a grid which overlays the state space. A state observation then falls into one tile per tiling and we will use as many weights as there are tiles. The feature vector is then just a one-hot vector of all active tiles.</p>
<p><img src="gridtiling.JPG" width="300px" style="display: block; margin: auto;"></p>
<p>We can define a function, which takes the original state observation as an input and returns a preprocessed state observation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define preprocessing function (we use grid tiling)</span>
n.tilings =<span class="st"> </span><span class="dv">8</span>
max.size =<span class="st"> </span><span class="dv">4096</span>
iht =<span class="st"> </span><span class="kw"><a href="../reference/tilecoding.html">IHT</a></span>(max.size)

position.max =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">1</span>]][<span class="dv">2</span>]
position.min =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">1</span>]][<span class="dv">1</span>]
velocity.max =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">2</span>]][<span class="dv">2</span>]
velocity.min =<span class="st"> </span>env<span class="op">$</span>state.space.bounds[[<span class="dv">2</span>]][<span class="dv">1</span>]
position.scale =<span class="st"> </span>n.tilings <span class="op">/</span><span class="st"> </span>(position.max <span class="op">-</span><span class="st"> </span>position.min)
velocity.scale =<span class="st"> </span>n.tilings <span class="op">/</span><span class="st"> </span>(velocity.max <span class="op">-</span><span class="st"> </span>velocity.min)

<span class="co"># Scale state first, then get active tiles and return n hot vector</span>
gridTiling =<span class="st"> </span><span class="cf">function</span>(state) {
  state =<span class="st"> </span><span class="kw">c</span>(position.scale <span class="op">*</span><span class="st"> </span>state[<span class="dv">1</span>], velocity.scale <span class="op">*</span><span class="st"> </span>state[<span class="dv">2</span>])
  active.tiles =<span class="st"> </span><span class="kw"><a href="../reference/tilecoding.html">tiles</a></span>(iht, <span class="dv">8</span>, state)
  <span class="kw"><a href="../reference/makeNHot.html">makeNHot</a></span>(active.tiles, max.size, <span class="dt">out =</span> <span class="st">"vector"</span>)
}</code></pre></div>
<p>We can then pass this function on via the <code>preprocessState</code> argument in <code>qlearning</code>. Via the <code>fun.approx</code> argument we tell the algorithm to use a linear combination of features to approximate the value function. Currently <code>fun.approx</code> supports <code>table</code> and <code>linear</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qlearning</a></span>(env, <span class="dt">fun.approx =</span> <span class="st">"linear"</span>, <span class="dt">preprocessState =</span> gridTiling, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 1211  903  536  420  406  241  241  239  233  232  204  241  194  235</span>
<span class="co">#&gt; [15]  233  167  198  165  234  162</span></code></pre></div>
</div>
<div id="sarsa-expected-sarsa-and-qsigma" class="section level3">
<h3 class="hasAnchor">
<a href="#sarsa-expected-sarsa-and-qsigma" class="anchor"></a>Sarsa, Expected Sarsa and Q(sigma)</h3>
<p>The Sarsa algorithm <span class="citation">(Rummery and Niranjan 1994)</span> is similar to Q-Learning but samples the next action instead of taking the max. Therefore the update rule is</p>
<p><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right].
\]</span></p>
<p>In contrast to Q-Learning Sarsa is an on-policy algorithm because the action values are adjusted using a target value <span class="math inline">\(Q(S_{t+1}, A_{t+1})\)</span> which uses an action $A_{t+1} $sampled from the same <span class="math inline">\(\epsilon\)</span>-greedy policy as <span class="math inline">\(A_t\)</span>.</p>
<p>In <code>reinforcelearn</code> this algorithm is implemented in the <code>sarsa</code> function. It takes the same arguments as the <code>qlearning</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/windyGridworld.html">windyGridworld</a></span>()
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">sarsa</a></span>(env, <span class="dt">n.episodes =</span> <span class="dv">50</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1]  817 1643  970  287  350  405  396   58  429  128  298  120  208  139</span>
<span class="co">#&gt; [15]   98  330   24   47  196  129   77  163  100   55  157   76  207   58</span>
<span class="co">#&gt; [29]   70   99  139   64  198  130   35   31  103  187   51   50   77  103</span>
<span class="co">#&gt; [43]   63   72  120  169   54   65   60  128</span></code></pre></div>
<p>Expected Sarsa <span class="citation">(Seijen et al. 2009)</span> is a generalization of Q-Learning to arbitrary target policies. While Sarsa samples the next action, Expected Sarsa computes the expectation by multiplying each action value with the probability of choosing this action. The update rule is</p>
<p><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_{a \in \mathcal{A}} \pi(a|S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right].
\]</span></p>
<p>The term <span class="math inline">\(\pi(a|S_{t+1})\)</span> is called the target policy because it is used to compute the target to update the values. If <span class="math inline">\(\pi(a|S_{t+1})\)</span> is the greedy policy Expected Sarsa is exactly Q-Learning. When the target policy uses the same <span class="math inline">\(\epsilon\)</span>-greedy policy as to sample <span class="math inline">\(A_t\)</span>, then Expected Sarsa is an on-policy algorithm. In R we can use the <code>target.policy</code> argument, which can be <code>"greedy"</code> or <code>"egreedy"</code> for <span class="math inline">\(\epsilon\)</span>-greedy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This is equivalent to qlearning(env):</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">expectedSarsa</a></span>(env, <span class="dt">target.policy =</span> <span class="st">"greedy"</span>)

<span class="co"># With an epsilon-greedy target policy:</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">expectedSarsa</a></span>(env, <span class="dt">target.policy =</span> <span class="st">"egreedy"</span>)</code></pre></div>
<p>The Q(<span class="math inline">\(\sigma\)</span>) algorithm <span class="citation">(Asis et al. 2017)</span> generalizes all the algorithms presented so far, i.e. Sarsa, Expected Sarsa and Q-Learning.</p>
<p>The update rule is</p>
<p><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \left(\sigma Q(S_{t+1}, A_{t+1}) + (1 - \sigma) \sum_{a \in \mathcal{A}} \pi(a|S_{t+1}) Q(S_{t+1}, a) \right) - Q(S_t, A_t) \right].
\]</span></p>
<p>The parameter <span class="math inline">\(\sigma\)</span> controls a weighting between the Sarsa target <span class="math inline">\(Q(S_{t+1}, A_{t+1})\)</span> and Expected Sarsa target <span class="math inline">\(\sum_{a \in \mathcal{A}} \pi(a|S_{t+1}) Q(S_{t+1}, a)\)</span>. Therefore Sarsa is equal to Q(1) and Expected Sarsa to Q(0).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qSigma</a></span>(env, <span class="dt">sigma =</span> <span class="fl">0.5</span>)

<span class="co"># This is equivalent to Sarsa:</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qSigma</a></span>(env, <span class="dt">sigma =</span> <span class="dv">1</span>)

<span class="co"># This is equivalent to Q-learning:</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">qSigma</a></span>(env, <span class="dt">sigma =</span> <span class="dv">0</span>, <span class="dt">target.policy =</span> <span class="st">"greedy"</span>)</code></pre></div>
</div>
<div id="eligibility-traces" class="section level3">
<h3 class="hasAnchor">
<a href="#eligibility-traces" class="anchor"></a>Eligibility Traces</h3>
<p>An eligibility trace is a scalar number per state-action pair (or weight when used with function approximation). The idea is to assign the current error back to all previously visited state-action pairs weighted by their eligibility. Recently and frequently visited state-action pairs will have a higher eligibility trace than those visited a long time ago. At the beginning of an episode all eligibility traces are set to 0. During the episode if a state-action pair is visited its eligibility trace is increased by</p>
<p><span class="math display">\[
E_t(S_t, A_t) = (1 - \psi) \, E_{t-1}(S_t, A_t) + 1,
\]</span></p>
<p>or for the weights</p>
<p><span class="math display">\[
E_t(w) = (1 - \psi) \, E_{t-1}(w) + x(S_t),
\]</span></p>
<p>when used with linear function approximation.</p>
<p>Then all action values are updated according to their share on the current error, e.g. for Q-Learning by</p>
<p><span class="math display">\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right] \, E_t(s, a) \qquad \forall s \in \mathcal{S}, \forall a \in \mathcal{A},
\]</span></p>
<p>and then over time fade away with an exponential decrease</p>
<p><span class="math display">\[
E_{t+1}(s, a) = \gamma \lambda (\sigma + (1- \sigma) \pi(A_{t+1} | S_{t+1})) \, E_t(s, a) \qquad \forall s \in \mathcal{S}, \forall a \in \mathcal{A}.
\]</span></p>
<p>The use of eligibility traces can make reinforcement learning agents learn much faster because all state-action pairs are updated at each time step.</p>
<p>The factor <span class="math inline">\(\psi\)</span> controls the type of eligibility, for <span class="math inline">\(\psi = 0\)</span> this is the standard accumulating trace, for <span class="math inline">\(\psi = 1\)</span> this is the replacing trace. The factor <span class="math inline">\(\lambda\)</span> is the eligibility decay parameter. The eligibility decrease is weighted with the target’s policy probability of the next action for the Expected Sarsa part of the target.</p>
<p>We can specify <span class="math inline">\(\psi\)</span> via the <code>eligibility.type</code> argument and <span class="math inline">\(\lambda\)</span> via the <code>lambda</code> argument.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">sarsa</a></span>(env, <span class="dt">lambda =</span> <span class="fl">0.9</span>, <span class="dt">eligibility.type =</span> <span class="dv">1</span>, <span class="dt">n.episodes =</span> <span class="dv">50</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 1382 2471  121  102   32   21   25   79   77  137   68   56   42   25</span>
<span class="co">#&gt; [15]   74   37   32   22   20   24   25   24   35   32   35   79   21   21</span>
<span class="co">#&gt; [29]   19   20   45   26   19   20   22   27   22   17   16   32   31   26</span>
<span class="co">#&gt; [43]   18   19   19   22   19   21   15   26</span></code></pre></div>
</div>
<div id="double-learning" class="section level3">
<h3 class="hasAnchor">
<a href="#double-learning" class="anchor"></a>Double Learning</h3>
<p>The idea of double learning <span class="citation">(H. V. Hasselt 2010)</span> is to decouple action selection and action evaluation. For example in Q-Learning the update is</p>
<p><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right].
\]</span></p>
<p>When using double learning we will store two action value functions (or two weight vectors in function approximation), one is used to find the best action</p>
<p><span class="math display">\[
a_* = \text{argmax}_a Q_A(S_{t+1}, a),
\]</span></p>
<p>while the other is then used to evaluate this action</p>
<p><span class="math display">\[
Q_B(S_{t+1}, a_*).
\]</span></p>
<p>Learning with two separate action values is slower, but tend to be more robust.</p>
<p>Actions are sampled due to an <span class="math inline">\(\epsilon\)</span>-greedy policy with respect to <span class="math inline">\(Q_A + Q_B\)</span>. Then at each step randomly one of the two action value functions is updated, e.g. when <span class="math inline">\(Q_A\)</span> is updated the update equation becomes</p>
<p><span class="math display">\[
Q_A(S_t, A_t) \leftarrow Q_A(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q_B(S_{t+1}, \text{argmax}_{a \in \mathcal{A}} Q_A(S_{t+1}, a)) - Q_A(S_t, A_t) \right]
\]</span></p>
<p>Double Learning can also be used for Sarsa, Expected Sarsa and Q(<span class="math inline">\(\sigma\)</span>). The update rule for Q(<span class="math inline">\(\sigma\)</span>) is</p>
<p><span class="math display">\[
Q_A(S_t, A_t) \leftarrow Q_A(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \left( \sigma Q_B(S_{t+1}, A_{t+1}) + (1 - \sigma) \sum_a \pi(a|S_{t+1}) Q_B(S_{t+1}, a) \right) - Q_A(S_t, A_t)
 \right].
\]</span></p>
<p>Update equations for <span class="math inline">\(Q_B\)</span> can be obtained by interchanging the roles of <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(Q_B\)</span> in the above equations.</p>
<p>To use Double Learning with <code>qSigma</code>, <code>qlearning</code>, <code>sarsa</code> and <code>expectedSarsa</code> just pass <code>double.learning = TRUE</code> to the algorithm. Double Learning works with tables and linear value function approximation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res =<span class="st"> </span><span class="kw"><a href="../reference/qSigma.html">expectedSarsa</a></span>(env, <span class="dt">double.learning =</span> <span class="ot">TRUE</span>, <span class="dt">n.episodes =</span> <span class="dv">50</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 1029 1810 1488  522  330  511  683  552  423  146  650  265  285  311</span>
<span class="co">#&gt; [15]  107  145  355  183  177   78  329  117  265  171  173  391   43  219</span>
<span class="co">#&gt; [29]  150   96  227  123  263  161  213  208   32  443   67  125   36  114</span>
<span class="co">#&gt; [43]  134   83  226  128  132  170   72  161</span></code></pre></div>
</div>
<div id="experience-replay" class="section level3">
<h3 class="hasAnchor">
<a href="#experience-replay" class="anchor"></a>Experience Replay</h3>
<p>When using function approximation in reinforcement learning training can be difficult because subsequent state observations are often highly correlated and we train on these states the order they are experienced. Experience replay <span class="citation">(Mnih et al. 2013)</span> is a simple idea to break these correlations and stabilize learning. Instead of training on a simple observation at each time step the algorithm trains now on more than one observation sampled randomly from a replay memory, which stores all previously visited states and actions. Because the observations are trained on in a random order correlations are much smaller .</p>
<p>In <code>reinforcelearn</code> experience replay can be used by passing on a list of experiences to the <code>replay.memory</code> argument. Each list entry is itself a list with the entries <code>state</code>, <code>action</code>, <code>reward</code> and <code>next.state</code>. <code>state</code> and <code>next.state</code> should have been preprocessed, e.g. by calling <code>preprocessState(state)</code>. A different possibility is to specify the <code>replay.memory.size</code> argument, which will then be initialized with experiences generated by a random policy. The number of experiences trained on at each step is controled via the <code>batch.size</code> argument.</p>
<p>When experiencing a new transition the algorithm replaces the oldest entry in the replay memory by the new transition.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fill a replay memory of size 100 on the mountain car task.</span>
<span class="co"># We will use grid tiling as defined above.</span>
memory =<span class="st"> </span><span class="kw">vector</span>(<span class="st">"list"</span>, <span class="dt">length =</span> <span class="dv">100</span>)
env =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>()
env<span class="op">$</span><span class="kw">reset</span>()
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  <span class="cf">if</span> (env<span class="op">$</span>done) {
    env<span class="op">$</span><span class="kw">reset</span>()
  }
  action =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="dv">0</span>, <span class="fl">0.5</span>))
  env<span class="op">$</span><span class="kw">step</span>(action)
  memory[[i]] =<span class="st"> </span><span class="kw">list</span>(<span class="dt">state =</span> <span class="kw">gridTiling</span>(env<span class="op">$</span>previous.state), <span class="dt">action =</span> action, 
    <span class="dt">reward =</span> env<span class="op">$</span>reward, <span class="dt">next.state =</span> <span class="kw">gridTiling</span>(env<span class="op">$</span>state))
}

<span class="co"># res = sarsa(env, fun.approx = "linear", preprocessState = gridTiling, </span>
<span class="co">#   replay.memory = memory, batch.size = 32, n.episodes = 30)</span>
<span class="co"># print(res$steps)</span>

<span class="co"># res = sarsa(env, fun.approx = "linear", preprocessState = gridTiling, </span>
<span class="co">#   replay.memory.size = 100, batch.size = 32, n.episodes = 30)</span></code></pre></div>
<p>As a default experiences will be randomly sampled from the replay memory. A prioritized experience replay prioritizes experiences with a high error, where the error is for example in Sarsa given by</p>
<p><span class="math display">\[
R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t).
\]</span></p>
<p>Each entry of the replay memory <span class="math inline">\(j\)</span> has a priority <span class="math inline">\(p_j\)</span> , proportional to the probability of being sampled.</p>
<p><span class="math display">\[
j \tilde{} P(j) = \frac{p_j^\alpha}{\sum_i p_i^\alpha}.
\]</span></p>
<p>When <span class="math inline">\(\alpha = 0\)</span> (the default) experiences are sampled with equal probability else experiences with a high error have a higher probability of being sampled. To each priority a small positive constant <span class="math inline">\(\theta\)</span> is added to prevent that experiences with an error of 0 are never replayed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Prioritized experience replay</span>
<span class="co"># res = sarsa(env, fun.approx = "linear", preprocessState = gridTiling,</span>
<span class="co">#   replay.memory.size = 100, batch.size = 32, n.episodes = 30,</span>
<span class="co">#   alpha = 0.5, theta = 0.05)</span></code></pre></div>
<hr>
</div>
<div id="td" class="section level3">
<h3 class="hasAnchor">
<a href="#td" class="anchor"></a>TD</h3>
<p>The TD algorithm is used to evaluate a fixed policy. It follows the policy to generate transitions and updates the state value function at each step by</p>
<p><span class="math display">\[
V(S_t) = V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) \right].
\]</span></p>
<p>The <code>td</code> function implements TD. It can be used with a tabular or linear function approximation and with eligibility traces. There is now one eligibility trace per state value (or weight) and its incrementally increased by</p>
<p><span class="math display">\[
E_t = (1 - \psi) E_{t-1} + 1
\]</span></p>
<p>and then the state value function is updated by</p>
<p><span class="math display">\[
V(s) = V(s) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) \right] E_t \quad \forall s \in \mathcal{S}.
\]</span></p>
<p>Afterwards the eligibility traces are decayed by</p>
<p><span class="math display">\[
E_{t+1} = \gamma \lambda E_t.
\]</span></p>
<p><code>td</code> takes a <code>policy</code> argument, which is the policy to evaluate. In the tabular case this is just a matrix (number of states x number of actions) with the probabilities of each action given a state. For the linear function approximation <code>policy</code> must be a function, which returns an action given a preprocessed state observation. You can specify a maximal number of steps or episodes, so <code>td</code> can be used with both continuing and episodic environments.</p>
<p>Here we will solve a random walk task <span class="citation">(Sutton and Barto 2017, Example 6.2)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Random Walk Task (Sutton &amp; Barto Example 6.2)</span>
P =<span class="st"> </span><span class="kw">array</span>(<span class="dt">dim =</span> <span class="kw">c</span>(<span class="dv">7</span>, <span class="dv">7</span>, <span class="dv">2</span>))
P[, , <span class="dv">1</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>)), <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>)), 
  <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>)), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>)), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)), 
  <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="dv">1</span>)), <span class="dt">ncol =</span> <span class="dv">7</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
P[, , <span class="dv">2</span>] =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>)), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>)), 
  <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>)), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">4</span>), <span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">2</span>)), 
  <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dv">1</span>, <span class="dv">0</span>), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="dv">1</span>), <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="dv">1</span>)), <span class="dt">ncol =</span> <span class="dv">7</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
R =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">12</span>), <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)
env =<span class="st"> </span><span class="kw"><a href="../reference/makeEnvironment.html">makeEnvironment</a></span>(<span class="dt">transitions =</span> P, <span class="dt">rewards =</span> R, <span class="dt">initial.state =</span> <span class="dv">3</span>)

<span class="co"># Uniform random policy</span>
random.policy =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>env<span class="op">$</span>n.actions, <span class="dt">nrow =</span> env<span class="op">$</span>n.states, 
  <span class="dt">ncol =</span> env<span class="op">$</span>n.actions)

<span class="co"># Estimate state value function with TD(0)</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/td.html">td</a></span>(env, random.policy, <span class="dt">n.episodes =</span> <span class="dv">100</span>, <span class="dt">lambda =</span> <span class="fl">0.5</span>)
<span class="kw">print</span>(res<span class="op">$</span>V)
<span class="co">#&gt; [1] 0.0000000 0.1468266 0.3319113 0.5742682 0.7571109 0.9284267 0.0000000</span></code></pre></div>
<p>Note that the state values fluctuate around the true values due to the learning rate, which changes the estimates in the direction of the last transition.</p>
<hr>
</div>
<div id="dynamic-programming" class="section level3">
<h3 class="hasAnchor">
<a href="#dynamic-programming" class="anchor"></a>Dynamic Programming</h3>
<p>Dynamic programming <span class="citation">(Sutton and Barto 2017)</span> is a class of solution methods solving a MDP not by interaction but by iterative computations using the state transition array and reward matrix. It can therefore only be applied when the model of the MDP is known.</p>
<p>Iterative policy evaluation evaluates a policy by applying the following iterative update for every state</p>
<p><span class="math display">\[
v_{k+1}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left[\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_k(s')\right].
\]</span></p>
<p>In R we can use</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Set up gridworld problem</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/gridworld.html">gridworld</a></span>()
  
<span class="co"># Define uniform random policy, take each action with equal probability</span>
random.policy =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>env<span class="op">$</span>n.actions, <span class="dt">nrow =</span> env<span class="op">$</span>n.states, 
  <span class="dt">ncol =</span> env<span class="op">$</span>n.actions)

<span class="co"># Evaluate this policy</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/dp.html">evaluatePolicy</a></span>(env, random.policy, <span class="dt">precision =</span> <span class="fl">0.01</span>)
<span class="kw">print</span>(<span class="kw">round</span>(<span class="kw">matrix</span>(res<span class="op">$</span>v, <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)))
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,]    0  -14  -20  -22</span>
<span class="co">#&gt; [2,]  -14  -18  -20  -20</span>
<span class="co">#&gt; [3,]  -20  -20  -18  -14</span>
<span class="co">#&gt; [4,]  -22  -20  -14    0</span></code></pre></div>
<p>In theory it converges to the true values, but in practise we have to stop iteration before that. You can either specify a maximal number of iterations via the <code>n.iter</code> argument or a <code>precision</code> term, then the evaluation stops if the change in two subsequent values is less than <code>precision</code> for every state. You can specify an initial value function via the <code>v</code> argument. Note that the values of all terminal states must be 0 else the algorithm does not work.</p>
<p>Policy iteration tries to find the best policy in the MDP by iterating between evaluating and improving a policy. The update cycle is</p>
<p><span class="math display">\[
\pi_1 \rightarrow v_{\pi_1} \rightarrow \pi_2 \rightarrow v_{\pi_2} \rightarrow \; ... \; \rightarrow \pi_* \rightarrow v_{\pi_*}.
\]</span></p>
<p>The policy improvement step finds the best action by acting greedily with respect to the value function of the previous policy</p>
<p><span class="math display">\[
\pi'(a | s) = \left\{ \begin{array}{ll} 1, &amp; \text{if} \; a = \text{argmax}_{a \in \mathcal{A}} (\mathcal{R}_s^a + \gamma P_{ss'}^a \, V(s'))  \\
                                        0, &amp; \text{else.} \end{array} \right.
\]</span></p>
<p>Here is an example finding the optimal value function and policy in the gridworld using <code>iteratePolicy</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Find optimal policy using Policy Iteration</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/dp.html">iteratePolicy</a></span>(env)
<span class="kw">print</span>(<span class="kw">round</span>(<span class="kw">matrix</span>(res<span class="op">$</span>v, <span class="dt">ncol =</span> <span class="dv">4</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)))
<span class="co">#&gt;      [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt; [1,]    0   -1   -2   -3</span>
<span class="co">#&gt; [2,]   -1   -2   -3   -2</span>
<span class="co">#&gt; [3,]   -2   -3   -2   -1</span>
<span class="co">#&gt; [4,]   -3   -2   -1    0</span></code></pre></div>
<p>You can specify an initial policy else the initial policy will be the uniform random policy.</p>
<p><code>iteratePolicy</code> stops if the policy does not change in two subsequent iterations or if the specified number of iterations is exhausted. For the policy evaluation step in policy iteration the same stop criteria as in <code>evaluatePolicy</code> are applied via the <code>precision.eval</code> and <code>n.iter.eval</code> can be passed on.</p>
<p>Value iteration evaluates each policy only once and then immediately improves the policy by acting greedily</p>
<p><span class="math display">\[
V_{k+1}(s) \leftarrow \max_{a \in \mathcal{A}} \left[ \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a V_k(s') \right].
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Find optimal policy using Value Iteration</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/dp.html">iterateValue</a></span>(env, <span class="dt">n.iter =</span> <span class="dv">100</span>)
<span class="kw">print</span>(res<span class="op">$</span>policy)
<span class="co">#&gt;       [,1] [,2] [,3] [,4]</span>
<span class="co">#&gt;  [1,]    1    0    0    0</span>
<span class="co">#&gt;  [2,]    1    0    0    0</span>
<span class="co">#&gt;  [3,]    1    0    0    0</span>
<span class="co">#&gt;  [4,]    1    0    0    0</span>
<span class="co">#&gt;  [5,]    0    0    1    0</span>
<span class="co">#&gt;  [6,]    1    0    0    0</span>
<span class="co">#&gt;  [7,]    1    0    0    0</span>
<span class="co">#&gt;  [8,]    0    0    0    1</span>
<span class="co">#&gt;  [9,]    0    0    1    0</span>
<span class="co">#&gt; [10,]    1    0    0    0</span>
<span class="co">#&gt; [11,]    0    1    0    0</span>
<span class="co">#&gt; [12,]    0    0    0    1</span>
<span class="co">#&gt; [13,]    0    1    0    0</span>
<span class="co">#&gt; [14,]    0    1    0    0</span>
<span class="co">#&gt; [15,]    0    1    0    0</span>
<span class="co">#&gt; [16,]    1    0    0    0</span></code></pre></div>
<p><code>iterateValue</code> runs until the improvement in the value function in two subsequent steps is smaller than the given precision in all states or if the specified number of iterations is exhausted.</p>
<p><code>evaluatePolicy</code>, <code>iteratePolicy</code> and <code>iterateValue</code> return a list with state value function, action value function and policy.</p>
<hr>
</div>
<div id="actor-critic" class="section level3">
<h3 class="hasAnchor">
<a href="#actor-critic" class="anchor"></a>Actor Critic</h3>
<p>An actor critic is a policy-based reinforcement learning algorithm. It uses both parametrized policy and value function. In <code>reinforcelearn</code> a TD actor critic is implemented, which uses a state value function as a critic.</p>
<p>The policy can be a softmax policy for discrete actions or a gaussian policy for a continuous action space.</p>
<p>The parameters of the critic are updated (for linear function approximation) by</p>
<p><span class="math display">\[
w_{t+1} = w_t + \beta [ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] x(S_t).
\]</span></p>
<p>and the policy’s parameter by</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t + \alpha \gamma^t [ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] \nabla_\theta \log \pi(A_t | S_t, \theta),
\]</span></p>
<p>where <span class="math inline">\(\nabla_\theta \log \pi(A_t | S_t, \theta)\)</span> is the gradient of the log policy. There are now two learning rates <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">env =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>()

<span class="co"># Linear function approximation and softmax policy</span>
res =<span class="st"> </span><span class="kw"><a href="../reference/actorCritic.html">actorCritic</a></span>(env, <span class="dt">fun.approx =</span> <span class="st">"linear"</span>, 
  <span class="dt">preprocessState =</span> gridTiling, <span class="dt">n.episodes =</span> <span class="dv">30</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 891 902 277 420 234 426 235 236 193 155 237 157 192 184 147 150 157</span>
<span class="co">#&gt; [18] 189 193 201 127 151 116 124 148 148 147 143 149 151</span></code></pre></div>
<p>With a gaussian policy we can also solve problems with a continuous action space.</p>
<p>Here we will solve a continuous version of the mountain car problem, where the action is a real number.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Mountain Car with continuous action space</span>
env =<span class="st"> </span><span class="kw"><a href="../reference/mountainCar.html">mountainCar</a></span>(<span class="dt">action.space =</span> <span class="st">"Continuous"</span>)

<span class="co"># Linear function approximation and gaussian policy</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
res =<span class="st"> </span><span class="kw"><a href="../reference/actorCritic.html">actorCritic</a></span>(env, <span class="dt">fun.approx =</span> <span class="st">"linear"</span>, <span class="dt">policy =</span> <span class="st">"gaussian"</span>, 
  <span class="dt">preprocessState =</span> gridTiling, <span class="dt">n.episodes =</span> <span class="dv">20</span>)
<span class="kw">print</span>(res<span class="op">$</span>steps)
<span class="co">#&gt;  [1] 780 319 317 157 149 194 153 132 106 151 106 136 110  98  96 101 144</span>
<span class="co">#&gt; [18]  96 103 165</span></code></pre></div>
<hr>
<div id="multi-armed-bandit" class="section level4">
<h4 class="hasAnchor">
<a href="#multi-armed-bandit" class="anchor"></a>Multi-armed Bandit</h4>
<p>A multi-armed bandit is a simplified reinforcement learning problem with an episode which consists only of one step. In the simplest form there are no states. The goal is to estimate the values of all actions and to find the best action by trying these out over a number of episodes.</p>
<p>In the following we will consider an example bandit with four different actions. For each action the reward will be sampled from a probability distribution. The reward of the first action is sampled from a normal distribution with mean 1 and standard deviation 1, the second action from a normal distribution with mean 2 and standard deviation 4, the third action from a uniform distribution with minimum 0 and maximum 5 and the fourth action from an exponential distribution with rate parameter 0.25. Therefore the fourth action is the best with an expected reward of 4.</p>
<p>To solve this bandit problem we need to specify the reward function,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define reward function</span>
rewardFun =<span class="st"> </span><span class="cf">function</span>(action) {
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {
    reward =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
  }
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {
    reward =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">2</span>, <span class="dt">sd =</span> <span class="dv">4</span>)
  }
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) {
    reward =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">5</span>)
  }
  <span class="cf">if</span> (action <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) {
    reward =<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">1</span>, <span class="dt">rate =</span> <span class="fl">0.25</span>)
  }
  reward
}</code></pre></div>
<p>To solve the bandit, i.e. to find out, which action returns the most reward, we can use the <code>solveBandit</code> function. There are several different action selection methods implemented, e.g. <code>greedy</code>, <code>epsilon-greedy</code>, <code>UCB</code> and <code>gradient-bandit</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/solveBandit.html">solveBandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>,
  <span class="dt">action.selection =</span> <span class="st">"greedy"</span>)</code></pre></div>
<p>In the <code>solveBandit</code> function we can specify the argument <code>initial.value</code> which sets all Q values initially to this number. Additionally we can assign a confidence to this initial value via the <code>initial.visits</code> argument. A value of 10 for example means that the algorithm has already seen 10 rewards for each action with an average value of <code>initial.value</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/solveBandit.html">solveBandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"greedy"</span>, 
  <span class="dt">initial.value =</span> <span class="dv">5</span>, <span class="dt">initial.visits =</span> <span class="dv">100</span>)</code></pre></div>
<p>An epsilon-greedy policy can be used as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/solveBandit.html">solveBandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"egreedy"</span>, <span class="dt">epsilon =</span> <span class="fl">0.5</span>)</code></pre></div>
<p>In the following we will decrease epsilon every 100 episodes by a factor of 0.5.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/solveBandit.html">solveBandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"egreedy"</span>, <span class="dt">epsilon =</span> <span class="fl">0.5</span>,
  <span class="dt">epsilon.decay =</span> <span class="fl">0.5</span>, <span class="dt">epsilon.decay.after =</span> <span class="dv">100</span>)</code></pre></div>
<p>The schedule after which epsilon is decreased can be controled by the <code>epsilon.decay</code> and <code>epsilon.decay.after</code> parameters. After every <code>epsilon.decay.after</code> episodes epsilon will be multiplied with <code>epsilon.decay</code>, usually a value between 0 and 1, and therefore decreases over time.</p>
<p>The estimates come close to the true values and will therefore correctly estimate the 4th action as the best one.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/solveBandit.html">solveBandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">1000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"UCB"</span>, <span class="dt">C =</span> <span class="dv">2</span>)</code></pre></div>
<p>The bandit algorithm is implemented in the following way. Action values are increased incrementally which is computationally cheaper as storing all rewards in a table.</p>
<p>There is also a gradient-bandit algorithm <span class="citation">(Sutton and Barto 2017)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/solveBandit.html">solveBandit</a></span>(rewardFun, <span class="dt">n.actions =</span> <span class="dv">4</span>, <span class="dt">n.episodes =</span> <span class="dv">10000</span>, 
  <span class="dt">action.selection =</span> <span class="st">"gradientbandit"</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>)</code></pre></div>
<hr>
</div>
</div>
<div id="other-vignettes" class="section level3">
<h3 class="hasAnchor">
<a href="#other-vignettes" class="anchor"></a>Other vignettes</h3>
<p>Have a look at the other vignettes:</p>
<ul>
<li><p><a href="introduction.html">Introduction to reinforcelearn</a></p></li>
<li><p><a href="environments.html">How to create an environment?</a></p></li>
</ul>
<hr>
<div id="refs" class="references">
<div id="ref-deasis2017">
<p>Asis, Kristopher De, J. Fernando Hernandez-Garcia, G. Zacharias Holland, and Richard S. Sutton. 2017. “Multi-Step Reinforcement Learning: A Unifying Algorithm.” <em>CoRR</em> abs/1703.01327. <a href="http://arxiv.org/abs/1703.01327" class="uri">http://arxiv.org/abs/1703.01327</a>.</p>
</div>
<div id="ref-hasselt2010">
<p>Hasselt, Hado V. 2010. “Double Q-Learning.” In <em>Advances in Neural Information Processing Systems 23</em>, edited by J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, 2613–21. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/3964-double-q-learning.pdf" class="uri">http://papers.nips.cc/paper/3964-double-q-learning.pdf</a>.</p>
</div>
<div id="ref-mnih2013">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning.” <em>CoRR</em> abs/1312.5602. <a href="http://arxiv.org/abs/1312.5602" class="uri">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-rummery1994">
<p>Rummery, G. A., and M. Niranjan. 1994. “On-Line Q-Learning Using Connectionist Systems.”</p>
</div>
<div id="ref-vanseijen2009">
<p>Seijen, Harm van, Hado van Hasselt, Shimon Whiteson, and Marco Wiering. 2009. “A Theoretical and Empirical Analysis of Expected Sarsa.” In <em>ADPRL 2009: Proceedings of the Ieee Symposium on Adaptive Dynamic Programming and Reinforcement Learning</em>, 177–84.</p>
</div>
<div id="ref-sutton2017">
<p>Sutton, Richard S., and Andrew G. Barto. 2017. “Reinforcement Learning : An Introduction.” Cambridge, MA, USA: <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html" class="uri">http://incompleteideas.net/sutton/book/the-book-2nd.html</a>; MIT Press.</p>
</div>
<div id="ref-watkins1989">
<p>Watkins, Christopher John Cornish Hellaby. 1989. “Learning from Delayed Rewards.” PhD thesis, King’s College, Cambridge.</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Markus Dumke.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
