% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlearning_approx.R
\name{qlearning_approx}
\alias{qlearning_approx}
\title{Q-Learning with linear function approximation}
\usage{
qlearning_approx(envir, make_feature_vector, n.features, n.episodes = 10,
  alpha = 0.1, epsilon = 0.1, discount.factor = 1, render = TRUE,
  seed = NULL, ...)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{make_feature_vector}{function which returns a feature vector for a given state observation.}

\item{n.features}{integer scalar: number of features}

\item{n.episodes}{scalar integer: the number of episodes}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update. Useful in non-stationary environments, giving high
value to the last observed returns.}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{render}{logical scalar: should the environment be rendered}

\item{seed}{scalar integer: random seed}

\item{...}{arguments passed to make_feature_vector}
}
\value{
Numeric matrix of weights. Number of weights is number of features x number of actions
}
\description{
Q-Learning with linear function approximation
}
\examples{
\dontrun{
# Make sure you have gym-http-api and python installed.
# Then start a server from command line by running: python gym_http_server.py
MountainCar = makeEnvironment("MountainCar-v0")

weights = qlearning_approx(MountainCar, make_feature_vector, n.features = 10, 
  state.space.bounds = MountainCar$state.space.bounds, n.grid = 10, n.episodes = 10, 
  render = TRUE)
}

}
\seealso{
\link{qlearning}
}
