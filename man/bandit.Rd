% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bandit.R
\name{bandit}
\alias{bandit}
\title{Multi-armed bandit}
\usage{
bandit(rewardFun, n.actions, n.episodes = 100,
  action.selection = c("egreedy", "greedy", "UCB", "gradientbandit"),
  epsilon = 0.1, alpha = 0.1, initial.value = 0, initial.visits = 0,
  C = 2, updateEpsilon = identity2, updateAlpha = identity2)
}
\arguments{
\item{rewardFun}{[\code{function}] \cr
A function, which takes an action (\code{integer(1)}) as first argument
and returns a numeric scalar reward.}

\item{n.actions}{[\code{integer(1)}] \cr
Number of actions.}

\item{n.episodes}{[\code{integer(1)}] \cr
Number of episodes.}

\item{action.selection}{[\code{character(1)}] \cr
Which method to use for action selection, one of \code{"egreedy"},
\code{"greedy"}, \code{"UCB"} or \code{"gradientbandit"}.}

\item{epsilon}{[\code{numeric(1) in [0,1]}] \cr
Ratio of random exploration in epsilon-greedy action selection.}

\item{alpha}{[\code{numeric(1)}] \cr
Parameter of the gradient bandit algorithm, a higher alpha value gives more weight to
recent rewards (useful for non-stationary environments).}

\item{initial.value}{[\code{numeric(1)}] \cr
Initial values for the action values Q, set this to the maximal possible
reward to encourage exploration (optimistic initialization).}

\item{initial.visits}{[\code{integer(1)}] \cr
Set this to a high number to encourage exploration
(together with a high \code{initial.value}).}

\item{C}{[\code{numeric(1)}] \cr
Controls the degree of exploration. High C values lead to more exploration.}

\item{updateEpsilon}{[\code{function}] \cr
A function which takes two arguments, \code{epsilon} and the current number of episodes.
Could be used to decay the parameter over time.}

\item{updateAlpha}{[\code{function}] \cr
A function which takes two arguments, \code{alpha} and the current number of episodes.
Could be used to decay the parameter over time.}
}
\value{
[\code{numeric}] \cr
Returns the action values for the arms of the bandit or,
for gradient-bandit action selection, the probabilities for
each action.
}
\description{
Different solution methods for multi-armed bandits.
}
\details{
Multi-armed bandits are a simplified reinforcement learning problem,
each arm of the bandit pays off a reward and the goal is to maximize
this reward, i.e. to choose the best arm. The arms of the bandit
can be seen as actions, after each action the episode ends (there
are no states). To find the best action, the algorithm is faced with
a tradeoff between exploration and exploitation.

Actions are numerated starting with 0!
}
\examples{
set.seed(123)

# Define reward function
rewardFun = function(action) {
  if (action == 0) {
    reward = rnorm(1, mean = 1, sd = 1)
  }
  if (action == 1) {
    reward = rnorm(1, mean = 2, sd = 4)
  }
  if (action == 2) {
    reward = runif(1, min = 0, max = 5)
  }
  if (action == 3) {
    reward = rexp(1, rate = 0.25)
  }
  reward
}

bandit(rewardFun, n.actions = 4, n.episodes = 1000,
  action.selection = "greedy")
bandit(rewardFun, n.actions = 4, n.episodes = 1000,
  action.selection = "egreedy", epsilon = 0.5)
bandit(rewardFun, n.actions = 4, n.episodes = 1000,
  action.selection = "greedy",
  initial.value = 5, initial.visits = 100)
bandit(rewardFun, n.actions = 4, n.episodes = 1000,
  action.selection = "UCB", C = 2)
# true values: 1, 2, 2.5, 4

# Gradient bandit algorithm
bandit(rewardFun, n.actions = 4, n.episodes = 10000,
  action.selection = "gradientbandit", alpha = 0.1)

}
