% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DynamicProgramming.R
\name{evaluatePolicy}
\alias{evaluatePolicy}
\alias{iteratePolicy}
\alias{iterateValue}
\title{Dynamic Programming}
\usage{
evaluatePolicy(envir, policy, v = NULL, q = NULL, discount = 1,
  precision = 1e-04, n.iter = NULL)

iteratePolicy(envir, policy = NULL, discount = 1, n.iter = NULL,
  precision.eval = 1e-04, n.iter.eval = NULL)

iterateValue(envir, v = NULL, q = NULL, discount = 1, precision = 1e-04,
  n.iter = NULL)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr
The reinforcement learning environment created by \code{\link{makeEnvironment}}.}

\item{policy}{[\code{matrix(n.states x n.actions)}] \cr
A policy specified as a probability matrix (states x actions).}

\item{v}{[\code{numeric(n.states)}] \cr
Initial state value function. Terminal states must have a value of 0!}

\item{q}{[\code{matrix(n.states x n.actions)}] \cr
Initial action value function. Terminal states must have a value of 0!}

\item{discount}{[\code{numeric(1) in [0, 1]}] \cr
Discount factor.}

\item{precision}{[\code{numeric(1)}] \cr
Algorithm stops when improvement is
smaller than precision.}

\item{n.iter}{[\code{integer(1)}] \cr
Number of iterations. If supplied the \code{precision} argument will be ignored.}

\item{precision.eval}{[numeric(1)] \cr
Policy evaluation stops when improvement is smaller than precision.}

\item{n.iter.eval}{[integer(1)] \cr
Number of iterations per evaluation step.}
}
\value{
[\code{list(3)}] \cr
Returns the state value function [\code{numeric}], the
action value function [\code{matrix}]
and the policy [\code{matrix}].
}
\description{
These functions solve a Markov Decision Process using a model of the environment.
\code{evaluatePolicy} evaluates a given policy,
\code{iteratePolicy} and \code{iterateValue} can be used to find the optimal policy.
}
\details{
\code{evaluatePolicy} runs until the improvement in the state value
function in two subsequent steps is smaller than the given precision in all states or if the
specified number of iterations is exhausted.

Both \code{iteratePolicy} and \code{iterateValue} alternate between evaluating a policy and
improving the current policy by acting greedily with respect to the current policy's value function.
The difference between these two algorithms is that
\code{iteratePolicy} evaluates the policy until some stop criterion is met,
while \code{iterateValue} evaluates each policy only one step and
then immediately improves upon the current policy.

When the \code{policy} argument is \code{NULL} the initial policy will be a uniform random policy.

\code{iteratePolicy} stops if the policy does not change in two subsequent iterations or if the
specified number of iterations is exhausted. For the policy evaluation step in policy iteration
the same stop criteria mentioned above are applied.

\code{iterateValue} runs until the improvement in the value
function in two subsequent steps
is smaller than the given precision in all states or if the
specified number of iterations is exhausted.
}
\examples{
# Set up gridworld problem
env = smallGridworld()

# Define uniform random policy, take each action with equal probability
random.policy = matrix(1 / env$n.actions, nrow = env$n.states,
  ncol = env$n.actions)

# Evaluate given policy
res = evaluatePolicy(env, random.policy, precision = 0.001)
print(round(matrix(res$v, ncol = 4, byrow = TRUE)))

# Find optimal policy using Policy Iteration
res = iteratePolicy(env)
print(round(matrix(res$v, ncol = 4, byrow = TRUE)))

# Find optimal policy using Value Iteration
res = iterateValue(env)
print(res$policy)

}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction. Chapter 4.
}
