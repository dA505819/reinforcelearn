% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlearning.R
\name{qlearning}
\alias{qlearning}
\title{Q-Learning (Table-lookup)}
\usage{
qlearning(envir, n.episodes = 10L, learning.rate = 0.1, epsilon = 0.1,
  epsilon.decay = 0.5, epsilon.decay.after = 100L, initial.value = 0L,
  discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: number of episodes}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{epsilon}{scalar numeric: ratio of random exploration in
epsilon-greedy action selection}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
by this factor}

\item{epsilon.decay.after}{scalar integer: number of episodes afer
which to decay epsilon}

\item{initial.value}{scalar numeric: initial values for the action
values Q, set this to the maximal possible reward to encourage
exploration (optimistic initialization)}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal action value function
}
\description{
Off-policy TD control algorithm. Q-Learning finds the optimal action value
function Q independent of the policy followed. Using an epsilon-greedy
behaviour policy states and actions are sampled. Given a state-action pair
the optimal next action is considered by taking the max over all possible
successor action values.

Under the assumption that all state-action pairs are visited (which is
achieved using a stochastic epsilon-greedy policy) Q-Learning converges to
the optimal action value function Q*. The update formula is:
\deqn{Q(S, A) <- Q(S, A) + \alpha[R + \gamma max_a Q(S', a) - Q(S, A)]}
}
\examples{
# Solve the WindyGridworld environment using Q-Learning
grid = makeWindyGridworld()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array,
  reward.matrix = grid$reward.matrix,
  initial.state = 30L)
res = qlearning(Gridworld1, n.episodes = 100, seed = 123)

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\link{sarsa}
}
