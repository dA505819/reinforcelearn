% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlearning.R
\name{qlearning}
\alias{qlearning}
\title{Q-Learning (Table-lookup)}
\usage{
qlearning(envir, n.episodes = 10, learning.rate = 0.1, epsilon = 0.1,
  epsilon.decay = 0.5, discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: the number of episodes}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
every 100 episodes by multiplying with this factor}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal action value function
}
\description{
Off-policy TD control algorithm. Q-Learning finds the optimal action value
function Q independent of the policy followed. Using an epsilon-greedy
behaviour policy states and actions are sampled. Given a state-action pair
the optimal next action is considered by taking the max over all possible
successor action values.

Under the assumption that all state-action pairs are visited (which is
achieved using a stochastic epsilon-greedy policy) Q-Learning converges to
the optimal action value function Q*. The update formula is:
\deqn{Q(S, A) <- Q(S, A) + \alpha[R + \gamma max_a Q(S', a) - Q(S, A)]}
}
\examples{
# Solve the WindyGridworld environment using Q-Learning
windygrid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = windygrid$transition.array,
  reward.matrix = windygrid$reward.matrix,
  terminal.states = windygrid$terminal.states,
  initial.state = 30)
res = qlearning(WindyGridworld1, n.episodes = 100, seed = 123)

}
\references{
\href{https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=158}{Sutton and Barto (2017) page 140}
}
\seealso{
\link{sarsa}
}
