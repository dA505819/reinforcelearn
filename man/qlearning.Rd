% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlearning.R
\name{qlearning}
\alias{qlearning}
\title{Q-Learning (Table-lookup)}
\usage{
qlearning(envir, n.episodes = 10, alpha = 0.1, epsilon = 0.1,
  discount.factor = 1, seed = NULL, render = TRUE)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: the number of episodes}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update. Useful in non-stationary environments, giving high
value to the last observed returns.}

\item{epsilon}{scalar numeric, algorithm stops when improvement is smaller than epsilon}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}

\item{render}{logical scalar: should the environment be rendered}
}
\value{
optimal action value function
}
\description{
Off-policy TD control algorithm. Q-Learning finds the optimal action value
function Q independent of the policy followed. Using an epsilon-greedy
behaviour policy states and actions are sampled. Given a state-action pair
the optimal next action is considered by taking the max over all possible
successor action values.

Under the assumption that all state-action pairs are visited (which is
achieved using a stochastic epsilon-greedy policy) Q-Learning converges to
the optimal action value function Q*. The update formula is:
\deqn{Q(S, A) <- Q(S, A) + \alpha[R + \gamma max_a Q(S', a) - Q(S, A)]}
}
\examples{
# grid = gridworld$new()
# Q = qlearning(grid, n.episodes = 1000)

\dontrun{
# Make sure you have gym-http-api and python installed.
# Then start a server from command line by running: python gym_http_server.py
FrozenLake = makeEnvironment("FrozenLake-v0")
Q = qlearning(FrozenLake, n.episodes = 10)
}
}
\references{
\href{https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=158}{Sutton and Barto (2017) page 140}
}
\seealso{
\link{dqlearning}
}
