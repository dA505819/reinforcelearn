% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/actorCritic.R
\name{actorCritic}
\alias{actorCritic}
\title{Actor Critic}
\usage{
actorCritic(envir, fun.approx = "table", policy = "softmax",
  critic.type = "advantage", preprocessState = identity, n.episodes = 100,
  discount = 1, alpha = 0.01, beta = 0.1, lambda = 0)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr
The reinforcement learning environment created by \code{\link{makeEnvironment}}.}

\item{fun.approx}{[\code{character(1)}] \cr
Type of function approximator used for policy and value function. 
Currently supported are linear combination of features (\code{"linear"}) and 
table lookup (\code{"table"}).}

\item{policy}{[\code{character(1)}] \cr
Policy type, supported are \code{"softmax"} for a discrete action space and
\code{"gaussian"} for a continuous action space using a normal distribution.}

\item{critic.type}{[\code{character(1)}] \cr
Type of the critic. Currently only advantage actor critic is supported.}

\item{preprocessState}{[\code{function}] \cr
A function that takes the state observation returned from the environment as an input and
preprocesses this in a way the algorithm can work with it.}

\item{n.episodes}{[\code{integer(1)}] \cr
Number of episodes.}

\item{discount}{[\code{numeric(1) in [0, 1]}] \cr
Discount factor.}

\item{alpha}{[\code{numeric(1)}] \cr
Learning rate (step size) for the policy.}

\item{beta}{[\code{numeric(1)}] \cr
Learning rate (step size) for the critic.}

\item{lambda}{[\code{numeric(1) in [0, 1]}] \cr
Eligibility trace decay parameter.}
}
\value{
[\code{list}] \cr
Returns a list with policy and value function parameters and 
some statistics about learning behaviour, e.g. the number of 
steps and return per episode.
}
\description{
Policy-based reinforcement learning control algorithm using both policy (the actor) 
and value function (the critic).
}
\details{
When using a gaussian policy mean and variance will be parametrized 
as a linear combination of features mu = x * w_1 and sigma = exp(x * w_2).

Eligibility traces are used for both policy parameters and value function parameters.
}
\examples{
# Variant of cliff walking
library(reinforcelearn)
rewardFun = function(state, action, n.state) {
  if (n.state \%in\% 37:46) {
    return(- 100)
  } else {
    return(- 1)
  }
}
env = makeGridworld(shape = c(4, 12), goal.states = 47,
  cliff.states = 37:46, reward.step = - 1, reward.cliff = - 100,
  cliff.transition.done = TRUE, initial.state = 36, sampleReward = rewardFun)

res = actorCritic(env, n.episodes = 100)

#----------------
# Mountain Car
m = MountainCar()

# Define preprocessing function (we use grid tiling)
n.tilings = 8
max.size = 4096
iht = IHT(max.size)

position.max = m$state.space.bounds[[1]][2]
position.min = m$state.space.bounds[[1]][1]
velocity.max = m$state.space.bounds[[2]][2]
velocity.min = m$state.space.bounds[[2]][1]
position.scale = n.tilings / (position.max - position.min)
velocity.scale = n.tilings / (velocity.max - velocity.min)

# Scale state first, then get active tiles and return n hot vector
gridTiling = function(state) {
  state = c(position.scale * state[1], velocity.scale * state[2])
  active.tiles = tiles(iht, 8, state)
  makeNHot(active.tiles, max.size, out = "vector")
}

# Linear function approximation and softmax policy
res = actorCritic(m, fun.approx = "linear", 
  preprocessState = gridTiling, n.episodes = 50)

#----------------
# Mountain Car with continuous action space
m2 = MountainCar(action.space = "Continuous")

# Linear function approximation and gaussian policy
set.seed(123)
res = actorCritic(m, fun.approx = "linear", policy = "gaussian", 
  preprocessState = gridTiling, n.episodes = 50)

}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction. Chapter 13
}
