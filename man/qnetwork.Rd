% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qnetwork.R
\name{qnetwork}
\alias{qnetwork}
\title{Q-Network Tensorflow}
\usage{
qnetwork(envir, make_feature_vector, n.episodes = 10, epsilon = 0.1,
  learning.rate = 0.1, discount.factor = 1, ...)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{make_feature_vector}{function which returns a
feature vector for a given state observation.}

\item{n.episodes}{scalar integer: the number of episodes}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{...}{arguments passed to make_feature_vector}
}
\value{
list with entries weights and episode.finished.after the
number of time steps each episode needed
}
\description{
Simple neural network tensorflow implementation
}
\examples{
options(gym.api.path = "C:/Users/M/Downloads/WinPython-64bit-3.6.0.1Qt5/scripts/gym-http-api")
windygrid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = windygrid$transition.array,
  reward.matrix = windygrid$reward.matrix,
  terminal.states = windygrid$terminal.states,
  initial.state = windygrid$initial.state)
  
res = qnetwork(WindyGridworld1, make_one_hot_vector, n.episodes = 500, len = 72)
plot(1:500, res$episode.finished.after[1:500], ylim = c(0, 200), 
  type = "l", xlab = "Episode", ylab = "Steps per Episode")
abline(h = 15, col = "red") # optimal solution

}
