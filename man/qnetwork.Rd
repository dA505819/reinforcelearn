% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qnetwork.R
\name{qnetwork}
\alias{qnetwork}
\title{Q-Network Tensorflow}
\usage{
qnetwork(envir, make.feature.vector, n.weights = 10L, n.episodes = 10,
  epsilon = 0.1, epsilon.decay = 0.5, learning.rate = 0.1,
  discount.factor = 1, ...)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{make.feature.vector}{function which returns a
feature vector for a given state observation.}

\item{n.weights}{scalar integer: number of weights}

\item{n.episodes}{scalar integer: the number of episodes}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
every 100 episodes by multiplying with this factor}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{...}{arguments passed to make.feature.vector}
}
\value{
list with entries weights and episode.finished.after the
number of time steps each episode needed
}
\description{
Simple neural network tensorflow implementation
}
\examples{
grid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array,
  reward.matrix = grid$reward.matrix,
  terminal.states = grid$terminal.states,
  initial.state = grid$initial.state)
  
set.seed(123)   
res = qnetwork(WindyGridworld1, make_one_hot_vector, 
  n.episodes = 500, len = 70, n.weights = 70)
plot(1:500, res$steps.per.episode[1:500], ylim = c(0, 200), 
  type = "l", xlab = "Episode", ylab = "Steps per Episode")
abline(h = 15, col = "red") # optimal solution

}
