% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sarsa.R
\name{sarsa}
\alias{sarsa}
\title{SARSA(lambda)}
\usage{
sarsa(envir, lambda = NULL, n.episodes = 10, alpha = 0.1, epsilon = 0.1,
  discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{the environment, an R6 class. See also \code{\link[=envir]{envir()}}.}

\item{lambda}{scalar integer between 0 and 1}

\item{n.episodes}{scalar integer: the number of episodes}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update.Useful in non-stationary environments, giving high
value to the last observed returns. If NULL the exact mean with alpha = 1 /
number of episodes will be used.}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. The higher epsilon the more exploration.}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal action value function Q
}
\description{
Currently implemented: Sarsa(0). Sarsa is an on-policy TD control algorithm.
Action value function Q will be updated
towards the action value function of the next state and next action using an
epsilon-greedy policy derived from Q.
}
\examples{
grid = gridworld$new()
Q = sarsa(grid, n.episodes = 1000)
}
\references{
\href{https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=156}{Sutton and Barto (2017) page 138}
}
\seealso{
\code{\link[=expectedSarsa]{expectedSarsa()}}

\code{\link[=qlearning]{qlearning()}}
}
