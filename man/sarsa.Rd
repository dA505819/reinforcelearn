% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sarsa.R
\name{sarsa}
\alias{sarsa}
\title{SARSA}
\usage{
sarsa(envir, lambda = 0, n.steps = 100, alpha = 0.1, epsilon = 0.1,
  discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{lambda}{scalar numeric in (0, 1): Then lambda = 0 only current state
is updated (this is equivalent to TD(0)), for lambda = 1 all states visited
are updated, this is roughly equivalent to every-visit Monte Carlo.}

\item{n.steps}{integer scalar: number of evaluations (steps in the environment)}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update. Useful in non-stationary environments, giving high
value to the last observed returns.}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal action value function Q
}
\description{
Sarsa is an on-policy TD control algorithm to find the optimal policy.
The action value function Q will be updated
towards the action value function of the next state S' and next action A'
using an epsilon-greedy policy derived from Q.
The update formula is:
\deqn{Q(S, A) <- Q(S, A) + \alpha[R + \gamma Q(S', A') - Q(S, A)]}
}
\examples{
grid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix, 
  terminal.states = grid$terminal.states, 
  initial.state = grid$initial.state)
res = sarsa(WindyGridworld1, n.steps = 1000)

# Optimal action value function
matrix(apply(res$Q, 1, max), ncol = 10, byrow = TRUE)

# Optimal policy
matrix(max.col(res$Q) - 1, ncol = 10, byrow = TRUE)
}
\references{
\href{https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=156}{Sutton and Barto (2017) page 138}
}
\seealso{
\link{td}

\link{expectedSarsa}

\link{qlearning}
}
