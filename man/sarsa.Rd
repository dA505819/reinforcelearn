% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sarsa.R
\name{sarsa}
\alias{sarsa}
\title{SARSA (Table-lookup)}
\usage{
sarsa(envir, lambda = 0, n.episodes = 100L, learning.rate = 0.1,
  epsilon = 0.1, epsilon.decay = 0.5, epsilon.decay.after = 100L,
  initial.value = 0, discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr 
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{lambda}{[\code{numeric(1) in [0, 1]}] \cr 
Then \code{lambda = 0} only current state is updated 
(this is equivalent to TD(0)), for \code{lambda = 1} 
all states visited are updated, this is roughly equivalent to 
every-visit Monte Carlo.}

\item{n.episodes}{[\code{integer(1)}] \cr 
Number of episodes.}

\item{learning.rate}{[\code{numeric(1) in [0,1]}] \cr 
Learning rate (step size).}

\item{epsilon}{[\code{numeric(1) in [0,1]}] \cr 
Ratio of random exploration in 
epsilon-greedy action selection}

\item{epsilon.decay}{[\code{numeric(1) in [0,1]}] \cr 
Decay epsilon by this factor.}

\item{epsilon.decay.after}{[\code{integer(1)}] \cr
Number of episodes after which to decay epsilon.}

\item{initial.value}{[\code{numeric(1)}] \cr 
Initial values for the action 
values Q, set this to the maximal possible reward to encourage
exploration (optimistic initialization)}

\item{discount.factor}{[\code{numeric(1) in [0,1]}] \cr 
Discounting future rewards.}

\item{seed}{[\code{integer(1)}] \cr 
Random seed.}
}
\value{
[\code{list(3)}] \cr
  Returns the optimal action value function [\code{matrix}] and the 
  number of steps and rewards per episode [\code{numeric}]
}
\description{
Sarsa is an on-policy TD control algorithm to find the optimal policy.
The action value function Q will be updated 
towards the action value function of the next state S' and next action A' 
using an epsilon-greedy policy derived from Q.
The update formula is: 
\deqn{Q(S, A) <- Q(S, A) + \alpha[R + \gamma Q(S', A') - Q(S, A)]}
}
\examples{
grid = makeEnvironment(transition.array = windyGridworld$transitions,
  reward.matrix = windyGridworld$rewards,
  initial.state = 30L)
res = sarsa(grid, n.episodes = 100, seed = 123)

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\code{\link{qSigma}}

\code{\link{qlearning}}
}
