% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gridworld.R
\name{gridworld}
\alias{gridworld}
\title{Gridworld environment}
\usage{
gridworld(state, action, shape = c(4, 4), terminalStates = list(c(1, 1),
  c(4, 4)))
}
\arguments{
\item{state}{the current state, e.g. c(1, 3)}

\item{action}{the action, one of ("left", "right", "up", "down")}

\item{shape}{the shape of the grid, e.g. (4, 4)}

\item{terminalStates}{list of terminal states, each list element is a length two vector}
}
\value{
list with next state, reward and a flag if episode is finished, i.e. the new state is
a terminal state
}
\description{
Simple gridworld for reinforcement learning. Given a state and an action,
the next state and reward are returned.
}
\details{
Possible actions include going left, right, down or up. If an action would take you off
the grid, you remain in the previous state. For each step you get a reward of -1, until you reach
into a terminal state.
}
\examples{
actions = c("left", "right", "up", "down")
states = matrix(c(3, 3), nrow = 1, ncol = 2)
rewards = numeric(0)
sampledActions = character(0)
episodeOver = FALSE
i = 1

while(episodeOver == FALSE) {
  sampledActions = append(sampledActions, sample(actions, size = 1))
  i = i + 1
  step = gridworld(states[i - 1, ], sampledActions[i - 1])
  rewards = append(rewards, step[["reward"]])
  states = rbind(states, step[["state"]])
  episodeOver = step[["episodeOver"]]
}

print(sampledActions)
print(rewards)
print(states)

}
\references{
Gridworld example from Sutton & Barto, chapter 4
}
