% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluatePolicy.R
\name{evaluatePolicy}
\alias{evaluatePolicy}
\title{Policy Evaluation (Dynamic Programming)}
\usage{
evaluatePolicy(envir, policy, v = NULL, discount.factor = 1,
  precision = 1e-04, iter = NULL)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr 
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{policy}{[\code{matrix}] \cr 
A policy specified as a probability matrix (states x actions)}

\item{v}{[\code{numeric}] \cr 
Initial state value function.}

\item{discount.factor}{[\code{numeric(1) in [0,1]}] \cr 
Discounting future rewards.}

\item{precision}{[\code{numeric(1)}] \cr 
Algorithm stops when improvement is
smaller than precision}

\item{iter}{[\code{integer(1)}] \cr 
Number of iterations.}
}
\value{
[\code{numeric}]\cr Returns the state value function v.
}
\description{
Evaluate a given policy in an environment using dynamic programming 
using  the Bellmann expectation equation as an update rule
\deqn{v(s) <- \sum \pi(a|s) (R + \gamma \sum Pss' v(s')])}
}
\details{
The algorithm runs until the improvement in the value 
function in two subsequent steps
is smaller than the given precision in all states or if the 
specified number of iterations is exhausted.
}
\examples{
# Define uniform random policy, take each action with equal probability
grid = makeEnvironment(transition.array = gridworld$transitions, 
  reward.matrix = gridworld$rewards)
random.policy = matrix(1 / grid$n.actions, nrow = grid$n.states, 
  ncol = grid$n.actions)

# Evaluate given policy for gridworld example
v = evaluatePolicy(grid, random.policy, iter = 100)
v = evaluatePolicy(grid, random.policy, precision = 0.001)
print(round(matrix(v, ncol = 4, byrow = TRUE)))

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
