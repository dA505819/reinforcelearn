% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluatePolicy.R
\name{evaluatePolicy}
\alias{evaluatePolicy}
\title{Policy Evaluation (Dynamic Programming)}
\usage{
evaluatePolicy(envir, policy, discount.factor = 1, psi = 1e-04)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{policy}{numeric matrix: a policy specified as a probability matrix (states x actions)}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{psi}{scalar numeric, algorithm stops when improvement is smaller than psi}
}
\value{
the state value function v, a numeric vector
}
\description{
Evaluate a given policy in an environment using dynamic programming.
}
\details{
With the Bellmann equation the update
\deqn{v(s) <- \sum \pi(a|s) (R + \gamma \sum Pss' v(s')])}

The algorithm runs until the improvement in the value function in two subsequent steps
is smaller than epsilon.
}
\examples{
# Define uniform random policy, take each action with probability 0.25
grid = gridworld$new()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix, terminal.states = grid$terminal.states,
  initial.state = grid$initial.state)
random.policy = matrix(1 / grid$n.actions, nrow = grid$n.states, 
  ncol = grid$n.actions)

# Evaluate given policy for gridworld example
v = evaluatePolicy(Gridworld1, random.policy)
print(round(matrix(v, ncol = 4, byrow = TRUE)))
}
