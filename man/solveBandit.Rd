% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bandit.R
\name{solveBandit}
\alias{solveBandit}
\title{Solve multi-armed bandit}
\usage{
solveBandit(bandit, n.episodes = 100L,
  action.selection = c("epsilon-greedy", "greedy", "UCB", "gradient-bandit"),
  epsilon = 0.1, epsilon.decay = 0.5, epsilon.decay.after = 100L,
  alpha = 0.1, initial.value = 0, initial.visits = 0L, C = 2)
}
\arguments{
\item{bandit}{[\code{R6 class}] \cr 
Bandit problem, e.g. \code{\link{bandit}}.}

\item{n.episodes}{[\code{integer(1)}] \cr 
Number of episodes.}

\item{action.selection}{[\code{character(1)}] \cr 
Which method to use for 
action selection, e.g. "epsilon-greedy", "greedy" or "UCB"}

\item{epsilon}{[\code{numeric(1) in [0,1]}] \cr 
Ratio of random exploration in 
epsilon-greedy action selection}

\item{epsilon.decay}{[\code{numeric(1) in [0,1]}] \cr 
Decay epsilon by this factor.}

\item{epsilon.decay.after}{[\code{integer(1)}] \cr
Number of episodes after which to decay epsilon.}

\item{alpha}{[\code{numeric(1)}] \cr 
Parameter of gradient bandit algorithm, higher alpha 
value gives more weight to recent rewards 
(useful for non-stationary environments)}

\item{initial.value}{[\code{numeric(1)}] \cr 
Initial values for the action 
values Q, set this to the maximal possible reward to encourage
exploration (optimistic initialization)}

\item{initial.visits}{[\code{integer(1)}] \cr 
Set this to a high number to encourage exploration 
(together with a high \code{initial.value}).}

\item{C}{[\code{numeric(1)}] \cr 
Controls the degree of exploration. High C
values lead to more exploration}
}
\value{
[\code{numeric}] \cr
Returns the action values for the arms of the bandit, 
for gradient-bandit action selection the probabilities for 
each action will be returned.
}
\description{
Multi-armed bandits are a simplified reinforcement learning problem, 
each arm of the bandit pays off a reward and the goal is to maximize 
this reward, i.e. to choose the best arm. The arms of the bandit 
can be seen as actions, after each action the episode ends (there 
are no states). To find the best action, the algorithm is faced with 
a tradeoff between exploration and exploitation.
}
\details{
Upper-confidence-bound action selection selects actions with 
\deqn{argmax_a Q(a) + sqrt( (C * log(t)) / N_t(a) ),} where N_t(a) is the 
number of times action a was selected.
}
\examples{
set.seed(123)
ExampleBandit = bandit$new()
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "greedy")
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "epsilon-greedy", epsilon = 0.5)
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "greedy", 
  initial.value = 5, initial.visits = 100)
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "UCB", C = 2)
# true values: 1, 2, 2.5, 4
# Gradient bandit algorithm
solveBandit(ExampleBandit, n.episodes = 10000, 
  action.selection = "gradient-bandit", alpha = 0.1)
}
