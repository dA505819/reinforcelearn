% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bandit.R
\name{solveBandit}
\alias{solveBandit}
\title{Solve multi-armed bandit}
\usage{
solveBandit(bandit, n.episodes = 10L, action.selection = c("epsilon-greedy",
  "greedy", "UCB", "gradient-bandit"), epsilon = 0.1, epsilon.decay = 0.5,
  epsilon.decay.after = 100L, alpha = 0.1, initial.value = 0,
  initial.visits = 0L, C = 2)
}
\arguments{
\item{bandit}{an R6 class: bandit problem}

\item{n.episodes}{scalar integer: number of episodes}

\item{action.selection}{scalar character: which method to use for
action selection, e.g. "epsilon-greedy", "greedy" or "UCB"}

\item{epsilon}{scalar numeric: ratio of random exploration in
epsilon-greedy action selection}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
by this factor}

\item{epsilon.decay.after}{scalar integer: number of episodes afer
which to decay epsilon}

\item{alpha}{parameter of gradient bandit algorithm, higher alpha
value gives more weight to recent rewards
(useful for non-stationary environments)}

\item{initial.value}{scalar numeric: initial values for the action
values Q, set this to the maximal possible reward to encourage
exploration (optimistic initialization)}

\item{initial.visits}{scalar integer: set this to a high number to
encourage exploration (together with a high initial.value)}

\item{C}{scalar numeric: controls the degree of exploration. High C
values lead to more exploration}
}
\value{
numeric vector: the action values for the arms of the bandit,
for gradient-bandit action selection the probabilities for
each action will be returned.
}
\description{
Multi-armed bandits are a simplified reinforcement learning problem,
each arm of the bandit pays off a reward and the goal is to maximize
this reward, i.e. to choose the best arm. The arms of the bandit
can be seen as actions, after each action the episode ends (there
are no states). To find the best action, the algorithm is faced with
a tradeoff between exploration and exploitation.
}
\details{
Upper-confidence-bound action selection selects actions with
\deqn{argmax_a Q(a) + sqrt( (C * log(t)) / N_t(a) ),} where N_t(a) is the
number of times action a was selected.
}
\examples{
set.seed(123)
ExampleBandit = bandit$new()
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "greedy")
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "epsilon-greedy", epsilon = 0.5)
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "greedy", 
  initial.value = 5, initial.visits = 100)
solveBandit(ExampleBandit, n.episodes = 1000, 
  action.selection = "UCB", C = 2)
# true values: 1, 2, 2.5, 4
# Gradient bandit algorithm
solveBandit(ExampleBandit, n.episodes = 10000, 
  action.selection = "gradient-bandit", alpha = 0.1)
}
