% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DynamicProgramming.R
\name{iterateValue}
\alias{iterateValue}
\title{Value Iteration (Dynamic Programming)}
\usage{
iterateValue(envir, v = NULL, discount.factor = 1, precision = 1e-04,
  iter = NULL)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr 
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{v}{[\code{numeric}] \cr 
Initial state value function.}

\item{discount.factor}{[\code{numeric(1) in [0,1]}] \cr 
Discounting future rewards.}

\item{precision}{[\code{numeric(1)}] \cr 
Algorithm stops when improvement is
smaller than precision}

\item{iter}{[\code{integer(1)}] \cr 
Number of iterations.}
}
\value{
[\code{list(2)}] \cr
  Returns the optimal state value function [\code{numeric}] 
  and the optimal policy [\code{matrix}] (number of states x number of actions)
}
\description{
Find optimal policy by dynamic programming. Iterate between evaluating a
given policy (only one step), then improving the policy by a greedy update.
Converges to the optimal policy.
}
\details{
The algorithm runs until the improvement in the value 
function in two subsequent steps
is smaller than the given precision in all states or if the 
specified number of iterations is exhausted.
}
\examples{
grid = makeEnvironment(transition.array = gridworld$transitions, 
  reward.matrix = gridworld$rewards)
res = iterateValue(grid)

}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction
}
\seealso{
\code{\link{iteratePolicy}}
}
