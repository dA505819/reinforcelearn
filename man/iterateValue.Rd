% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iterateValue.R
\name{iterateValue}
\alias{iterateValue}
\title{Value Iteration}
\usage{
iterateValue(envir, v = NULL, discount.factor = 1, precision = 1e-04,
  iter = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{v}{numeric vector: initial state value function v}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{precision}{scalar numeric, algorithm stops when improvement is
smaller than precision}

\item{iter}{integer: number of iterations}
}
\value{
a list with the optimal state value function (a numeric vector)
and the optimal policy (a matrix of dimension: number of states x number of actions)
}
\description{
Find optimal policy by dynamic programming. Iterate between evaluating a
given policy (only one step), then improving the policy by a greedy update.
Converges to the optimal policy.
}
\details{
The algorithm runs until the improvement in the value
function in two subsequent steps
is smaller than the given precision in all states or if the
specified number of iterations is exhausted.
}
\examples{
grid = makeGridworld()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array,
  reward.matrix = grid$reward.matrix)
res = iterateValue(Gridworld1)

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
iteratePolicy
}
