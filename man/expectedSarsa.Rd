% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/expectedSarsa.R
\name{expectedSarsa}
\alias{expectedSarsa}
\title{Expected Sarsa}
\usage{
expectedSarsa(envir, n.episodes = 10, alpha = 0.1, epsilon = 0.1,
  discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: the number of episodes}

\item{alpha}{scalar numeric between 0 and 1: learning rate}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal action value function Q
}
\description{
Expected Sarsa is similar to Q-Learning but instead of taking the max over
all possible next actions, an average is computed.
}
\examples{
grid = gridworld$new()
# Q = expectedSarsa(grid, n.episodes = 1000) # not working
}
\references{
\href{https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=160}{Sutton and Barto (2017) page 142}
}
\seealso{
\link{sarsa}

\link{qlearning}
}
