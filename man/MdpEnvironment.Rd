% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/environment_mdp.R
\name{MdpEnvironment}
\alias{MdpEnvironment}
\title{MDP Environment}
\arguments{
\item{transitions}{[\code{array (n.states x n.states x n.actions)}] \cr
State transition array.}

\item{rewards}{[\code{matrix (n.states x n.actions)}] \cr
Reward array.}

\item{initial.state}{[\code{integer}] \cr
Optional starting state.
If a vector is given a starting state will be
randomly sampled from this vector whenever \code{reset} is called.
Note that states are numerated starting with
0. If \code{initial.state = NULL} all non-terminal states are
possible starting states.}
}
\description{
Markov Decision Process environment.
}
\section{Usage}{

\code{MdpEnvironment$new(transitions, rewards, initial.state, visualize)}
}

\section{Methods}{

\code{$new()} Initializes a new environment.

\code{$reset()} Starts a new episode.
Returns the \code{state}.

\code{$step(action)} Take action in environment.
Returns a list with \code{state}, \code{reward}, \code{done}.

\code{$visualize()} Visualizes environment.
}

\examples{
# Create a Markov Decision Process.
P = array(0, c(2, 2, 2))
P[, , 1] = matrix(c(0.5, 0.5, 0, 1), 2, 2, byrow = TRUE)
P[, , 2] = matrix(c(0, 1, 0, 1), 2, 2, byrow = TRUE)
R = matrix(c(5, 10, -1, 2), 2, 2, byrow = TRUE)
env = MdpEnvironment$new(transitions = P, rewards = R)
env$reset()
env$step(1L)
}
