% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predictMC.R
\name{predictMonteCarlo}
\alias{predictMonteCarlo}
\title{Monte Carlo Prediction (Table-lookup)}
\usage{
predictMonteCarlo(envir, policy, n.episodes = 100L, v = NULL,
  method = c("first-visit", "every-visit"), discount.factor = 1,
  learning.rate = 0.1, print.out = 50L)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{policy}{numeric matrix: a policy specified as a probability
matrix (states x actions)}

\item{n.episodes}{scalar integer: number of episodes}

\item{v}{numeric vector: initial state value function v}

\item{method}{scalar character: Monte Carlo first-visit or
every-visit method}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{print.out}{integer: when to print out the iteration number}
}
\description{
Predict state value function v with Monte Carlo methods. The state value
function is estimated from mean returns of episodes.
}
\details{
Only works for episodic tasks (i.e. there must be at least one terminal
state)! An incremental mean update is implemented. Use a high learning.rate to
give recent episodes a higher weight if you have a non-stationary environment
. First-visit Monte Carlo estimates the return following the first visit to
a state, every-visit Monte Carlo following all visits to a state in the episode. Returns
are averaged over multiple episodes. The update rule is
\deqn{V(S) <- V(S) + \alpha[G - V(S')]}
}
\examples{
set.seed(26)
grid = makeEnvironment(transition.array = gridworld$transitions, 
  reward.matrix = gridworld$rewards)
  
# Define random policy
random.policy = matrix(1 / grid$n.actions, nrow = grid$n.states, 
  ncol = grid$n.actions)
  
# Estimate state value function with Monte Carlo prediction
v = predictMonteCarlo(grid, random.policy, n.episodes = 100, 
  method = "first-visit", learning.rate = NULL)
v = predictMonteCarlo(grid, random.policy, n.episodes = 100, 
  method = "every-visit", learning.rate = NULL)
}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\link{td}
}
