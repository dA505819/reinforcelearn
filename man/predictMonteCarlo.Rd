% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predictMonteCarlo.R
\name{predictMonteCarlo}
\alias{predictMonteCarlo}
\title{Monte Carlo Prediction (Table-lookup)}
\usage{
predictMonteCarlo(envir, policy, n.episodes = 100L, v = NULL,
  method = c("first-visit", "every-visit"), discount.factor = 1,
  learning.rate = 0.1, print.out = 50L)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr 
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{policy}{[\code{matrix}] \cr 
A policy specified as a probability matrix (states x actions)}

\item{n.episodes}{[\code{integer(1)}] \cr 
Number of episodes.}

\item{v}{[\code{numeric}] \cr 
Initial state value function.}

\item{method}{[\code{character(1)}] \cr 
Monte Carlo first-visit or every-visit method}

\item{discount.factor}{[\code{numeric(1) in [0,1]}] \cr 
Discounting future rewards.}

\item{learning.rate}{[\code{numeric(1) in [0,1]}] \cr 
Learning rate (step size).}

\item{print.out}{[\code{integer(1)}] \cr 
When to print out the iteration number and value function.}
}
\value{
[\code{numeric}] \cr
  Returns the state value function v.
}
\description{
Predict state value function v with Monte Carlo methods. The state value 
function is estimated from mean returns of episodes.
}
\details{
Only works for episodic tasks (i.e. there must be at least one terminal 
state)! An incremental mean update is implemented. Use a high learning.rate to
give recent episodes a higher weight if you have a non-stationary environment
. First-visit Monte Carlo estimates the return following the first visit to 
a state, every-visit Monte Carlo following all visits to a state in the episode. Returns 
are averaged over multiple episodes. The update rule is
\deqn{V(S) <- V(S) + \alpha[G - V(S')]}
}
\examples{
set.seed(26)
grid = makeEnvironment(transition.array = gridworld$transitions, 
  reward.matrix = gridworld$rewards)
  
# Define random policy
random.policy = matrix(1 / grid$n.actions, nrow = grid$n.states, 
  ncol = grid$n.actions)
  
# Estimate state value function with Monte Carlo prediction
v = predictMonteCarlo(grid, random.policy, n.episodes = 100, 
  method = "first-visit", learning.rate = NULL)
v = predictMonteCarlo(grid, random.policy, n.episodes = 100, 
  method = "every-visit", learning.rate = NULL)
}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\code{\link{td}}
}
