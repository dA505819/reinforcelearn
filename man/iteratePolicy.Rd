% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iteratePolicy.R
\name{iteratePolicy}
\alias{iteratePolicy}
\title{Policy Iteration}
\usage{
iteratePolicy(envir, initial.policy = NULL, discount.factor = 1,
  precision = 1e-04, iter = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{initial.policy}{numeric matrix: a policy specified as a probability
matrix (states x actions)}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{precision}{scalar numeric, algorithm stops when improvement is
smaller than precision}

\item{iter}{integer: number of iterations}
}
\value{
a list with the optimal state value function (a numeric vector)
and the optimal policy (a matrix of dimension: number of states x number of actions)
}
\description{
Find optimal policy by dynamic programming. Iterate between evaluating a
given policy and improving the policy by a greedy update.
Converges to the optimal policy.
}
\details{
The algorithm runs until the policy does not change
in two subsequent steps or if the
specified number of iterations is exhausted.
}
\examples{
grid = gridworld$new()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix)
res = iteratePolicy(Gridworld1)

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
iterateValue
}
