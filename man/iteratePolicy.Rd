% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DynamicProgramming.R
\name{iteratePolicy}
\alias{iteratePolicy}
\title{Policy Iteration}
\usage{
iteratePolicy(envir, initial.policy = NULL, discount.factor = 1,
  precision = 1e-04, iter = NULL)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr 
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{initial.policy}{[\code{matrix}] \cr 
A policy specified as a probability matrix (states x actions)}

\item{discount.factor}{[\code{numeric(1) in [0,1]}] \cr 
Discounting future rewards.}

\item{precision}{[\code{numeric(1)}] \cr 
Algorithm stops when improvement is
smaller than precision}

\item{iter}{[\code{integer(1)}] \cr 
Number of iterations.}
}
\value{
[\code{list(2)}] \cr
Returns the optimal state value function [\code{numeric}] 
and the optimal policy [\code{matrix}] (number of states x number of actions)
}
\description{
Find optimal policy by dynamic programming. Iterate between evaluating a 
given policy and improving the policy by a greedy update. 
Converges to the optimal policy.
}
\details{
The algorithm runs until the policy does not change 
in two subsequent steps or if the 
specified number of iterations is exhausted.
}
\examples{
grid = makeEnvironment(transition.array = gridworld$transitions, 
  reward.matrix = gridworld$rewards)
res = iteratePolicy(grid)

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\code{\link{iterateValue}}
}
