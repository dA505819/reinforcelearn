% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iteratePolicy.R
\name{iteratePolicy}
\alias{iteratePolicy}
\title{Policy Iteration}
\usage{
iteratePolicy(envir, initial.policy = NULL, discount.factor = 1,
  precision = 1e-04)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{initial.policy}{numeric matrix: a policy specified as a probability
matrix (states x actions)}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{precision}{scalar numeric, algorithm stops when improvement is
smaller than precision}
}
\value{
the optimal state value function and optimal policy
}
\description{
Find optimal policy by dynamic programming. Iterate between evaluating a
given policy (until convergence) and improving the policy by a greedy update.
Converges to the optimal policy.
}
\examples{
grid = gridworld$new()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix)
res = iteratePolicy(Gridworld1)

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
