% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlearning2.R
\name{qlearning2}
\alias{qlearning2}
\title{Q-Learning with Function Approximation}
\usage{
qlearning2(envir, n.episodes = 10L, preprocessState = NULL,
  predict = NULL, train = NULL, double.qlearning = FALSE,
  experience.replay = FALSE, replay.memory = NULL,
  replay.memory.size = 1000L, batch.size = 32L, frozen.target = FALSE,
  copy.params.after = 100L, epsilon = 0.1, epsilon.decay = 0.5,
  epsilon.decay.after = 100L, discount.factor = 1, seed = NULL, ...)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: number of episodes}

\item{preprocessState}{function: takes a state observation
as input and returns a preprocessed state, e.g. a one-hot vector}

\item{predict}{function: predict returns vector of q values for a
given preprocessed state observation}

\item{train}{function: train the model, update the weights}

\item{double.qlearning}{logical scalar: whether to use double
qlearning}

\item{experience.replay}{logical scalar}

\item{replay.memory}{list: each list entry is a list with entries
state, action, reward, next.state. replay.memory might be filled
with experience sampled from a random policy.}

\item{replay.memory.size}{integer scalar}

\item{batch.size}{scalar integer: batch size, how many samples are
drawn from the replay memory. Must be smaller than
size of the replay memory!}

\item{frozen.target}{scalar logical: Q-Learning with frozen target network}

\item{copy.params.after}{scalar integer: copy parameters to fixed
target network every n episodes}

\item{epsilon}{scalar numeric: ratio of random exploration in
epsilon-greedy action selection}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
by this factor}

\item{epsilon.decay.after}{scalar integer: number of episodes afer
which to decay epsilon}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}

\item{...}{arguments passed on to preprocessState, predict or train}
}
\value{
list with weights
}
\description{
You can use Q-Learning with Experience Replay. Therefore you can
either specify an initial replay memory filled with experience
and provide the size of the replay memory. If you do not
specify a replay memory this will be initially filled with random
experience.
}
\seealso{
\link{qlearning}
}
