% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlearning2.R
\name{qlearning2}
\alias{qlearning2}
\title{Q-Learning with Function Approximation}
\usage{
qlearning2(envir, n.episodes = 10L, preprocessState = NULL,
  predict = NULL, train = NULL, predict2 = NULL, copy = NULL,
  double.qlearning = FALSE, experience.replay = FALSE,
  replay.memory = NULL, replay.memory.size = 1000L,
  initial.replay.memory.size = 1000L, batch.size = 32L,
  frozen.target = FALSE, update.target.after = 100L, epsilon = 0.1,
  epsilon.decay = 0.5, epsilon.decay.after = 100L, discount.factor = 1,
  seed = NULL, ...)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: number of episodes}

\item{preprocessState}{function: takes a state observation
as input and returns a preprocessed state, e.g. a one-hot vector}

\item{predict}{function: predict returns vector of q values for a
given preprocessed state observation}

\item{train}{function: train the model, update the weights}

\item{predict2}{function: predict function for the target network}

\item{copy}{function: copy model parameters to target network}

\item{double.qlearning}{logical scalar: whether to use double
qlearning}

\item{experience.replay}{logical scalar}

\item{replay.memory}{list: each list entry is a list with entries
state, action, reward, next.state. replay.memory might be filled
with experience sampled from a random policy.}

\item{replay.memory.size}{integer scalar: size of the replay memory}

\item{initial.replay.memory.size}{integer scalar:
how much of the replay memory is filled initially}

\item{batch.size}{scalar integer: batch size, how many samples are
drawn from the replay memory. Must be smaller than
size of the replay memory!}

\item{frozen.target}{scalar logical: Q-Learning with frozen target network}

\item{update.target.after}{scalar integer: copy parameters to fixed
target network every n steps}

\item{epsilon}{scalar numeric: ratio of random exploration in
epsilon-greedy action selection}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
by this factor}

\item{epsilon.decay.after}{scalar integer: number of episodes afer
which to decay epsilon}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}

\item{...}{arguments passed on to preprocessState, predict or train}
}
\value{
list with weights and number of steps
}
\description{
Q-Learning algorithm with Experience Replay and
Frozen Target Network and Double Q-Learning.
}
\details{
To use experience replay you can
either specify an initial replay memory filled with experience
and provide the size of the replay memory. If you do not
specify a replay memory this will be initially filled with random
experience.
Double Q-Learning works right now only if \code{frozen.target == TRUE}.
}
\examples{
# define the environment
grid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array,
  reward.matrix = grid$reward.matrix,
  initial.state = 30L)

# Build the DQN
# define a tensorflow graph for the function approximator (here a neural network)
library(tensorflow)
tf$reset_default_graph()
batch.size = 32L
inputs = tf$placeholder(tf$float32, shape(NULL, WindyGridworld1$n.states))
inputs2 = tf$placeholder(tf$float32, shape(NULL, WindyGridworld1$n.states))
weights = tf$Variable(tf$random_uniform(shape(WindyGridworld1$n.states,
  WindyGridworld1$n.actions), 0, 0.01, seed = 1))
weights2 = tf$Variable(tf$random_uniform(shape(WindyGridworld1$n.states,
  WindyGridworld1$n.actions), 0, 0.01, seed = 1))
Q = tf$matmul(inputs, weights)
Q2 = tf$matmul(inputs2, weights2)
nextQ = tf$placeholder(tf$float32, shape(NULL, WindyGridworld1$n.actions))
loss = tf$reduce_sum(tf$square(nextQ - Q))
optimizer = tf$train$GradientDescentOptimizer(learning_rate = 0.1)
trainModel = optimizer$minimize(loss)
copy.params = tf$assign(weights2, weights)

# initialize the session and the weights
sess = tf$Session()
sess$run(tf$global_variables_initializer())

# takes the state and returns a one-hot vector
preprocessState = function(state_) {
  one_hot = matrix(0L, nrow = length(state_), ncol = WindyGridworld1$n.states)
  one_hot[cbind(seq_along(state_), state_)] = 1L
  one_hot
}
# predict returns vector of q values for a given state
predict = function(inputs_) {
  sess$run(Q, feed_dict = dict(inputs = inputs_))
}
predict2 = function(inputs_) {
  sess$run(Q2, feed_dict = dict(inputs2 = inputs_))
}
# train model, update weights, e.g. gradient descent: this is supervised learning
train = function(inputs_, outputs_, predictions_ = NULL) {
  sess$run(tuple(trainModel, weights),
    feed_dict = dict(inputs = inputs_, nextQ = outputs_))
}

# copy model weights to target
copy = function() {
  sess$run(copy.params)
}

# DQN
res = qlearning2(WindyGridworld1, n.episodes = 300L,
  preprocessState, predict, train, seed = 2L)

# please rerun the tensorflow model building
# DQN with experience replay
res2 = qlearning2(WindyGridworld1, n.episodes = 300L,
  preprocessState, predict, train,
  experience.replay = TRUE, replay.memory.size = 10000L,
  initial.replay.memory.size = 10000L, seed = 2L)

# DQN with frozen target network
res3 = qlearning2(WindyGridworld1, n.episodes = 300L,
  preprocessState, predict, train, predict2, copy,
  frozen.target = TRUE, update.target.after = 100L, seed = 2L)

# DQN with experience replay and frozen target network
res4 = qlearning2(WindyGridworld1, n.episodes = 300L,
  preprocessState, predict, train, predict2, copy,
  experience.replay = TRUE, replay.memory.size = 10000L,
  initial.replay.memory.size = 10000L,
  frozen.target = TRUE, update.target.after = 100L, seed = 2L)
  
# DQN with frozen target network and double q-learning
res = qlearning2(WindyGridworld1, n.episodes = 300L,
  preprocessState, predict, train, predict2, copy, double.qlearning = TRUE, 
  frozen.target = TRUE, update.target.after = 100L, seed = 2L)
# double qlearning currently only works for frozen targets

# DQN with experience replay, frozen target network and double q-learning
res = qlearning2(WindyGridworld1, n.episodes = 300L,
  preprocessState, predict, train, predict2, copy, double.qlearning = TRUE, 
  experience.replay = TRUE, replay.memory.size = 10000L, 
  initial.replay.memory.size = 10000L,
  frozen.target = TRUE, update.target.after = 100L, seed = 2L)

# DQN with experience replay passed on to function
# pass initial random experience to the function
replay.memory.size = 10000L # steps
replay.memory2 = vector("list", length = replay.memory.size)
# fill this initially with experience generated by random policy
WindyGridworld1$reset()
for (i in 1:replay.memory.size) {
  state = WindyGridworld1$state
  action = sample(0:3, 1)
  WindyGridworld1$step(action)
  replay.memory2[[i]] <- list(action = action, reward = WindyGridworld1$reward,
    state = state, next.state = WindyGridworld1$state)
  if (WindyGridworld1$episode.over == TRUE) {
    print(i)
    WindyGridworld1$reset()
  }
}

res5 = qlearning2(WindyGridworld1, n.episodes = 300L,
  preprocessState, predict, train,
  experience.replay = TRUE, replay.memory = replay.memory2,
  replay.memory.size = 12000L, seed = 2L)
  

}
\seealso{
\link{qlearning}
}
