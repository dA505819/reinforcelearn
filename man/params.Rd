% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/params_documentation.R
\name{params}
\alias{params}
\title{Documentation of all parameters}
\usage{
params(policy, initial.policy, envir, bandit, discount.factor, precision,
  lambda, epsilon, epsilon.decay, epsilon.decay.after, seed, method, n.steps,
  n.episodes, initial.value, initial.visits, C, action.selection, learning.rate,
  preprocessState, predict, predict2, copy, train, ..., experience.replay,
  replay.memory, replay.memory.size, initial.replay.memory.size, batch.size,
  alpha, theta, fixed.target, update.target.after, double.qlearning, sigma, v,
  iter)
}
\arguments{
\item{policy}{numeric matrix: a policy specified as a probability
matrix (states x actions)}

\item{initial.policy}{numeric matrix: a policy specified as a probability
matrix (states x actions)}

\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{bandit}{an R6 class: bandit problem}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{precision}{scalar numeric, algorithm stops when improvement is
smaller than precision}

\item{lambda}{scalar numeric in (0, 1): Then lambda = 0 only
current state is updated (this is equivalent to TD(0)), for
lambda = 1 all states visited are updated, this is roughly
equivalent to every-visit Monte Carlo.}

\item{epsilon}{scalar numeric: ratio of random exploration in
epsilon-greedy action selection}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
by this factor}

\item{epsilon.decay.after}{scalar integer: number of episodes afer
which to decay epsilon}

\item{seed}{scalar integer: random seed}

\item{method}{scalar character: Monte Carlo first-visit or
every-visit method}

\item{n.steps}{integer scalar: number of evaluations (steps in the
environment)}

\item{n.episodes}{scalar integer: number of episodes}

\item{initial.value}{scalar numeric: initial values for the action
values Q, set this to the maximal possible reward to encourage
exploration (optimistic initialization)}

\item{initial.visits}{scalar integer: set this to a high number to
encourage exploration (together with a high initial.value)}

\item{C}{scalar numeric: controls the degree of exploration. High C
values lead to more exploration}

\item{action.selection}{scalar character: which method to use for
action selection, e.g. "epsilon-greedy", "greedy" or "UCB"}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{preprocessState}{function: takes a state observation as input
and returns a preprocessed state, e.g. a one-hot vector}

\item{predict}{function: predict returns vector of q values for a
given preprocessed state observation}

\item{predict2}{function: predict function for the target network}

\item{copy}{function: copy model parameters to target network}

\item{train}{function: train the model, update the weights}

\item{...}{arguments passed on to preprocessState, predict or train}

\item{experience.replay}{logical scalar}

\item{replay.memory}{list: each list entry is a list with entries
state, action, reward, next.state. replay.memory might be filled
with experience sampled from a random policy.}

\item{replay.memory.size}{integer scalar: size of the replay memory}

\item{initial.replay.memory.size}{integer scalar: how much of the
replay memory is filled initially}

\item{batch.size}{scalar integer: batch size, how many samples are
drawn from the replay memory. Must be smaller than size of the
replay memory!}

\item{alpha}{positive scalar numeric: If alpha = 0 sampling
from replay memory will be uniform, otherwise observations with
high td error will be proportionally prioritized.}

\item{theta}{positive scalar numeric: theta is a small positive
constant that prevents the edge-case of transitions not being
revisited once their error is zero.}

\item{fixed.target}{scalar logical: Q-Learning with fixed target
network}

\item{update.target.after}{scalar integer: copy parameters to fixed
target network every n steps}

\item{double.qlearning}{logical scalar: whether to use double
qlearning}

\item{sigma}{scalar integer: sampling parameter, for sigma = 0 the
sarsa algorithm is obtained, for sigma = 1 expected sarsa.}

\item{v}{numeric vector: initial state value function v}

\item{iter}{integer: number of iterations}
}
\description{
Documentation of all parameters
}
