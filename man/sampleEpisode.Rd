% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predictMC.R
\name{sampleEpisode}
\alias{sampleEpisode}
\title{Sample episode}
\usage{
sampleEpisode(policy, envir, initial.state, initial.action = NULL)
}
\arguments{
\item{policy}{a policy specified as a probability matrix (states x actions)}

\item{envir}{the environment, an R6 class. See also \code{\link[=envir]{envir()}}.}

\item{initial.state}{integer: the initial state}

\item{initial.action}{character: the initial action. Default is NULL,
then first action will be sampled from policy.}
}
\value{
a list with sampled actions, states and returns of the episode.
}
\description{
Sample an episode in an environment given a policy.
Note that this only works for episodic environments
(e.g. there must be at least one terminal state). There is no action in the
last time step and no reward for the first time step:
\tabular{rrrrr}{
S_1 \tab S_2 \tab ... \tab S_T-1 \tab S_T \cr
A_1 \tab A_2 \tab ... \tab A_T-1 \tab NA \cr
NA \tab R_2 \tab ... \tab R_T-1 \tab R_T \cr
}
S1, S2, ..., ST-1, ST
A1, A2, ..., AT-1, NA
NA, R2, ..., RT-1, RT
}
\examples{
set.seed(26)
grid = gridworld$new(shape = c(4, 4), terminal.states = c(1, 16))

# Define random policy
random.policy = matrix(1 / grid$n.actions, nrow = grid$n.states, 
  ncol = grid$n.actions)

# Sample an episode for the random.policy
episode = sampleEpisode(random.policy, grid, initial.state = 3)
print(episode$actions)
print(episode$rewards)
print(episode$states)

# Specify an initial action
grid$setEpisodeOverFalse()
episode = sampleEpisode(random.policy, grid, initial.state = 7, 
  initial.action = "right")
print(episode$actions)
print(episode$rewards)
print(episode$states)

}
