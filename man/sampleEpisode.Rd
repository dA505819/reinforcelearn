% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predictMC.R
\name{sampleEpisode}
\alias{sampleEpisode}
\title{Sample episode}
\usage{
sampleEpisode(policy, envir, initial.state = NULL, initial.action = NULL)
}
\arguments{
\item{policy}{numeric matrix: a policy specified as a probability matrix (states x actions)}

\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{initial.state}{integer: the initial state. Default is NULL, then the
initial state will be sampled from the initial.states of the environment}

\item{initial.action}{integer: the initial action. Default is NULL,
then first action will also be sampled from policy.}
}
\value{
a list with sampled actions, states and returns of the episode.
}
\description{
Sample an episode in an environment given a policy.
Note that this only works for episodic environments
(e.g. there must be at least one terminal state). There is no action in the
last time step and no reward for the first time step:
\tabular{rrrrr}{
S_1 \tab S_2 \tab ... \tab S_T-1 \tab S_T \cr
A_1 \tab A_2 \tab ... \tab A_T-1 \tab NA \cr
NA \tab R_2 \tab ... \tab R_T-1 \tab R_T \cr
}
S1, S2, ..., ST-1, ST
A1, A2, ..., AT-1, NA
NA, R2, ..., RT-1, RT
}
\examples{
set.seed(26)
grid = gridworld$new()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix, terminal.states = grid$terminal.states,
  initial.state = grid$initial.state)
  
# Define random policy
random.policy = matrix(1 / Gridworld1$n.actions, nrow = Gridworld1$n.states, 
  ncol = Gridworld1$n.actions)

# Sample an episode using the random.policy
episode = sampleEpisode(random.policy, Gridworld1, initial.state = 3)
print(episode$actions)
print(episode$rewards)
print(episode$states)


}
