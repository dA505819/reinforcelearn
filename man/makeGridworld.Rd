% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/makeGridworld.R
\name{makeGridworld}
\alias{makeGridworld}
\title{Make Gridworld Environment}
\usage{
makeGridworld(shape = c(4L, 4L), terminal.states = c(0L, 15L))
}
\arguments{
\item{shape}{length-two integer, shape of the grid}

\item{terminal.states}{integer vector of the terminal states in the
gridworld, states are numerated starting with 0 with increasing
number from left to right}
}
\value{
R6 class
}
\description{
Simple gridworld environment for reinforcement learning.
}
\details{
The states are enumerated as follows (example 4x4 grid):
\tabular{rrrr}{
0 \tab 1 \tab 2 \tab 3 \cr
4 \tab 5 \tab 6 \tab 7 \cr
8 \tab 9 \tab 10 \tab 11 \cr
12 \tab 13 \tab 14 \tab 15 \cr
}
So a board position could look like this (T: terminal state, x: current state):
\tabular{rrrr}{
T \tab o \tab o \tab o \cr
o \tab o \tab o \tab o \cr
o \tab x \tab o \tab o \cr
o \tab o \tab o \tab T \cr
}
Possible actions include going left, right, down or up. If an action would
take you off the grid, you remain in the previous state. For each step you
get a reward of -1, until you reach into a terminal state.
}
\examples{
grid = makeGridworld()
grid = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix)
}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\link{makeEnvironment}
}
