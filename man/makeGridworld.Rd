% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/makeGridworld.R
\name{makeGridworld}
\alias{makeGridworld}
\title{Make Gridworld}
\usage{
makeGridworld(shape = c(4, 4), goal.states = c(0, 15),
  cliff.states = NULL, reward.step = -1, reward.cliff = -100,
  wind = rep(0, shape[2]), cliff.transition.states = NULL)
}
\arguments{
\item{shape}{[\code{integer(2)}] \cr 
Shape of the gridworld (number of rows x number of columns).}

\item{goal.states}{[\code{integer}] \cr 
Goal states in the gridworld.}

\item{cliff.states}{[\code{integer}] \cr 
Cliff states in the gridworld.}

\item{reward.step}{[\code{integer(1)}] \cr 
Reward for taking a step.}

\item{reward.cliff}{[\code{integer(1)}] \cr 
Reward for taking a step in the cliff state.}

\item{wind}{[\code{integer}] \cr 
Strength of the upward wind in each cell.}

\item{cliff.transition.states}{[\code{integer}] \cr 
States to which the environment transitions if stepping into the cliff. 
If it is a vector, all states will have equal probability.}
}
\value{
[\code{list(2)}] \cr
  Returns a list with the state transition array [\code{array(3)}] and reward matrix 
  [\code{matrix}] of the gridworld.
  These can then be passed on to \code{makeEnvironment} to create a full reinforcement 
  learning environment.
}
\description{
\code{makeGridworld} is used to create gridworld environments, 
which can be used as reinforcement learning problems.
}
\details{
In a gridworld the episodic task is to get from a start state to a goal state. 
The grid cells are the states.

Possible actions include going left (action 0), right (action 1), up (action 2) or down (action 3). 
If an action would take you off the grid, you remain in the previous state. For each step you
get a reward of \code{reward.step}, until you reach a goal state, then the episode is done.
 
When stepping into a cliff state you get a reward of \code{reward.cliff}, 
usually a high negative reward and transition to a state \code{cliff.transition.states}. 

In each column a deterministic wind specified via \code{wind} pushes you up a specific number of 
grid cells (for the next action).

The states are enumerated row-wise  and numeration starts with 0. 
Here is an example 4x4 grid:
\tabular{rrrr}{
 0 \tab 1 \tab 2 \tab 3 \cr
 4 \tab 5 \tab 6 \tab 7 \cr
 8 \tab 9 \tab 10 \tab 11 \cr
 12 \tab 13 \tab 14 \tab 15 \cr
}
So a board position could look like this (G: goal state, x: current state, C: cliff state):
\tabular{rrrr}{
 G \tab o \tab o \tab o \cr
 o \tab o \tab o \tab o \cr
 o \tab x \tab o \tab o \cr
 o \tab o \tab o \tab C \cr
}
}
\examples{
# Gridworld Environment (Sutton & Barto Example 4.1)
gridworld = makeGridworld()
env = makeEnvironment(transition.array = gridworld$transitions, 
  reward.matrix = gridworld$rewards)
  
# Windy Gridworld (Sutton & Barto Example 6.5) 
windy.gridworld = makeGridworld(shape = c(7, 10), goal.states = 37, 
  reward.step = - 1, wind = c(0, 0, 0, 1, 1, 1, 2, 2, 1, 0))
env = makeEnvironment(transition.array = windy.gridworld$transitions, 
  reward.matrix = windy.gridworld$rewards, initial.state = 30)
  
# Cliff Walking (Sutton & Barto Example 6.6)   
cliff = makeGridworld(shape = c(4, 12), goal.states = 47, cliff.states = 37:46, 
  reward.step = - 1, reward.cliff = - 100, cliff.transition.states = 36)
env = makeEnvironment(transition.array = cliff$transitions, 
  reward.matrix = cliff$rewards, initial.state = 36) 
  
}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction
}
\seealso{
gridworld

windy.gridworld

cliff

\code{\link{makeEnvironment}}
}
