% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/makeGridworld.R
\name{makeGridworld}
\alias{makeGridworld}
\title{Make Gridworld}
\usage{
makeGridworld(shape = c(4, 4), goal.states = c(1, 16),
  cliff.states = NULL, reward.step = -1, reward.cliff = -100,
  diagonal.moves = FALSE, wind = rep(0, shape[2]),
  cliff.transition.states = NULL, stochasticity = 0)
}
\arguments{
\item{shape}{[\code{integer(2)}] \cr 
Shape of the gridworld (number of rows x number of columns).}

\item{goal.states}{[\code{integer}] \cr 
Goal states in the gridworld.}

\item{cliff.states}{[\code{integer}] \cr 
Cliff states in the gridworld.}

\item{reward.step}{[\code{integer(1)}] \cr 
Reward for taking a step.}

\item{reward.cliff}{[\code{integer(1)}] \cr 
Reward for taking a step in the cliff state.}

\item{diagonal.moves}{[\code{logical(1)}] \cr
Should diagonal moves be allowed?}

\item{wind}{[\code{integer}] \cr 
Strength of the upward wind in each cell.}

\item{cliff.transition.states}{[\code{integer}] \cr 
States to which the environment transitions if stepping into the cliff. 
If it is a vector, all states will have equal probability.}

\item{stochasticity}{[\code{numeric(1)}] \cr
Probability of random transition to any of the neighboring states when taking any action.}
}
\value{
[\code{list(2)}] \cr
  Returns a list with the state transition array [\code{array(3)}] and reward matrix 
  [\code{matrix}] of the gridworld.
  These can then be passed on to \code{\link{makeEnvironment}} to create a full reinforcement 
  learning environment.
}
\description{
\code{makeGridworld} creates gridworlds, which can be used as reinforcement learning problems.
}
\details{
A gridworld is an episodic navigation task, the goal is to get from start state to goal state. 

Possible actions include going left, right, up or down. If \code{diagonal.moves = TRUE} diagonal 
moves are also possible, leftup, leftdown, rightup and rightdown.
 
When stepping into a cliff state you get a reward of \code{reward.cliff}, 
usually a high negative reward and transition to a state specified by \code{cliff.transition.states}. 

In each column a deterministic wind specified via \code{wind} pushes you up a specific number of 
grid cells (for the next action).

A stochastic gridworld is a gridworld where with probability \code{stochasticity} the next state
is chosen at random from all neighbor states independent of the actual action.

If an action would take you off the grid, the new state is the nearest cell inside the grid. 
For each step you get a reward of \code{reward.step}, until you reach a goal state, 
then the episode is done.

States are enumerated row-wise and numeration starts with 1. 
Here is an example 4x4 grid:
\tabular{rrrr}{
 1 \tab 2 \tab 3 \tab 4 \cr
 5 \tab 6 \tab 7 \tab 8 \cr
 9 \tab 10 \tab 11 \tab 12 \cr
 13 \tab 14 \tab 15 \tab 16 \cr
}
So a board position could look like this (G: goal state, x: current state, C: cliff state):
\tabular{rrrr}{
 G \tab o \tab o \tab o \cr
 o \tab o \tab o \tab o \cr
 o \tab x \tab o \tab o \cr
 o \tab o \tab o \tab C \cr
}

A few gridworlds are already included in the package and can be loaded by typing 
\code{\link{gridworld}}, \code{\link{windy.gridworld}} and \code{\link{cliff}}.
}
\examples{
# Gridworld Environment (Sutton & Barto Example 4.1)
gridworld = makeGridworld()
env = makeEnvironment(transitions = gridworld$transitions, 
  rewards = gridworld$rewards)
  
# Windy Gridworld (Sutton & Barto Example 6.5) 
windy.gridworld = makeGridworld(shape = c(7, 10), goal.states = 38, 
  reward.step = - 1, wind = c(0, 0, 0, 1, 1, 1, 2, 2, 1, 0))
env = makeEnvironment(transitions = windy.gridworld$transitions, 
  rewards = windy.gridworld$rewards, initial.state = 31 - 1)
  
# Cliff Walking (Sutton & Barto Example 6.6)   
cliff = makeGridworld(shape = c(4, 12), goal.states = 48, cliff.states = 38:47, 
  reward.step = - 1, reward.cliff = - 100, cliff.transition.states = 37)
env = makeEnvironment(transitions = cliff$transitions, 
  rewards = cliff$rewards, initial.state = 37 - 1) 

}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction
}
\seealso{
\code{\link{makeEnvironment}}
}
