% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Gridworld.R
\name{makeGridworld}
\alias{makeGridworld}
\title{Make Gridworld Environment}
\usage{
makeGridworld(shape = c(4L, 4L), terminal.states = c(0L, 15L))
}
\arguments{
\item{shape}{[\code{integer(2)}] \cr 
Shape of the gridworld}

\item{terminal.states}{[\code{integer}] \cr 
Terminal states in the 
gridworld, states are numerated starting with 0 with increasing 
number from left to right}
}
\value{
[\code{R6 class}] \cr
  Returns the gridworld environment.
}
\description{
Simple gridworld environment for reinforcement learning.
}
\details{
The states are enumerated as follows (example 4x4 grid):
\tabular{rrrr}{
 0 \tab 1 \tab 2 \tab 3 \cr
 4 \tab 5 \tab 6 \tab 7 \cr
 8 \tab 9 \tab 10 \tab 11 \cr
 12 \tab 13 \tab 14 \tab 15 \cr
}
So a board position could look like this (T: terminal state, x: current state):
\tabular{rrrr}{
 T \tab o \tab o \tab o \cr
 o \tab o \tab o \tab o \cr
 o \tab x \tab o \tab o \cr
 o \tab o \tab o \tab T \cr
}
Possible actions include going left, right, down or up. If an action would
take you off the grid, you remain in the previous state. For each step you
get a reward of -1, until you reach into a terminal state.
}
\examples{
grid = makeGridworld()
grid = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix)
}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction
}
\seealso{
\code{\link{makeEnvironment}}
}
