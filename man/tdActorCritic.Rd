% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/actorCritic.R
\name{tdActorCritic}
\alias{tdActorCritic}
\title{TD Actor Critic}
\usage{
tdActorCritic(envir, fun.approx = "table", policy = "softmax",
  preprocessState = identity, n.episodes = 100, discount = 1,
  alpha = 0.01, beta = 0.1, lambda = 0)
}
\description{
TD Actor Critic
}
\examples{
# Variant of cliff walking
library(reinforcelearn)
rewardFun = function(state, action, n.state) {
  if (n.state \%in\% 37:46) {
    return(- 100)
  } else {
    return(- 1)
  }
}
env = makeGridworld(shape = c(4, 12), goal.states = 47,
  cliff.states = 37:46, reward.step = - 1, reward.cliff = - 100,
  cliff.transition.done = TRUE, initial.state = 36, sampleReward = rewardFun)

tdActorCritic(env, n.episodes = 300)

# Mountain Car
m = MountainCar()

# Define preprocessing function (we use grid tiling)
n.tilings = 8
max.size = 4096
iht = IHT(max.size)

position.max = m$state.space.bounds[[1]][2]
position.min = m$state.space.bounds[[1]][1]
velocity.max = m$state.space.bounds[[2]][2]
velocity.min = m$state.space.bounds[[2]][1]
position.scale = n.tilings / (position.max - position.min)
velocity.scale = n.tilings / (velocity.max - velocity.min)

# Scale state first, then get active tiles and return n hot vector
preprocessState = function(state) {
  state = c(position.scale * state[1], velocity.scale * state[2])
  active.tiles = tiles(iht, 8, state)
  makeNHot(active.tiles, max.size, out = "vector")
}

# Linear function approximation and softmax policy
tdActorCritic(m, fun.approx = "linear", preprocessState = preprocessState)

#----------------
# Mountain Car with continuous action space
m2 = MountainCar(action.space = "Continuous")

# Linear function approximation and gaussian policy
tdActorCritic(m, fun.approx = "linear", policy = "gaussian", 
  preprocessState = preprocessState)

}
