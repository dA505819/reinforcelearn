% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dqlearning.R
\name{dqlearning}
\alias{dqlearning}
\title{Double Q-Learning}
\usage{
dqlearning(envir, n.episodes = 10, alpha = 0.1, epsilon = 0.1,
  discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{the environment, an R6 class. See also \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: the number of episodes}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update. Useful in non-stationary environments, giving high
value to the last observed returns.}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more exploration.}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal value function Q
}
\description{
The idea behind Double Q-Learning is to have two separate action value
functions Q1 and Q2. Actions are chosen from an epsilon-greedy policy derived
from Q1 + Q2. With equal probability one of Q1 and Q2 is then updated
following the same update rule as in Q-Learning. This avoids maximization bias.
The update formulas are:
\deqn{Q1(S, A) <- Q1(S, A) + \alpha[R + \gamma Q2(S', argmax_a Q1(S', a)) - Q1(S, A)]}
\deqn{Q2(S, A) <- Q2(S, A) + \alpha[R + \gamma Q1(S', argmax_a Q2(S', a)) - Q2(S, A)]}
}
\examples{
grid = gridworld$new()
# Q = dqlearning(grid, n.episodes = 1000) # not working
}
\references{
\href{https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf#page=163}{Sutton and Barto (2017) page 145}
}
\seealso{
\link{qlearning}
}
