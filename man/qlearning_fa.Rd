% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlearning_fa.R
\name{qlearning_fa}
\alias{qlearning_fa}
\title{Q-Learning with Function Approximation}
\usage{
qlearning_fa(envir, makeFeatureVector, predict, train, n.episodes = 10,
  epsilon = 0.1, epsilon.decay = 0.5, discount.factor = 1, seed = NULL,
  ...)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{makeFeatureVector}{function: takes a state observation
as input and returns a preprocessed state, e.g. a one-hot vector}

\item{predict}{function: predict returns vector of q values for a
given preprocessed state observation}

\item{train}{function: train the model, update the weights}

\item{n.episodes}{scalar integer: the number of episodes}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
every 100 episodes by multiplying with this factor}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}

\item{...}{arguments passed on to makeFeatureVector, predict or train}
}
\value{
list with entries weights and the number of steps.per.episode
}
\description{
To represent the Q values of how good an action is in a given state,
a function approximator is used, which the user passes on to this
function.
}
\details{
Three functions need to be passed on to the qlearnin algorithm:
\itemize{
\item \code{makeFeatureVector(state_, ...)} takes the state observation from the
environment and does some preprocessing (e.g. for an image
greyscaling) or discretizes a continuous state space (e.g. grid
tilings) and returns a vector.
\item \code{predict(inputs_, ...)} returns a vector of Q values for a given
preprocessed state
\item \code{train(inputs_, outputs_, predictions_, ...)} updates the weights
based on some learning method, e.g. gradient descent
(minimizing the loss between predicted values and some true ouputs)
}
}
\examples{
# Define the environment
grid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array,
  reward.matrix = grid$reward.matrix,
  terminal.states = grid$terminal.states,
  initial.state = 30)
  
# Define a tensorflow graph for a neural network.
library(tensorflow)
tf$reset_default_graph()
inputs = tf$placeholder(tf$float32, shape(1L, WindyGridworld1$n.states))
weights = tf$Variable(tf$random_uniform(shape(WindyGridworld1$n.states, 
  WindyGridworld1$n.actions), 0, 0.01))
Q = tf$matmul(inputs, weights)

nextQ = tf$placeholder(tf$float32, shape(1L, WindyGridworld1$n.actions))
loss = tf$reduce_sum(tf$square(nextQ - Q))
optimizer = tf$train$GradientDescentOptimizer(learning_rate = 0.1)
trainModel = optimizer$minimize(loss)

# initialize the session and the weights
sess = tf$Session()
sess$run(tf$global_variables_initializer())

# takes the state and returns a one-hot vector
makeFeatureVector = function(state_) {
  one_hot = matrix(rep(0L, WindyGridworld1$n.states), nrow = 1L)
  one_hot[1L, state_ + 1L] = 1L
  one_hot
}

# predict returns vector of q values for a given state
predict = function(inputs_) {
  sess$run(Q, feed_dict = dict(inputs = inputs_))
}

# train model, update weights, e.g. gradient descent: this is supervised learning
train = function(inputs_, outputs_, predictions_ = NULL) {
  sess$run(tuple(trainModel, weights),
    feed_dict = dict(inputs = inputs_, nextQ = outputs_))
}

res = qlearning_fa(WindyGridworld1, makeFeatureVector, predict, train, 
  n.episodes = 100, seed = 123)

}
\seealso{
\link{qlearning}
}
