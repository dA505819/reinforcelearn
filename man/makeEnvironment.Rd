% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/makeEnvironment.R
\name{makeEnvironment}
\alias{makeEnvironment}
\title{Make Reinforcement Learning Environment}
\usage{
makeEnvironment(gym.envir.name = NULL,
  gym.api.path = getOption("gym.api.path"), max.steps.episode = 200,
  transition.array = NULL, reward.matrix = NULL, terminal.states = NULL,
  initial.state = NULL, ...)
}
\arguments{
\item{gym.envir.name}{scalar character, e.g. "FrozenLake-v0",
see \href{https://gym.openai.com/envs}{OpenAI Gym} for possible environments.}

\item{gym.api.path}{character path to your gym-http-api folder, best practise
is to set this using options(gym.api.path = "your_path")}

\item{max.steps.episode}{scalar integer: maximal number of steps allowed in
environment}

\item{transition.array}{numerical matrix (n.states x n.states x n.actions)
for each action giving the probabilities for transitions from one state to
all other states}

\item{reward.matrix}{numerical matrix the rewards for transitions from one
state to another}

\item{terminal.states}{integer vector: terminal.states of MDP. Note that
states are numerated starting with 0.}

\item{initial.state}{scalar integer or integer vector: the starting state. If
a vector is given a starting state will be randomly sampled from this
vector when reset is called. Note that states are numerated starting with
0.}

\item{...}{not used}
}
\value{
Reinforcement Learning Environment, an R6 class.
}
\description{
This function creates an environment for reinforcement learning.
You could either use an existing environment from OpenAI Gym or specify the
transition array and reward matrix for a Markov Decision Process.
}
\details{
State and action space can be either "Discrete" or "Box". For the discrete
case states and actions are numerated starting from 0.
}
\section{Methods}{
 \describe{
\item{\code{envir$initialize()}}{Creates a new environment.}
\item{\code{envir$step(action, render = TRUE)}}{
Takes a step in the environment given an action,
returns the next state, reward and if the episode is finished (logical).
If a transition array and reward matrix are given, the next step will be
sampled from the MDP, else the step \link[gym:env_step]{gym::env_step} function will be called.
If render = TRUE the environment will be rendered.}
\item{\code{envir$reset()}}{Resets the
\code{episode.over} flag of the environment and returns an initial state.
Useful when starting a new episode.}
}
}

\examples{
\dontrun{
# Create an environment from an OpenAI Gym environment.
# Make sure you have gym-http-api and python installed.
# Then run in command line: python gym_http_server.py to start a server.
FrozenLake = makeEnvironment("FrozenLake-v0")
FrozenLake$reset()
FrozenLake$step(action = 0)

# Now we can start a new FrozenLake environment by running:
FrozenLake$initialize()

# Create the MountainCar environment which has a continuous state space.
MountainCar = makeEnvironment("MountainCar-v0")
}

# Create an environment from a transition array and reward matrix.
grid = gridworld$new()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix, terminal.states = grid$terminal.states,
  initial.state = grid$initial.state)
  
# Create the WindyGridworld environment.
windygrid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = windygrid$transition.array, 
  reward.matrix = windygrid$reward.matrix, 
  terminal.states = windygrid$terminal.states, 
  initial.state = windygrid$initial.state)
}
\seealso{
\href{https://gym.openai.com/docs}{OpenAI Gym}
}
