% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/makeEnvironment.R
\name{makeEnvironment}
\alias{makeEnvironment}
\title{Make Reinforcement Learning Environment}
\usage{
makeEnvironment(gym.envir.name = NULL, transitions = NULL, rewards = NULL,
  initial.state = NULL, reset = NULL, sampleReward = NULL,
  render = TRUE)
}
\arguments{
\item{gym.envir.name}{[\code{character(1)}] \cr
Name of Gym environment, e.g. "CartPole-v0", have a look at 
\url{https://gym.openai.com/envs}.}

\item{transitions}{[\code{array (n.states x n.states x n.actions)}] \cr
Transition array: For each action specifying the probabilities for 
transitions between states. Only used for MDPs.}

\item{rewards}{[\code{matrix (n.states x n.actions)} or \code{array (n.states x n.states x n.actions)}] \cr 
Reward array: This can be a matrix (n.states x n.actions) or a 3-dimensional array 
(n.states x n.states x n.actions). The reward will be sampled from the specified array 
depending on state, action and possibly also the next state. 
Only used for MDPs.}

\item{initial.state}{[\code{integer}] \cr
The starting state if \code{reset} is \code{NULL} else this argument is unused.
If a vector is given a starting state will be
randomly sampled from this vector when \code{reset} is called. 
Note that states are numerated starting with 
0. If \code{initial.state = NULL} all states are possible initial states. Only used for MDPs.}

\item{reset}{[\code{function}] \cr 
Function that returns an initial state observation, takes no arguments. Only used for MDPs.}

\item{sampleReward}{[\code{function}] \cr 
Function that returns the next reward given the current state, action and next state. 
Otherwise the reward will be sampled from the reward array of the MDP specified 
by \code{rewards}. Only used for MDPs.}

\item{render}{[\code{logical(1)}] \cr 
Whether to render the Gym environment. If \code{TRUE} a python window 
with a graphical interface opens when steps are sampled in the 
environment for a gym environment.}
}
\value{
[\code{R6 class}] \cr 
  Reinforcement Learning Environment
}
\description{
This function creates an environment for reinforcement learning. 
You could either use an existing environment from OpenAI Gym or 
specify the transition array and reward matrix of a 
Markov Decision Process.
}
\details{
State and action space can be either "Discrete" or "Box". In the 
discrete case states and actions are numerated starting from 0.
}
\section{Methods}{
 \describe{
\item{\code{envir$initialize()}}{Creates a new environment.} 
\item{\code{envir$step(action, render)}}{
  Takes a step in the environment given an action,
  returns the next state, reward and if the episode has finished. 
  If a transition array and reward matrix are given, the next step 
  will be sampled from the MDP, else the step 
  \code{\link[gym]{env_step}} function will be called.
  If \code{render = TRUE} a Gym environment will be rendered.} 
\item{\code{envir$reset()}}{Resets the
  \code{done} flag of the environment and returns an initial state.
   Useful when starting a new episode.}
\item{\code{envir$close()}}{Close the python window for a gym 
  environment.}   
}
}

\examples{
\dontrun{
# Create an OpenAI Gym environment.
# Make sure you have Python and Gym installed.
# Note: If makeEnvironment returns an error, this is a bug, please run the code again!
CartPole = makeEnvironment("CartPole-v0")
CartPole$reset()
CartPole$step(action = 0)
CartPole$close()

# Create the MountainCar environment which has a continuous state space.
MountainCar = makeEnvironment("MountainCar-v0")

MountainCar$state.space
MountainCar$state.space.bounds
}

# Create an environment from a transition array and reward matrix (here a 4x4 gridworld).
grid = makeEnvironment(transitions = gridworld$transitions,
  rewards = gridworld$rewards)
  
# Specify a custom probability distribution for the starting state.
resetGrid = function() {
  p = c(0, 0.2, 0, 0.15, 0.15, 0, 0, 0, 0, 0.3, 0.2, 0, 0, 0, 0, 0)
  sample(0:15, prob = p, size = 1)
}

grid = makeEnvironment(transitions = gridworld$transitions,
  rewards = gridworld$rewards, reset = resetGrid)
grid$reset()
print(grid$state)

# Specify a custom reward function.
sampleReward = function(state, action, n.state) {
  if (state == 2 & action == 1L) {
    rexp(1)
  } else {
    rnorm(1)
  }
}

grid = makeEnvironment(transitions = gridworld$transitions,
  rewards = gridworld$rewards, sampleReward = sampleReward)
grid$reset()
grid$step(2)
print(grid$reward)
  
# Create the Windy Gridworld from transition array and reward matrix.
grid = makeEnvironment(transitions = windy.gridworld$transitions,
  rewards = windy.gridworld$rewards,
  initial.state = 30)
  
# Create the Cliff Walking Gridworld from transition array and reward matrix.
grid = makeEnvironment(transitions = cliff$transitions,
  rewards = cliff$rewards,
  initial.state = 36)
  
}
\seealso{
\url{https://github.com/openai/gym-http-api}
}
