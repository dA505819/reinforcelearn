% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/makeEnvironment.R
\name{makeEnvironment}
\alias{makeEnvironment}
\title{Make Reinforcement Learning Environment}
\usage{
makeEnvironment(gym.envir.name = NULL, transition.array = NULL,
  reward.matrix = NULL, initial.state = NULL, render = TRUE)
}
\arguments{
\item{gym.envir.name}{[\code{character(1)}] \cr
Name of Gym environment, e.g. "CartPole-v0", have a look at 
\url{https://gym.openai.com/envs}.}

\item{transition.array}{[\code{array (n.states x n.states x n.actions)}] \cr
Transition array: For each action specifying the probabilities for 
transitions between states.}

\item{reward.matrix}{[\code{matrix (n.states x n.actions)}] \cr 
Reward matrix: The reward for taking action a in state s.}

\item{initial.state}{[\code{integer}] \cr
The starting state. If a vector is given a starting state will be
randomly sampled from this vector when \code{reset} is called. 
Note that states are numerated starting with 
0. If \code{NULL} all states are possible initial states.}

\item{render}{[\code{logical(1)}] \cr 
Whether to render the environment. If \code{TRUE} a python window 
with a graphical interface opens when steps are sampled in the 
environment for a gym environment.}
}
\value{
[\code{R6 class}] \cr 
  Reinforcement Learning Environment
}
\description{
This function creates an environment for reinforcement learning. 
You could either use an existing environment from OpenAI Gym or 
specify the transition array and reward matrix of a 
Markov Decision Process.
}
\details{
State and action space can be either "Discrete" or "Box". In the 
discrete case states and actions are numerated starting from 0.
}
\section{Methods}{
 \describe{
\item{\code{envir$initialize()}}{Creates a new environment.} 
\item{\code{envir$step(action, render = TRUE)}}{
  Takes a step in the environment given an action,
  returns the next state, reward and if the episode has finished. 
  If a transition array and reward matrix are given, the next step 
  will be sampled from the MDP, else the step 
  \code{\link[gym]{env_step}} function will be called.
  If \code{render = TRUE} a Gym environment will be rendered.} 
\item{\code{envir$reset()}}{Resets the
  \code{done} flag of the environment and returns an initial state.
   Useful when starting a new episode.}
\item{\code{envir$close()}}{Close the python window for a gym 
  environment.}   
}
}

\examples{
\dontrun{
# Create an OpenAI Gym environment.
# Make sure you have Python and Gym installed.
CartPole = makeEnvironment("CartPole-v0")
CartPole$reset()
CartPole$step(action = 0)
CartPole$close()

# Create the MountainCar environment which has a continuous state space.
MountainCar = makeEnvironment("MountainCar-v0")

MountainCar$state.space
MountainCar$state.space.bounds
}

# Create an environment from a transition array and reward matrix (here a simple gridworld).
grid = makeEnvironment(transition.array = gridworld$transitions,
  reward.matrix = gridworld$rewards)
  
# Create the WindyGridworld environment from transition array and reward matrix.
grid = makeEnvironment(transition.array = windyGridworld$transitions,
  reward.matrix = windyGridworld$rewards,
  initial.state = 30L)
  
}
\seealso{
\url{https://github.com/openai/gym-http-api}
}
