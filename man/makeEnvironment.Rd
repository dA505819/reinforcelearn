% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/makeEnvironment.R
\name{makeEnvironment}
\alias{makeEnvironment}
\title{Make Reinforcement Learning Environment}
\usage{
makeEnvironment(gym.envir.name = NULL, transition.array = NULL,
  reward.matrix = NULL, initial.state = NULL, render = TRUE)
}
\arguments{
\item{gym.envir.name}{scalar character, e.g. "CartPole-v0",
see \href{https://gym.openai.com/envs}{OpenAI Gym} for possible environments.}

\item{transition.array}{numerical matrix (n.states x n.states x n.actions)
for each action giving the probabilities for transitions from one state to
all other states}

\item{reward.matrix}{numerical matrix the rewards for transitions from one
state to another}

\item{initial.state}{scalar integer or integer vector: the starting state. If
a vector is given a starting state will be randomly sampled from this
vector when reset is called. Note that states are numerated starting with
0. If NULL all states are possible initial states.}

\item{render}{logical scalar: whether to render the environment}
}
\value{
Reinforcement Learning Environment, an R6 class.
}
\description{
This function creates an environment for reinforcement learning.
You could either use an existing environment from OpenAI Gym or specify the
transition array and reward matrix for a Markov Decision Process.
}
\details{
State and action space can be either "Discrete" or "Box". For the discrete
case states and actions are numerated starting from 0.
}
\section{Methods}{
 \describe{
\item{\code{envir$initialize()}}{Creates a new environment.}
\item{\code{envir$step(action, render = TRUE)}}{
Takes a step in the environment given an action,
returns the next state, reward and if the episode is finished (logical).
If a transition array and reward matrix are given, the next step will be
sampled from the MDP, else the step \link[gym:env_step]{gym::env_step} function will be called.
If render = TRUE the environment will be rendered.}
\item{\code{envir$reset()}}{Resets the
\code{episode.over} flag of the environment and returns an initial state.
Useful when starting a new episode.}
\item{\code{envir$close()}}{Close the python window for a gym environment.}
}
}

\examples{
\dontrun{
# Create an OpenAI Gym environment.
# Make sure you have Python and Gym installed.
CartPole = makeEnvironment("CartPole-v0")
CartPole$reset()
CartPole$step(action = 0)
CartPole$close()

# Create the MountainCar environment which has a continuous state space.
MountainCar = makeEnvironment("MountainCar-v0")
}

# Create an environment from a transition array and reward matrix (here a simple gridworld).
grid = gridworld$new()
Gridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix)
  
# Create the WindyGridworld environment.
grid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix, initial.state = 30)
  
}
\seealso{
\href{https://gym.openai.com/docs}{OpenAI Gym}
}
