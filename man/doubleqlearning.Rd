% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/double_qlearning.R
\name{doubleqlearning}
\alias{doubleqlearning}
\title{Double Q-Learning (Table-lookup)}
\usage{
doubleqlearning(envir, n.episodes = 10, learning.rate = 0.1,
  epsilon = 0.1, epsilon.decay = 0.5, discount.factor = 1, seed = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: the number of episodes}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{epsilon}{scalar numeric between 0 and 1: proportion of random samples
in epsilon-greedy behaviour policy. Higher values of epsilon lead to more
exploration.}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
every 100 episodes by multiplying with this factor}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal action value function
}
\description{
Off-policy TD control algorithm. Double Q-Learning finds the optimal
action value function Q independent of the policy followed.
The idea behind Double Q-Learning is to have two separate action value
functions Q1 and Q2. Actions are chosen from an epsilon-greedy policy derived
from Q1 + Q2. With equal probability either Q1 or Q2 is then updated
following the same update rule as in Q-Learning. This avoids maximization bias.
}
\details{
Under the assumption that all state-action pairs are visited (which is
achieved using a stochastic epsilon-greedy policy) Double Q-Learning converges to
the optimal action value function Q*.
The update formulas are:
\deqn{Q1(S, A) <- Q1(S, A) + \alpha [R + \gamma Q2(S', argmax_a Q1(S', a)) - Q1(S, A)]}
\deqn{Q2(S, A) <- Q2(S, A) + \alpha [R + \gamma Q1(S', argmax_a Q2(S', a)) - Q2(S, A)]}
}
\examples{
# Solve the WindyGridworld environment using Q-Learning
windygrid = WindyGridworld$new()
WindyGridworld1 = makeEnvironment(transition.array = windygrid$transition.array,
  reward.matrix = windygrid$reward.matrix,
  terminal.states = windygrid$terminal.states,
  initial.state = 30)
res = doubleqlearning(WindyGridworld1, n.episodes = 100, seed = 123)

}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\link{qlearning}
}
