% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MonteCarloMethods.R
\name{MonteCarloControl}
\alias{MonteCarloControl}
\title{On-policy Monte Carlo Control (Table-lookup)}
\usage{
MonteCarloControl(envir, n.episodes = 100L, discount.factor = 1,
  learning.rate = 0.1, epsilon = 0.1, initial.policy = NULL)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr 
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{n.episodes}{[\code{integer(1)}] \cr 
Number of episodes.}

\item{discount.factor}{[\code{numeric(1) in [0,1]}] \cr 
Discounting future rewards.}

\item{learning.rate}{[\code{numeric(1) in [0,1]}] \cr 
Learning rate (step size).}

\item{epsilon}{[\code{numeric(1) in [0,1]}] \cr 
Ratio of random exploration in 
epsilon-greedy action selection}

\item{initial.policy}{[\code{matrix}] \cr 
A policy specified as a probability matrix (states x actions)}
}
\value{
[\code{list(2)}] \cr
  Returns the action value function Q and policy
}
\description{
Find optimal policy by sampling full episodes of experience and 
estimate values of every state-action pair. 
To ensure that every state-action pair is visited, an epsilon-greedy policy is used, 
so that every state-action pair has a positive probability of being selected.
}
\details{
Works only for episodic tasks (i.e. there must be a terminal state)! 
This method uses the first-visit Monte Carlo policy evaluation.
}
\examples{
grid = makeEnvironment(transition.array = windyGridworld$transitions,
  reward.matrix = windyGridworld$rewards,
  initial.state = 30L)
# res = MonteCarloControl(grid, n.episodes = 100)
}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction
}
\seealso{
\code{\link{predictMonteCarlo}}
}
