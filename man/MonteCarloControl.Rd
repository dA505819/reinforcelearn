% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MonteCarloControl.R
\name{MonteCarloControl}
\alias{MonteCarloControl}
\title{On-policy Monte Carlo Control (Table-lookup)}
\usage{
MonteCarloControl(envir, n.episodes = 100L, discount.factor = 1,
  learning.rate = 0.1, epsilon = 0.1, initial.policy = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{n.episodes}{scalar integer: number of episodes}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{epsilon}{scalar numeric: ratio of random exploration in
epsilon-greedy action selection}

\item{initial.policy}{numeric matrix: a policy specified as a probability
matrix (states x actions)}
}
\value{
list with action value function Q and policy
}
\description{
Find optimal policy by sampling full episodes of experience and
estimate values of every state-action pair.
To ensure that every state-action pair is visited, an epsilon-greedy policy is used,
so that every state-action pair has a positive probability of being selected.
}
\details{
Works only for episodic tasks (i.e. there must be a terminal state)!
This method uses the first-visit Monte Carlo policy evaluation.
}
\examples{
grid = makeEnvironment(transition.array = windyGridworld$transitions,
  reward.matrix = windyGridworld$rewards,
  initial.state = 30L)
# res = MonteCarloControl(grid, n.episodes = 100)
}
\references{
Sutton and Barto (Book draft 2016): Reinforcement Learning: An Introduction
}
\seealso{
\link{predictMonteCarlo}
}
