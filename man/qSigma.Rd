% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qSigma.R, R/tdControl.R
\name{qSigma}
\alias{qSigma}
\alias{qlearning}
\alias{sarsa}
\alias{expectedSarsa}
\title{Q Sigma}
\usage{
qSigma(envir, value.function = "table", sigma = 1, lambda = 0,
  n.episodes = 100, learning.rate = 0.1, epsilon = 0.1,
  epsilon.decay = 0.5, epsilon.decay.after = 100, initial.value = 0,
  discount.factor = 1, on.policy = TRUE, double.learning = FALSE,
  replay.memory = NULL, replay.memory.size = 1, batch.size = 1,
  alpha = 0, theta = 0.01, model = NULL, preprocessState = NULL,
  update.target.after = 1)

qlearning(envir, value.function = "table", lambda = 0, n.episodes = 100,
  learning.rate = 0.1, epsilon = 0.1, epsilon.decay = 0.5,
  epsilon.decay.after = 100, initial.value = 0, discount.factor = 1,
  double.learning = FALSE, replay.memory = NULL, replay.memory.size = 1,
  batch.size = 1, alpha = 0, theta = 0.01, model = NULL,
  preprocessState = NULL, update.target.after = 1)

sarsa(envir, value.function = "table", lambda = 0, n.episodes = 100,
  learning.rate = 0.1, epsilon = 0.1, epsilon.decay = 0.5,
  epsilon.decay.after = 100, initial.value = 0, discount.factor = 1,
  double.learning = FALSE, replay.memory = NULL, replay.memory.size = 1,
  batch.size = 1, alpha = 0, theta = 0.01, model = NULL,
  preprocessState = NULL, update.target.after = 1)

expectedSarsa(envir, value.function = "table", lambda = 0,
  n.episodes = 100, learning.rate = 0.1, epsilon = 0.1,
  epsilon.decay = 0.5, epsilon.decay.after = 100, initial.value = 0,
  discount.factor = 1, double.learning = FALSE, replay.memory = NULL,
  replay.memory.size = 1, batch.size = 1, alpha = 0, theta = 0.01,
  model = NULL, preprocessState = NULL, update.target.after = 1)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr 
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{value.function}{[\code{character(1)}] \cr 
How to represent the value function? Currently are \code{"table"} 
and \code{"neural.network"} are supported.}

\item{sigma}{[\code{numeric(1) in [0, 1]}] \cr 
Parameter of the Q(sigma) algorithm. It controls if the temporal-difference target 
is equal to the sarsa target (for \code{sigma = 1}) or the expected sarsa target 
(for \code{sigma = 0}). For intermediate values of \code{sigma} a weighted mean 
between the two targets is used.}

\item{lambda}{[\code{numeric(1) in [0, 1]}] \cr 
The \code{lambda} parameter combines different n-step returns using eligibility traces. 
Not implemented!}

\item{n.episodes}{[\code{integer(1)}] \cr 
Number of episodes.}

\item{learning.rate}{[\code{R6 class}] \cr 
Learning rate used for gradient descent.}

\item{epsilon}{[\code{numeric(1) in [0, 1]}] \cr 
Ratio of random exploration in epsilon-greedy action selection.}

\item{epsilon.decay}{[\code{numeric(1) in [0, 1]}] \cr 
Decay \code{epsilon} by multiplying with this factor.}

\item{epsilon.decay.after}{[\code{integer(1)}] \cr 
Number of episodes after which to decay epsilon.}

\item{initial.value}{[\code{numeric(1)}] \cr 
Initial value for the value function. Only used with a tabular value function. 
Set this to the maximal possible reward to encourage
exploration (optimistic initialization).}

\item{discount.factor}{[\code{numeric(1) in [0, 1]}] \cr 
Discount factor.}

\item{on.policy}{[\code{logical(1)}] \cr 
Should the temporal-difference target be computed on-policy 
using the epsilon-greedy behavior policy or off-policy using a greedy policy in the 
expected sarsa part of the update.}

\item{double.learning}{[\code{logical(1)}] \cr 
Should double learning be used?}

\item{replay.memory}{[\code{list}] \cr 
Initial replay memory, which can be passed on. Make sure, it is the same size as 
\code{replay.memory.size}. When omitted, the replay memory will be 
initially filled with random experiences.}

\item{replay.memory.size}{[\code{integer(1)}] \cr 
Size of the replay memory.}

\item{batch.size}{[\code{integer(1)}] \cr 
Batch size, how many experiences are sampled from the replay memory at each step? 
Must be smaller than size of the replay memory!}

\item{alpha}{[\code{numeric(1) in [0, 1]}] \cr 
If \code{alpha = 0} sampling from replay memory will be uniform, otherwise observations with
high temporal-difference error will be proportionally prioritized. Not implemented!}

\item{theta}{[\code{numeric(1) in (0, 1]}] \cr 
\code{theta} is a small positive constant that prevents the edge-case of transitions not being 
revisited once their error is zero. Not implemented!}

\item{model}{[\code{keras model}] \cr 
A neural network model specified using the \code{keras} package. 
See Details for more information.}

\item{preprocessState}{[\code{function}] \cr 
A function that takes the state observation returned from the environment as an input and 
preprocesses this in a way the algorithm can work with it. See Details for more information.}

\item{update.target.after}{[\code{integer(1)}] \cr 
When using double learning the target network / table will be updated after 
\code{update.target.after} steps.}
}
\value{
[\code{list(2)}] \cr
  Returns the action value function or model parameters [\code{matrix}] and the 
  number of steps per episode [\code{numeric}].
}
\description{
Q Sigma
}
\examples{
grid = makeEnvironment(transition.array = windyGridworld$transitions,
  reward.matrix = windyGridworld$rewards,
  initial.state = 30L)
  
qSigma(grid, sigma = 0.5)
qlearning(grid)
sarsa(grid)
expectedSarsa(grid)

\dontrun{
library(keras)
model = keras_model_sequential()
model \%>\% layer_dense(units = 4, activation = 'linear', input_shape = c(70))
  
makeOneHot = function(state) {
  one.hot = matrix(rep(0L, 70L), nrow = 1)
  one.hot[1L, state + 1L] = 1L
  one.hot
}
  
qSigma(grid, value.function = "neural.network", model = model, preprocessState = makeOneHot)
qSigma(grid, value.function = "neural.network", model = model, preprocessState = makeOneHot, 
  double.learning = TRUE, update.target.after = 100,
  replay.memory.size = 1000, batch.size = 32)
}

}
\references{
De Asis et al. (2017): Multi-step Reinforcement Learning: A Unifying Algorithm

Hasselt et al. (2015): Deep Reinforcement Learning with Double Q-Learning

Mnih et al. (2013): Playing Atari with Deep Reinforcement Learning

Schaul et al. (2016): Prioritized Experience Replay

Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction
}
