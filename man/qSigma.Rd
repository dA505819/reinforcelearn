% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qSigma.R
\name{qSigma}
\alias{qSigma}
\title{Q(sigma) (Table-lookup)}
\usage{
qSigma(envir, sigma = 1, lambda = 0, n.episodes = 100,
  learning.rate = 0.1, epsilon = 0.1, epsilon.decay = 0.5,
  epsilon.decay.after = 100L, initial.value = 0L, discount.factor = 1,
  seed = NULL)
}
\arguments{
\item{envir}{an R6 class: the reinforcement learning environment
created by \link{makeEnvironment}.}

\item{sigma}{scalar integer: sampling parameter, for sigma = 0 the
sarsa algorithm is obtained, for sigma = 1 expected sarsa.}

\item{lambda}{scalar numeric in (0, 1): Then lambda = 0 only
current state is updated (this is equivalent to TD(0)), for
lambda = 1 all states visited are updated, this is roughly
equivalent to every-visit Monte Carlo.}

\item{n.episodes}{scalar integer: number of episodes}

\item{learning.rate}{scalar numeric between 0 and 1: learning rate}

\item{epsilon}{scalar numeric: ratio of random exploration in
epsilon-greedy action selection}

\item{epsilon.decay}{scalar numeric between 0 and 1: decay epsilon
by this factor}

\item{epsilon.decay.after}{scalar integer: number of episodes afer
which to decay epsilon}

\item{initial.value}{scalar numeric: initial values for the action
values Q, set this to the maximal possible reward to encourage
exploration (optimistic initialization)}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{seed}{scalar integer: random seed}
}
\value{
optimal action value function Q
}
\description{
Q(sigma) is a generalization of Sarsa and Expected Sarsa algorithms.
}
\examples{
grid = makeWindyGridworld()
WindyGridworld1 = makeEnvironment(transition.array = grid$transition.array, 
  reward.matrix = grid$reward.matrix, initial.state = 30L)
res = qSigma(WindyGridworld1, n.episodes = 100, seed = 123)

}
\references{
De Asis et al. (2017): Multi-step Reinforcement Learning: A Unifying Algorithm
}
\seealso{
\link{sarsa}

\link{qlearning}
}
