% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predictMC.R
\name{predictMC}
\alias{predictMC}
\title{Monte Carlo Prediction}
\usage{
predictMC(policy, envir, n.episodes = 10, discount.factor = 1,
  method = c("first-visit, every-visit"), alpha = NULL)
}
\arguments{
\item{policy}{a policy specified as a probability matrix (states x actions)}

\item{envir}{the environment, a function returning the next state and reward given an action}

\item{n.episodes}{scalar integer: the number of episodes}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{method}{character: first-visit or every-visit method}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update.Useful in non-stationary environments, giving high
value to the last observed returns. If NULL the exact mean with alpha = 1 /
number of episodes will be used.}
}
\description{
Predict state value function v with Monte Carlo methods. The state value
function is estimated from mean returns of episodes.
}
\details{
Only works for episodic tasks (i.e. there must be at least one terminal
state)! An incremental mean update is implemented. Use a high alpha value to
give recent episodes a higher weight if you have a non-stationary environment
. First-visit Monte Carlo estimates the return following the first visit to
a state, every-visit Monte Carlo following all visits in the episode. Returns
are averaged over multiple episodes.
}
\examples{
set.seed(1477)
grid = gridworld$new()

# Define random policy
random.policy = matrix(1 / grid$n.actions, nrow = grid$n.states, 
  ncol = grid$n.actions)
  
# Estimate state value function with Monte Carlo prediction
v = predictMC(random.policy, grid, n.episodes = 100, method = "first-visit")
v = predictMC(random.policy, grid, n.episodes = 100, method = "every-visit")
}
\seealso{
\link{TD}
}
