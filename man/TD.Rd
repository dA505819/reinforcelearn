% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TD.R
\name{TD}
\alias{TD}
\title{Temporal difference learning}
\usage{
TD(policy, envir, n.episodes = 1, n = 10, discount.factor = 1,
  alpha = 0.1)
}
\arguments{
\item{policy}{a policy specified as a probability matrix (states x actions)}

\item{envir}{the environment, an R6 class. See also \code{\link[=envir]{envir()}}.}

\item{n.episodes}{scalar integer: the number of episodes}

\item{n}{scalar integer: number of steps TD target looks into the
future}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update.Useful in non-stationary environments, giving high
value to the last observed returns. If NULL the exact mean with alpha = 1 /
number of episodes will be used.}
}
\value{
value function
}
\description{
Temporal difference (TD) learning works by sampling one step from the
environment and plugging this into the update equation (Bootstrapping). This
works also for non-episodic environments.

Currently implemented: n-step TD (forward-view)
}
\examples{
set.seed(1477)
# Define environment, here simple gridworld
grid = gridworld$new()

# Define random policy
random.policy = matrix(1 / grid$n.actions, nrow = grid$n.states, 
  ncol = grid$n.actions)

# Estimate state value function with temporal-difference learning
v = TD(random.policy, grid, n = 2, n.episodes = 10000, alpha = 0.1)
}
\seealso{
\link{predictMC}
}
