% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TD.R
\name{TD}
\alias{TD}
\title{Temporal difference learning}
\usage{
TD(policy, envir, n.episodes = 1, n = 10, discount.factor = 1,
  alpha = 0.1)
}
\arguments{
\item{policy}{a policy specified as a probability matrix (states x actions)}

\item{envir}{the environment, a function returning the next state and reward given an action}

\item{n.episodes}{scalar integer: the number of episodes}

\item{n}{scalar integer: number of steps TD target looks into the
future}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update.Useful in non-stationary environments, giving high
value to the last observed returns. If NULL the exact mean with alpha = 1 /
number of episodes will be used.}
}
\value{
value function
}
\description{
Temporal difference (TD) learning works by sampling one step from the
environment and plugging this into the update equation (Bootstrapping). This
works also for non-episodic environments.

Currently implemented: n-step TD (forward-view)
}
\examples{
set.seed(1477)
grid = gridworld$new()

# Define random policy
n.states = nrow(grid$reward.matrix)
n.actions = ncol(grid$reward.matrix)
random.policy = matrix(1 / n.actions, nrow = n.states, ncol = n.actions)

# Estimate state value function with temporal-difference learning
v = TD(random.policy, grid, n = 2, alpha = 0.1)
}
\seealso{
\link{predictMC}
}
