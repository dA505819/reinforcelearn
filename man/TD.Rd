% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/td.R
\name{td}
\alias{td}
\title{Temporal difference learning}
\usage{
td(envir, policy, lambda = 0, n.steps = 100, discount = 1,
  learning.rate = 0.1)
}
\arguments{
\item{envir}{[\code{R6 class}] \cr
The reinforcement learning environment
created by \code{\link{makeEnvironment}}.}

\item{policy}{[\code{matrix}] \cr 
A policy specified as a probability matrix (states x actions).}

\item{lambda}{[\code{numeric(1) in [0, 1]}] \cr
The \code{lambda} parameter combines different n-step returns using eligibility traces.
Only used if the replay memory is of size 1, i.e. no experience replay is used.}

\item{n.steps}{[\code{integer(1)}] \cr 
Number of evaluations (steps in the environment).}

\item{discount}{[\code{numeric(1) in [0, 1]}] \cr
Discount factor.}

\item{learning.rate}{[\code{numeric(1)}] \cr
Learning rate used for gradient descent.}
}
\value{
[\code{numeric}] \cr
  Returns the state value function v.
}
\description{
Temporal difference (TD) learning is a method to estimate the state value 
function for a given policy. It works by sampling one step from the 
environment and plugging this into the update equation (Bootstrapping). This
also works for non-episodic environments.
}
\details{
The implementation works with eligibility traces. Eligibility traces combine 
a frequency and recency heuristic. Whenever a state is visited, the
eligibility of this state is increased. Over time the eligibility decreases 
exponentially. This way, states that occured often and recently get most 
credit for a reward and therefore are updated more strongly than states 
observed infrequently and longer time ago.
}
\examples{
# Define environment, here simple gridworld
env = gridworld()
  
# Define random policy
random.policy = matrix(1 / env$n.actions, nrow = env$n.states, 
  ncol = env$n.actions)

# Estimate state value function with temporal-difference learning (TD(0))
v = td(env, random.policy, lambda = 0, n.steps = 1000)
print(round(matrix(v, ncol = 4, byrow = TRUE)))
}
\references{
Sutton and Barto (Book draft 2017): Reinforcement Learning: An Introduction
}
