% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/TD.R
\name{TD}
\alias{TD}
\title{Temporal difference learning}
\usage{
TD(policy, envir, n.steps = 100, discount.factor = 1, alpha = NULL)
}
\arguments{
\item{policy}{a policy specified as a probability matrix \link{states x actions}}

\item{envir}{the environment, a function returning the next state and reward given an action}

\item{n.steps}{scalar integer: number of steps}

\item{discount.factor}{scalar numeric, discounting future rewards}

\item{alpha}{scalar numeric between 0 and 1: weighting factor in the
incremental mean update.Useful in non-stationary environments, giving high
value to the last observed returns. If NULL the exact mean with alpha = 1 /
number of episodes will be used.}
}
\value{
value function
}
\description{
Temporal difference (TD) learning works by sampling one step from the
environment and plugging this into the update equation (Bootstrapping). This
works also for non-episodic environments.

Currently implemented: TD(0)
}
\examples{
set.seed(1477)
grid = gridworld_R6$new()

# Define random policy
n.states = nrow(grid$reward.matrix)
n.actions = ncol(grid$reward.matrix)
random.policy = matrix(1 / n.actions, nrow = n.states, ncol = n.actions)

# Estimate state value function with temporal-difference learning
v = TD(random.policy, grid, n.steps = 100, alpha = 0.1)
}
\seealso{
\link{predictMC}
}
