% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gridworld_R6.R
\docType{data}
\name{gridworld_R6}
\alias{gridworld_R6}
\title{Gridworld environment as R6 class}
\format{An object of class \code{R6ClassGenerator} of length 24.}
\usage{
gridworld_R6
}
\arguments{
\item{shape}{length-two integer vector: the shape of the grid, e.g. (4, 4)}

\item{terminal.states}{integer vector of terminal states}
}
\value{
A ref class with method `step()`
}
\description{
Simple gridworld for reinforcement learning. With the step method given a state and an action in a gridworld,
the next state and reward are returned.
}
\details{
The states are enumerated as follows (example 4x4 grid):
\tabular{rrrr}{
 1 \tab 2 \tab 3 \tab 4 \cr
 5 \tab 6 \tab 7 \tab 8 \cr
 9 \tab 10 \tab 11 \tab 12 \cr
 13 \tab 14 \tab 15 \tab 16 \cr
}
So a board position could look like this (T: terminal state, x: current state):
\tabular{rrrr}{
 T \tab o \tab o \tab o \cr
 o \tab o \tab o \tab o \cr
 o \tab x \tab o \tab o \cr
 o \tab o \tab o \tab T \cr
}
Possible actions include going left, right, down or up. If an action would take you off
the grid, you remain in the previous state. For each step you get a reward of -1, until you reach
into a terminal state.
}
\examples{
set.seed(27)
grid = gridworld_R6$new(shape = c(4, 4), terminal.states = c(1, 16))

# initial state = 3
states = 3
rewards = numeric(0)
sampled.actions = character(0)
episode.over = FALSE
i = 1

while(grid$episode.over == FALSE) {
  sampled.actions = append(sampled.actions, sample(grid$actions, size = 1))
  grid$step(states[i], sampled.actions[i])
  states = append(states, grid$next.state)
  rewards = append(rewards, grid$reward)
  episode.over = grid$episode.over
  i = i + 1
}

print(rewards)
print(states)
}
\references{
Gridworld example from Sutton & Barto, chapter 4
}
\keyword{datasets}
