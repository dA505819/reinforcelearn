---
output: github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  fig.path = "README-"
)
```

# Reinforcement Learning in R <img src="reinforcelearn.png" align="right" height="36"/>

[![Travis-CI Build Status](https://travis-ci.org/markdumke/reinforcelearn.svg?branch=master)](https://travis-ci.org/markdumke/reinforcelearn)
[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/reinforcelearn)](https://cran.r-project.org/package=reinforcelearn)
[![Coverage Status](https://img.shields.io/codecov/c/github/markdumke/reinforcelearn/master.svg?maxAge=600)](https://codecov.io/github/markdumke/reinforcelearn?branch=master)

### Documentation

[Website](https://markdumke.github.io/reinforcelearn)

----

### Installation

```{r, eval = FALSE}
# install from CRAN
install.packages("reinforcelearn")

# install development version from github
devtools::install_github("markdumke/reinforcelearn")
```

----

### Get started

Reinforcement Learning with the package `reinforcelearn` is as easy as

```{r}
library(reinforcelearn)

# Create gridworld environment
env = makeEnvironment("WindyGridworld")

# Solve environment using Sarsa
res = sarsa(env, n.episodes = 30)
print(res$steps)
```

----

### Environments

With `makeEnvironment` you can create reinforcement learning environments.

```{r}
# Create environment.
step = function(self, action) {
  state = list(mean = action + rnorm(1), sd = runif(1))
  reward = rnorm(1, state[[1]], state[[2]])
  done = FALSE
  list(state, reward, done)
}

reset = function() {
  state = list(mean = 0, sd = 1)
  state
}

env = makeEnvironment("custom", step = step, reset = reset)
```

The environment is an `R6` class with a set of attributes and methods.
You can interact with the environment via the `reset` and `step` method.

```{r}
# Reset environment.
env$reset()
# Take action.
env$step(100)
```

There are some predefined environment classes, e.g. `MDPEnvironment`, which allows you to create a Markov Decision Process by passing on state transition array and reward matrix, or `GymEnvironment`, where you can use toy problems from [OpenAI Gym](https://gym.openai.com/).

```{r, eval = FALSE}
# Create an OpenAI Gym environment.
# Make sure you have Python, gym and reticulate installed.
env = makeEnvironment("Gym", "MountainCar-v0")

# Take random actions for 200 steps.
env$reset()
for (i in 1:200) {
  action = sample(0:2, 1)
  env$step(action)
  env$visualize()
}
env$close()
```

This should open a Python window showing the interaction with the environment.

For more details on how to create an environment have a look at the vignette: [How to create an environment?](https://markdumke.github.io/reinforcelearn/articles/environments.html)

----

### Algorithms

After you created an environment you can use various reinforcement learning algorithms to solve this environment. For example, for a tabular environment like gridworld you can use tabular Q-Learning to solve it and find the optimal action value function $Q*$. You can set various parameters like the learning rate, the number of episodes, the discount factor or epsilon.

```{r}
# Create the windy gridworld environment.
env = makeEnvironment("WindyGridworld")
res = qlearning(env, n.episodes = 30)

print(res$steps)

# Show value of each state.
print(matrix(round(apply(res$Q1, 1, max), 1), ncol = 10, byrow = TRUE))
```

We can then get the policy by taking the argmax over the action value function Q.

```{r}
policy = max.col(res$Q1) - 1L
print(matrix(policy, ncol = 10, byrow = TRUE))
```

For more details on algorithms have a look at the vignette: [How to solve an environment?](https://markdumke.github.io/reinforcelearn/articles/algorithms.html)

----

### Value function approximation

When the state space is large or even continuous tabular solution methods cannot be applied. Then it is better to approximate the value function using a function approximator. We need to define a function, which preprocesses the state observation, so that the function approximator can work with it.
Here is an example solving the mountain car problem using linear function approximation. 

```{r}
# Set up the Mountain Car problem.
m = makeEnvironment("MountainCar")

# Define preprocessing function (here grid tiling).
n.tilings = 8
max.size = 4096
iht = IHT(max.size)

position.max = m$state.space.bounds[[1]][2]
position.min = m$state.space.bounds[[1]][1]
velocity.max = m$state.space.bounds[[2]][2]
velocity.min = m$state.space.bounds[[2]][1]
position.scale = n.tilings / (position.max - position.min)
velocity.scale = n.tilings / (velocity.max - velocity.min)

preprocessState = function(state) {
  # scale state observation
  state = matrix(c(position.scale * state[1], velocity.scale * state[2]), ncol = 2)
  # get active tiles
  active.tiles = tiles(iht, 8, state)
  # return n hot vector with 1 at the position of each active tile
  nHot(active.tiles, max.size)
}

set.seed(123)
res = qlearning(m, fun.approx = "linear", 
  preprocessState = preprocessState, n.episodes = 20)
print(res$steps)
```

----

### Vignettes

Also have a look at the vignettes for further examples.

- [How to create an environment?](https://markdumke.github.io/reinforcelearn/articles/environments.html)
- [How to solve an environment?](https://markdumke.github.io/reinforcelearn/articles/algorithms.html)

----

Logo is a modification of https://www.r-project.org/logo/.
